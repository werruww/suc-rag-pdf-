{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "0uhiRnIWrH77"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "TpJ8QtCXsHqb"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "LpcCavRjsHnr"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!curl -fsSL https://ollama.com/install.sh | sh"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "9oDlKEZNsHlE",
        "outputId": "a2255eb5-03d3-4f54-8a0d-9401b62e11a0"
      },
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            ">>> Installing ollama to /usr/local\n",
            ">>> Downloading Linux amd64 bundle\n",
            "######################################################################## 100.0%\n",
            ">>> Creating ollama user...\n",
            ">>> Adding ollama user to video group...\n",
            ">>> Adding current user to ollama group...\n",
            ">>> Creating ollama systemd service...\n",
            "\u001b[1m\u001b[31mWARNING:\u001b[m systemd is not running\n",
            "\u001b[1m\u001b[31mWARNING:\u001b[m Unable to detect NVIDIA/AMD GPU. Install lspci or lshw to automatically detect and install GPU dependencies.\n",
            ">>> The Ollama API is now available at 127.0.0.1:11434.\n",
            ">>> Install complete. Run \"ollama\" from the command line.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!nohup ollama serve &"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "d9G2GnpCsWt0",
        "outputId": "b3ca9461-bff3-4c4c-8498-449a21790da4"
      },
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "nohup: appending output to 'nohup.out'\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!ollama pull deepseek-r1:1.5b"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "pf7FyJqaszTk",
        "outputId": "acc6d119-e1ad-4061-ede6-6c30e552a854"
      },
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[?2026h\u001b[?25l\u001b[1G\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[1G\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[1G\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[1G\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[1G\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[1G\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[1G\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[1G\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[1G\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[1G\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[1G\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[A\u001b[1G\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[A\u001b[1G\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[A\u001b[1G\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[A\u001b[1G\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[A\u001b[1G\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[A\u001b[1G\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[A\u001b[1G\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[A\u001b[1G\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[A\u001b[1G\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[A\u001b[1G\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[A\u001b[1G\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[A\u001b[1G\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[A\u001b[1G\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[A\u001b[1G\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[A\u001b[1G\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[A\u001b[1G\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[A\u001b[1G\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[A\u001b[1G\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[A\u001b[1G\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[A\u001b[1G\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[A\u001b[1G\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[A\u001b[1G\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[A\u001b[1G\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[A\u001b[1G\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[A\u001b[1G\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[A\u001b[1G\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[A\u001b[1G\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[A\u001b[1G\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[A\u001b[1G\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[A\u001b[1G\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[A\u001b[1G\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[A\u001b[1G\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[A\u001b[1G\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[A\u001b[1G\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[A\u001b[1G\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[A\u001b[1G\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[A\u001b[1G\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[A\u001b[1G\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[A\u001b[1G\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[A\u001b[1G\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[A\u001b[1G\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[A\u001b[1G\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[A\u001b[1G\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[A\u001b[1G\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[A\u001b[1G\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[A\u001b[1G\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[A\u001b[1G\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[A\u001b[1G\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[A\u001b[1G\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[A\u001b[1G\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[A\u001b[1G\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[A\u001b[A\u001b[1G\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[A\u001b[A\u001b[1G\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[A\u001b[A\u001b[1G\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[A\u001b[A\u001b[1G\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[A\u001b[A\u001b[1G\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[A\u001b[A\u001b[1G\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[A\u001b[A\u001b[1G\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[A\u001b[A\u001b[1G\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[A\u001b[A\u001b[1G\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[A\u001b[A\u001b[1G\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[A\u001b[A\u001b[1G\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[A\u001b[A\u001b[1G\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[A\u001b[A\u001b[1G\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[A\u001b[A\u001b[1G\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[A\u001b[A\u001b[A\u001b[1G\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[A\u001b[A\u001b[A\u001b[1G\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[A\u001b[A\u001b[A\u001b[1G\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[A\u001b[A\u001b[A\u001b[1G\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[A\u001b[A\u001b[A\u001b[1G\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[A\u001b[A\u001b[A\u001b[1G\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[A\u001b[A\u001b[A\u001b[1G\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[A\u001b[A\u001b[A\u001b[1G\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[A\u001b[A\u001b[A\u001b[1G\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[A\u001b[A\u001b[A\u001b[1G\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[A\u001b[A\u001b[A\u001b[1G\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[A\u001b[A\u001b[A\u001b[1G\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[A\u001b[A\u001b[A\u001b[1G\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[A\u001b[A\u001b[A\u001b[A\u001b[1G\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[A\u001b[A\u001b[A\u001b[A\u001b[1G\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[A\u001b[A\u001b[A\u001b[A\u001b[1G\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[A\u001b[A\u001b[A\u001b[A\u001b[1G\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[A\u001b[A\u001b[A\u001b[A\u001b[1G\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[A\u001b[A\u001b[A\u001b[A\u001b[1G\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[A\u001b[A\u001b[A\u001b[A\u001b[1G\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[A\u001b[A\u001b[A\u001b[A\u001b[1G\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[A\u001b[A\u001b[A\u001b[A\u001b[1G\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[A\u001b[A\u001b[A\u001b[A\u001b[1G\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[A\u001b[A\u001b[A\u001b[A\u001b[1G\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[A\u001b[A\u001b[A\u001b[A\u001b[1G\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[1G\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[1G\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[1G\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[1G\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[1G\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[1G\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[1G\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[1G\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[1G\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[1G\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[1G\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[1G\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[1G\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[1G\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[1G\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[1G\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[1G\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[1G\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[1G\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[1G\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[1G\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[1G\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[1G\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[1G\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[1G\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[1G\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[1G\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[1G\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[1G\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[1G\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[1G\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[1G\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[1G\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[1G\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[1G\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[1G\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[1G\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[1G\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[1G\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[1G\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[1G\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[1G\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[1G\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[1G\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[1G\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[1G\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[1G\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[1G\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[1G\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[1G\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[1G\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[1G\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[1G\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[1G\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[1G\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[1G\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[1G\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[1G\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[1G\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[1G\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[1G\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[1G\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[1G\u001b[?25h\u001b[?2026l\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!ollama list"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "k6ZctWoMtIuE",
        "outputId": "9bc8acbe-8b16-402c-9a0b-736a2b8c67ae"
      },
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "NAME                ID              SIZE      MODIFIED       \n",
            "deepseek-r1:1.5b    a42b25d8c10a    1.1 GB    15 seconds ago    \n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "curl -fsSL https://ollama.com/install.sh | sh\n",
        "nohup ollama serve &\n",
        "ollama pull llama3:8b\n",
        "ollama list\n",
        "\n"
      ],
      "metadata": {
        "id": "ZDdr-VMksHht"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!git clone https://github.com/weiwill88/Local_Pdf_Chat_RAG.git"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "E-IXA-OOscdk",
        "outputId": "c0153562-442a-4a8b-b03e-18e1b8af1959"
      },
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Cloning into 'Local_Pdf_Chat_RAG'...\n",
            "remote: Enumerating objects: 58, done.\u001b[K\n",
            "remote: Counting objects: 100% (58/58), done.\u001b[K\n",
            "remote: Compressing objects: 100% (39/39), done.\u001b[K\n",
            "remote: Total 58 (delta 18), reused 52 (delta 14), pack-reused 0 (from 0)\u001b[K\n",
            "Receiving objects: 100% (58/58), 3.71 MiB | 11.19 MiB/s, done.\n",
            "Resolving deltas: 100% (18/18), done.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "%cd Local_Pdf_Chat_RAG"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "i_rf7cAJskfT",
        "outputId": "a946c99a-10d5-449d-b25a-2c0b87fa0ddc"
      },
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "/content/Local_Pdf_Chat_RAG\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install -r /content/Local_Pdf_Chat_RAG/requirements.txt"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "4zjNXyqYslPk",
        "outputId": "f8f3b313-2f45-4757-ee31-2dcfbd1d6124"
      },
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting gradio>=3.50.0 (from -r /content/Local_Pdf_Chat_RAG/requirements.txt (line 2))\n",
            "  Downloading gradio-5.30.0-py3-none-any.whl.metadata (16 kB)\n",
            "Collecting pdfminer.six>=20221105 (from -r /content/Local_Pdf_Chat_RAG/requirements.txt (line 3))\n",
            "  Downloading pdfminer_six-20250506-py3-none-any.whl.metadata (4.2 kB)\n",
            "Requirement already satisfied: sentence-transformers>=2.2.2 in /usr/local/lib/python3.11/dist-packages (from -r /content/Local_Pdf_Chat_RAG/requirements.txt (line 4)) (4.1.0)\n",
            "Collecting faiss-cpu (from -r /content/Local_Pdf_Chat_RAG/requirements.txt (line 5))\n",
            "  Downloading faiss_cpu-1.11.0-cp311-cp311-manylinux_2_28_x86_64.whl.metadata (4.8 kB)\n",
            "Requirement already satisfied: langchain-text-splitters in /usr/local/lib/python3.11/dist-packages (from -r /content/Local_Pdf_Chat_RAG/requirements.txt (line 6)) (0.3.8)\n",
            "Collecting python-dotenv>=1.0.0 (from -r /content/Local_Pdf_Chat_RAG/requirements.txt (line 7))\n",
            "  Downloading python_dotenv-1.1.0-py3-none-any.whl.metadata (24 kB)\n",
            "Requirement already satisfied: requests>=2.31.0 in /usr/local/lib/python3.11/dist-packages (from -r /content/Local_Pdf_Chat_RAG/requirements.txt (line 8)) (2.32.3)\n",
            "Requirement already satisfied: markdown>=3.4.3 in /usr/local/lib/python3.11/dist-packages (from -r /content/Local_Pdf_Chat_RAG/requirements.txt (line 9)) (3.8)\n",
            "Collecting fastapi>=0.104.1 (from -r /content/Local_Pdf_Chat_RAG/requirements.txt (line 10))\n",
            "  Downloading fastapi-0.115.12-py3-none-any.whl.metadata (27 kB)\n",
            "Collecting uvicorn>=0.23.2 (from -r /content/Local_Pdf_Chat_RAG/requirements.txt (line 11))\n",
            "  Downloading uvicorn-0.34.2-py3-none-any.whl.metadata (6.5 kB)\n",
            "Collecting python-multipart>=0.0.6 (from -r /content/Local_Pdf_Chat_RAG/requirements.txt (line 12))\n",
            "  Downloading python_multipart-0.0.20-py3-none-any.whl.metadata (1.8 kB)\n",
            "Requirement already satisfied: urllib3>=1.26.16 in /usr/local/lib/python3.11/dist-packages (from -r /content/Local_Pdf_Chat_RAG/requirements.txt (line 13)) (2.4.0)\n",
            "Requirement already satisfied: jieba>=0.42.1 in /usr/local/lib/python3.11/dist-packages (from -r /content/Local_Pdf_Chat_RAG/requirements.txt (line 14)) (0.42.1)\n",
            "Requirement already satisfied: numpy>=1.26.1 in /usr/local/lib/python3.11/dist-packages (from -r /content/Local_Pdf_Chat_RAG/requirements.txt (line 19)) (2.0.2)\n",
            "Collecting rank-bm25>=0.2.2 (from -r /content/Local_Pdf_Chat_RAG/requirements.txt (line 20))\n",
            "  Downloading rank_bm25-0.2.2-py3-none-any.whl.metadata (3.2 kB)\n",
            "Collecting aiofiles<25.0,>=22.0 (from gradio>=3.50.0->-r /content/Local_Pdf_Chat_RAG/requirements.txt (line 2))\n",
            "  Downloading aiofiles-24.1.0-py3-none-any.whl.metadata (10 kB)\n",
            "Requirement already satisfied: anyio<5.0,>=3.0 in /usr/local/lib/python3.11/dist-packages (from gradio>=3.50.0->-r /content/Local_Pdf_Chat_RAG/requirements.txt (line 2)) (4.9.0)\n",
            "Collecting ffmpy (from gradio>=3.50.0->-r /content/Local_Pdf_Chat_RAG/requirements.txt (line 2))\n",
            "  Downloading ffmpy-0.5.0-py3-none-any.whl.metadata (3.0 kB)\n",
            "Collecting gradio-client==1.10.1 (from gradio>=3.50.0->-r /content/Local_Pdf_Chat_RAG/requirements.txt (line 2))\n",
            "  Downloading gradio_client-1.10.1-py3-none-any.whl.metadata (7.1 kB)\n",
            "Collecting groovy~=0.1 (from gradio>=3.50.0->-r /content/Local_Pdf_Chat_RAG/requirements.txt (line 2))\n",
            "  Downloading groovy-0.1.2-py3-none-any.whl.metadata (6.1 kB)\n",
            "Requirement already satisfied: httpx>=0.24.1 in /usr/local/lib/python3.11/dist-packages (from gradio>=3.50.0->-r /content/Local_Pdf_Chat_RAG/requirements.txt (line 2)) (0.28.1)\n",
            "Requirement already satisfied: huggingface-hub>=0.28.1 in /usr/local/lib/python3.11/dist-packages (from gradio>=3.50.0->-r /content/Local_Pdf_Chat_RAG/requirements.txt (line 2)) (0.31.2)\n",
            "Requirement already satisfied: jinja2<4.0 in /usr/local/lib/python3.11/dist-packages (from gradio>=3.50.0->-r /content/Local_Pdf_Chat_RAG/requirements.txt (line 2)) (3.1.6)\n",
            "Requirement already satisfied: markupsafe<4.0,>=2.0 in /usr/local/lib/python3.11/dist-packages (from gradio>=3.50.0->-r /content/Local_Pdf_Chat_RAG/requirements.txt (line 2)) (3.0.2)\n",
            "Requirement already satisfied: orjson~=3.0 in /usr/local/lib/python3.11/dist-packages (from gradio>=3.50.0->-r /content/Local_Pdf_Chat_RAG/requirements.txt (line 2)) (3.10.18)\n",
            "Requirement already satisfied: packaging in /usr/local/lib/python3.11/dist-packages (from gradio>=3.50.0->-r /content/Local_Pdf_Chat_RAG/requirements.txt (line 2)) (24.2)\n",
            "Requirement already satisfied: pandas<3.0,>=1.0 in /usr/local/lib/python3.11/dist-packages (from gradio>=3.50.0->-r /content/Local_Pdf_Chat_RAG/requirements.txt (line 2)) (2.2.2)\n",
            "Requirement already satisfied: pillow<12.0,>=8.0 in /usr/local/lib/python3.11/dist-packages (from gradio>=3.50.0->-r /content/Local_Pdf_Chat_RAG/requirements.txt (line 2)) (11.2.1)\n",
            "Requirement already satisfied: pydantic<2.12,>=2.0 in /usr/local/lib/python3.11/dist-packages (from gradio>=3.50.0->-r /content/Local_Pdf_Chat_RAG/requirements.txt (line 2)) (2.11.4)\n",
            "Collecting pydub (from gradio>=3.50.0->-r /content/Local_Pdf_Chat_RAG/requirements.txt (line 2))\n",
            "  Downloading pydub-0.25.1-py2.py3-none-any.whl.metadata (1.4 kB)\n",
            "Requirement already satisfied: pyyaml<7.0,>=5.0 in /usr/local/lib/python3.11/dist-packages (from gradio>=3.50.0->-r /content/Local_Pdf_Chat_RAG/requirements.txt (line 2)) (6.0.2)\n",
            "Collecting ruff>=0.9.3 (from gradio>=3.50.0->-r /content/Local_Pdf_Chat_RAG/requirements.txt (line 2))\n",
            "  Downloading ruff-0.11.10-py3-none-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (25 kB)\n",
            "Collecting safehttpx<0.2.0,>=0.1.6 (from gradio>=3.50.0->-r /content/Local_Pdf_Chat_RAG/requirements.txt (line 2))\n",
            "  Downloading safehttpx-0.1.6-py3-none-any.whl.metadata (4.2 kB)\n",
            "Collecting semantic-version~=2.0 (from gradio>=3.50.0->-r /content/Local_Pdf_Chat_RAG/requirements.txt (line 2))\n",
            "  Downloading semantic_version-2.10.0-py2.py3-none-any.whl.metadata (9.7 kB)\n",
            "Collecting starlette<1.0,>=0.40.0 (from gradio>=3.50.0->-r /content/Local_Pdf_Chat_RAG/requirements.txt (line 2))\n",
            "  Downloading starlette-0.46.2-py3-none-any.whl.metadata (6.2 kB)\n",
            "Collecting tomlkit<0.14.0,>=0.12.0 (from gradio>=3.50.0->-r /content/Local_Pdf_Chat_RAG/requirements.txt (line 2))\n",
            "  Downloading tomlkit-0.13.2-py3-none-any.whl.metadata (2.7 kB)\n",
            "Requirement already satisfied: typer<1.0,>=0.12 in /usr/local/lib/python3.11/dist-packages (from gradio>=3.50.0->-r /content/Local_Pdf_Chat_RAG/requirements.txt (line 2)) (0.15.3)\n",
            "Requirement already satisfied: typing-extensions~=4.0 in /usr/local/lib/python3.11/dist-packages (from gradio>=3.50.0->-r /content/Local_Pdf_Chat_RAG/requirements.txt (line 2)) (4.13.2)\n",
            "Requirement already satisfied: fsspec in /usr/local/lib/python3.11/dist-packages (from gradio-client==1.10.1->gradio>=3.50.0->-r /content/Local_Pdf_Chat_RAG/requirements.txt (line 2)) (2025.3.2)\n",
            "Requirement already satisfied: websockets<16.0,>=10.0 in /usr/local/lib/python3.11/dist-packages (from gradio-client==1.10.1->gradio>=3.50.0->-r /content/Local_Pdf_Chat_RAG/requirements.txt (line 2)) (15.0.1)\n",
            "Requirement already satisfied: charset-normalizer>=2.0.0 in /usr/local/lib/python3.11/dist-packages (from pdfminer.six>=20221105->-r /content/Local_Pdf_Chat_RAG/requirements.txt (line 3)) (3.4.2)\n",
            "Requirement already satisfied: cryptography>=36.0.0 in /usr/local/lib/python3.11/dist-packages (from pdfminer.six>=20221105->-r /content/Local_Pdf_Chat_RAG/requirements.txt (line 3)) (43.0.3)\n",
            "Requirement already satisfied: transformers<5.0.0,>=4.41.0 in /usr/local/lib/python3.11/dist-packages (from sentence-transformers>=2.2.2->-r /content/Local_Pdf_Chat_RAG/requirements.txt (line 4)) (4.51.3)\n",
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.11/dist-packages (from sentence-transformers>=2.2.2->-r /content/Local_Pdf_Chat_RAG/requirements.txt (line 4)) (4.67.1)\n",
            "Requirement already satisfied: torch>=1.11.0 in /usr/local/lib/python3.11/dist-packages (from sentence-transformers>=2.2.2->-r /content/Local_Pdf_Chat_RAG/requirements.txt (line 4)) (2.6.0+cu124)\n",
            "Requirement already satisfied: scikit-learn in /usr/local/lib/python3.11/dist-packages (from sentence-transformers>=2.2.2->-r /content/Local_Pdf_Chat_RAG/requirements.txt (line 4)) (1.6.1)\n",
            "Requirement already satisfied: scipy in /usr/local/lib/python3.11/dist-packages (from sentence-transformers>=2.2.2->-r /content/Local_Pdf_Chat_RAG/requirements.txt (line 4)) (1.15.3)\n",
            "Requirement already satisfied: langchain-core<1.0.0,>=0.3.51 in /usr/local/lib/python3.11/dist-packages (from langchain-text-splitters->-r /content/Local_Pdf_Chat_RAG/requirements.txt (line 6)) (0.3.59)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.11/dist-packages (from requests>=2.31.0->-r /content/Local_Pdf_Chat_RAG/requirements.txt (line 8)) (3.10)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.11/dist-packages (from requests>=2.31.0->-r /content/Local_Pdf_Chat_RAG/requirements.txt (line 8)) (2025.4.26)\n",
            "Requirement already satisfied: click>=7.0 in /usr/local/lib/python3.11/dist-packages (from uvicorn>=0.23.2->-r /content/Local_Pdf_Chat_RAG/requirements.txt (line 11)) (8.2.0)\n",
            "Requirement already satisfied: h11>=0.8 in /usr/local/lib/python3.11/dist-packages (from uvicorn>=0.23.2->-r /content/Local_Pdf_Chat_RAG/requirements.txt (line 11)) (0.16.0)\n",
            "Requirement already satisfied: sniffio>=1.1 in /usr/local/lib/python3.11/dist-packages (from anyio<5.0,>=3.0->gradio>=3.50.0->-r /content/Local_Pdf_Chat_RAG/requirements.txt (line 2)) (1.3.1)\n",
            "Requirement already satisfied: cffi>=1.12 in /usr/local/lib/python3.11/dist-packages (from cryptography>=36.0.0->pdfminer.six>=20221105->-r /content/Local_Pdf_Chat_RAG/requirements.txt (line 3)) (1.17.1)\n",
            "Requirement already satisfied: httpcore==1.* in /usr/local/lib/python3.11/dist-packages (from httpx>=0.24.1->gradio>=3.50.0->-r /content/Local_Pdf_Chat_RAG/requirements.txt (line 2)) (1.0.9)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.11/dist-packages (from huggingface-hub>=0.28.1->gradio>=3.50.0->-r /content/Local_Pdf_Chat_RAG/requirements.txt (line 2)) (3.18.0)\n",
            "Requirement already satisfied: langsmith<0.4,>=0.1.125 in /usr/local/lib/python3.11/dist-packages (from langchain-core<1.0.0,>=0.3.51->langchain-text-splitters->-r /content/Local_Pdf_Chat_RAG/requirements.txt (line 6)) (0.3.42)\n",
            "Requirement already satisfied: tenacity!=8.4.0,<10.0.0,>=8.1.0 in /usr/local/lib/python3.11/dist-packages (from langchain-core<1.0.0,>=0.3.51->langchain-text-splitters->-r /content/Local_Pdf_Chat_RAG/requirements.txt (line 6)) (9.1.2)\n",
            "Requirement already satisfied: jsonpatch<2.0,>=1.33 in /usr/local/lib/python3.11/dist-packages (from langchain-core<1.0.0,>=0.3.51->langchain-text-splitters->-r /content/Local_Pdf_Chat_RAG/requirements.txt (line 6)) (1.33)\n",
            "Requirement already satisfied: python-dateutil>=2.8.2 in /usr/local/lib/python3.11/dist-packages (from pandas<3.0,>=1.0->gradio>=3.50.0->-r /content/Local_Pdf_Chat_RAG/requirements.txt (line 2)) (2.9.0.post0)\n",
            "Requirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.11/dist-packages (from pandas<3.0,>=1.0->gradio>=3.50.0->-r /content/Local_Pdf_Chat_RAG/requirements.txt (line 2)) (2025.2)\n",
            "Requirement already satisfied: tzdata>=2022.7 in /usr/local/lib/python3.11/dist-packages (from pandas<3.0,>=1.0->gradio>=3.50.0->-r /content/Local_Pdf_Chat_RAG/requirements.txt (line 2)) (2025.2)\n",
            "Requirement already satisfied: annotated-types>=0.6.0 in /usr/local/lib/python3.11/dist-packages (from pydantic<2.12,>=2.0->gradio>=3.50.0->-r /content/Local_Pdf_Chat_RAG/requirements.txt (line 2)) (0.7.0)\n",
            "Requirement already satisfied: pydantic-core==2.33.2 in /usr/local/lib/python3.11/dist-packages (from pydantic<2.12,>=2.0->gradio>=3.50.0->-r /content/Local_Pdf_Chat_RAG/requirements.txt (line 2)) (2.33.2)\n",
            "Requirement already satisfied: typing-inspection>=0.4.0 in /usr/local/lib/python3.11/dist-packages (from pydantic<2.12,>=2.0->gradio>=3.50.0->-r /content/Local_Pdf_Chat_RAG/requirements.txt (line 2)) (0.4.0)\n",
            "Requirement already satisfied: networkx in /usr/local/lib/python3.11/dist-packages (from torch>=1.11.0->sentence-transformers>=2.2.2->-r /content/Local_Pdf_Chat_RAG/requirements.txt (line 4)) (3.4.2)\n",
            "Collecting nvidia-cuda-nvrtc-cu12==12.4.127 (from torch>=1.11.0->sentence-transformers>=2.2.2->-r /content/Local_Pdf_Chat_RAG/requirements.txt (line 4))\n",
            "  Downloading nvidia_cuda_nvrtc_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl.metadata (1.5 kB)\n",
            "Collecting nvidia-cuda-runtime-cu12==12.4.127 (from torch>=1.11.0->sentence-transformers>=2.2.2->-r /content/Local_Pdf_Chat_RAG/requirements.txt (line 4))\n",
            "  Downloading nvidia_cuda_runtime_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl.metadata (1.5 kB)\n",
            "Collecting nvidia-cuda-cupti-cu12==12.4.127 (from torch>=1.11.0->sentence-transformers>=2.2.2->-r /content/Local_Pdf_Chat_RAG/requirements.txt (line 4))\n",
            "  Downloading nvidia_cuda_cupti_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl.metadata (1.6 kB)\n",
            "Collecting nvidia-cudnn-cu12==9.1.0.70 (from torch>=1.11.0->sentence-transformers>=2.2.2->-r /content/Local_Pdf_Chat_RAG/requirements.txt (line 4))\n",
            "  Downloading nvidia_cudnn_cu12-9.1.0.70-py3-none-manylinux2014_x86_64.whl.metadata (1.6 kB)\n",
            "Collecting nvidia-cublas-cu12==12.4.5.8 (from torch>=1.11.0->sentence-transformers>=2.2.2->-r /content/Local_Pdf_Chat_RAG/requirements.txt (line 4))\n",
            "  Downloading nvidia_cublas_cu12-12.4.5.8-py3-none-manylinux2014_x86_64.whl.metadata (1.5 kB)\n",
            "Collecting nvidia-cufft-cu12==11.2.1.3 (from torch>=1.11.0->sentence-transformers>=2.2.2->-r /content/Local_Pdf_Chat_RAG/requirements.txt (line 4))\n",
            "  Downloading nvidia_cufft_cu12-11.2.1.3-py3-none-manylinux2014_x86_64.whl.metadata (1.5 kB)\n",
            "Collecting nvidia-curand-cu12==10.3.5.147 (from torch>=1.11.0->sentence-transformers>=2.2.2->-r /content/Local_Pdf_Chat_RAG/requirements.txt (line 4))\n",
            "  Downloading nvidia_curand_cu12-10.3.5.147-py3-none-manylinux2014_x86_64.whl.metadata (1.5 kB)\n",
            "Collecting nvidia-cusolver-cu12==11.6.1.9 (from torch>=1.11.0->sentence-transformers>=2.2.2->-r /content/Local_Pdf_Chat_RAG/requirements.txt (line 4))\n",
            "  Downloading nvidia_cusolver_cu12-11.6.1.9-py3-none-manylinux2014_x86_64.whl.metadata (1.6 kB)\n",
            "Collecting nvidia-cusparse-cu12==12.3.1.170 (from torch>=1.11.0->sentence-transformers>=2.2.2->-r /content/Local_Pdf_Chat_RAG/requirements.txt (line 4))\n",
            "  Downloading nvidia_cusparse_cu12-12.3.1.170-py3-none-manylinux2014_x86_64.whl.metadata (1.6 kB)\n",
            "Requirement already satisfied: nvidia-cusparselt-cu12==0.6.2 in /usr/local/lib/python3.11/dist-packages (from torch>=1.11.0->sentence-transformers>=2.2.2->-r /content/Local_Pdf_Chat_RAG/requirements.txt (line 4)) (0.6.2)\n",
            "Requirement already satisfied: nvidia-nccl-cu12==2.21.5 in /usr/local/lib/python3.11/dist-packages (from torch>=1.11.0->sentence-transformers>=2.2.2->-r /content/Local_Pdf_Chat_RAG/requirements.txt (line 4)) (2.21.5)\n",
            "Requirement already satisfied: nvidia-nvtx-cu12==12.4.127 in /usr/local/lib/python3.11/dist-packages (from torch>=1.11.0->sentence-transformers>=2.2.2->-r /content/Local_Pdf_Chat_RAG/requirements.txt (line 4)) (12.4.127)\n",
            "Collecting nvidia-nvjitlink-cu12==12.4.127 (from torch>=1.11.0->sentence-transformers>=2.2.2->-r /content/Local_Pdf_Chat_RAG/requirements.txt (line 4))\n",
            "  Downloading nvidia_nvjitlink_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl.metadata (1.5 kB)\n",
            "Requirement already satisfied: triton==3.2.0 in /usr/local/lib/python3.11/dist-packages (from torch>=1.11.0->sentence-transformers>=2.2.2->-r /content/Local_Pdf_Chat_RAG/requirements.txt (line 4)) (3.2.0)\n",
            "Requirement already satisfied: sympy==1.13.1 in /usr/local/lib/python3.11/dist-packages (from torch>=1.11.0->sentence-transformers>=2.2.2->-r /content/Local_Pdf_Chat_RAG/requirements.txt (line 4)) (1.13.1)\n",
            "Requirement already satisfied: mpmath<1.4,>=1.1.0 in /usr/local/lib/python3.11/dist-packages (from sympy==1.13.1->torch>=1.11.0->sentence-transformers>=2.2.2->-r /content/Local_Pdf_Chat_RAG/requirements.txt (line 4)) (1.3.0)\n",
            "Requirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.11/dist-packages (from transformers<5.0.0,>=4.41.0->sentence-transformers>=2.2.2->-r /content/Local_Pdf_Chat_RAG/requirements.txt (line 4)) (2024.11.6)\n",
            "Requirement already satisfied: tokenizers<0.22,>=0.21 in /usr/local/lib/python3.11/dist-packages (from transformers<5.0.0,>=4.41.0->sentence-transformers>=2.2.2->-r /content/Local_Pdf_Chat_RAG/requirements.txt (line 4)) (0.21.1)\n",
            "Requirement already satisfied: safetensors>=0.4.3 in /usr/local/lib/python3.11/dist-packages (from transformers<5.0.0,>=4.41.0->sentence-transformers>=2.2.2->-r /content/Local_Pdf_Chat_RAG/requirements.txt (line 4)) (0.5.3)\n",
            "Requirement already satisfied: shellingham>=1.3.0 in /usr/local/lib/python3.11/dist-packages (from typer<1.0,>=0.12->gradio>=3.50.0->-r /content/Local_Pdf_Chat_RAG/requirements.txt (line 2)) (1.5.4)\n",
            "Requirement already satisfied: rich>=10.11.0 in /usr/local/lib/python3.11/dist-packages (from typer<1.0,>=0.12->gradio>=3.50.0->-r /content/Local_Pdf_Chat_RAG/requirements.txt (line 2)) (13.9.4)\n",
            "Requirement already satisfied: joblib>=1.2.0 in /usr/local/lib/python3.11/dist-packages (from scikit-learn->sentence-transformers>=2.2.2->-r /content/Local_Pdf_Chat_RAG/requirements.txt (line 4)) (1.5.0)\n",
            "Requirement already satisfied: threadpoolctl>=3.1.0 in /usr/local/lib/python3.11/dist-packages (from scikit-learn->sentence-transformers>=2.2.2->-r /content/Local_Pdf_Chat_RAG/requirements.txt (line 4)) (3.6.0)\n",
            "Requirement already satisfied: pycparser in /usr/local/lib/python3.11/dist-packages (from cffi>=1.12->cryptography>=36.0.0->pdfminer.six>=20221105->-r /content/Local_Pdf_Chat_RAG/requirements.txt (line 3)) (2.22)\n",
            "Requirement already satisfied: jsonpointer>=1.9 in /usr/local/lib/python3.11/dist-packages (from jsonpatch<2.0,>=1.33->langchain-core<1.0.0,>=0.3.51->langchain-text-splitters->-r /content/Local_Pdf_Chat_RAG/requirements.txt (line 6)) (3.0.0)\n",
            "Requirement already satisfied: requests-toolbelt<2.0.0,>=1.0.0 in /usr/local/lib/python3.11/dist-packages (from langsmith<0.4,>=0.1.125->langchain-core<1.0.0,>=0.3.51->langchain-text-splitters->-r /content/Local_Pdf_Chat_RAG/requirements.txt (line 6)) (1.0.0)\n",
            "Requirement already satisfied: zstandard<0.24.0,>=0.23.0 in /usr/local/lib/python3.11/dist-packages (from langsmith<0.4,>=0.1.125->langchain-core<1.0.0,>=0.3.51->langchain-text-splitters->-r /content/Local_Pdf_Chat_RAG/requirements.txt (line 6)) (0.23.0)\n",
            "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.11/dist-packages (from python-dateutil>=2.8.2->pandas<3.0,>=1.0->gradio>=3.50.0->-r /content/Local_Pdf_Chat_RAG/requirements.txt (line 2)) (1.17.0)\n",
            "Requirement already satisfied: markdown-it-py>=2.2.0 in /usr/local/lib/python3.11/dist-packages (from rich>=10.11.0->typer<1.0,>=0.12->gradio>=3.50.0->-r /content/Local_Pdf_Chat_RAG/requirements.txt (line 2)) (3.0.0)\n",
            "Requirement already satisfied: pygments<3.0.0,>=2.13.0 in /usr/local/lib/python3.11/dist-packages (from rich>=10.11.0->typer<1.0,>=0.12->gradio>=3.50.0->-r /content/Local_Pdf_Chat_RAG/requirements.txt (line 2)) (2.19.1)\n",
            "Requirement already satisfied: mdurl~=0.1 in /usr/local/lib/python3.11/dist-packages (from markdown-it-py>=2.2.0->rich>=10.11.0->typer<1.0,>=0.12->gradio>=3.50.0->-r /content/Local_Pdf_Chat_RAG/requirements.txt (line 2)) (0.1.2)\n",
            "Downloading gradio-5.30.0-py3-none-any.whl (54.2 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m54.2/54.2 MB\u001b[0m \u001b[31m16.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading gradio_client-1.10.1-py3-none-any.whl (323 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m323.1/323.1 kB\u001b[0m \u001b[31m25.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading pdfminer_six-20250506-py3-none-any.whl (5.6 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m5.6/5.6 MB\u001b[0m \u001b[31m102.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading faiss_cpu-1.11.0-cp311-cp311-manylinux_2_28_x86_64.whl (31.3 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m31.3/31.3 MB\u001b[0m \u001b[31m57.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading python_dotenv-1.1.0-py3-none-any.whl (20 kB)\n",
            "Downloading fastapi-0.115.12-py3-none-any.whl (95 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m95.2/95.2 kB\u001b[0m \u001b[31m8.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading uvicorn-0.34.2-py3-none-any.whl (62 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m62.5/62.5 kB\u001b[0m \u001b[31m5.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading python_multipart-0.0.20-py3-none-any.whl (24 kB)\n",
            "Downloading rank_bm25-0.2.2-py3-none-any.whl (8.6 kB)\n",
            "Downloading aiofiles-24.1.0-py3-none-any.whl (15 kB)\n",
            "Downloading groovy-0.1.2-py3-none-any.whl (14 kB)\n",
            "Downloading ruff-0.11.10-py3-none-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (11.6 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m11.6/11.6 MB\u001b[0m \u001b[31m103.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading safehttpx-0.1.6-py3-none-any.whl (8.7 kB)\n",
            "Downloading semantic_version-2.10.0-py2.py3-none-any.whl (15 kB)\n",
            "Downloading starlette-0.46.2-py3-none-any.whl (72 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m72.0/72.0 kB\u001b[0m \u001b[31m6.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading tomlkit-0.13.2-py3-none-any.whl (37 kB)\n",
            "Downloading nvidia_cublas_cu12-12.4.5.8-py3-none-manylinux2014_x86_64.whl (363.4 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m363.4/363.4 MB\u001b[0m \u001b[31m4.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading nvidia_cuda_cupti_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl (13.8 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m13.8/13.8 MB\u001b[0m \u001b[31m102.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading nvidia_cuda_nvrtc_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl (24.6 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m24.6/24.6 MB\u001b[0m \u001b[31m73.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading nvidia_cuda_runtime_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl (883 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m883.7/883.7 kB\u001b[0m \u001b[31m53.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading nvidia_cudnn_cu12-9.1.0.70-py3-none-manylinux2014_x86_64.whl (664.8 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m664.8/664.8 MB\u001b[0m \u001b[31m2.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading nvidia_cufft_cu12-11.2.1.3-py3-none-manylinux2014_x86_64.whl (211.5 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m211.5/211.5 MB\u001b[0m \u001b[31m6.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading nvidia_curand_cu12-10.3.5.147-py3-none-manylinux2014_x86_64.whl (56.3 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m56.3/56.3 MB\u001b[0m \u001b[31m12.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading nvidia_cusolver_cu12-11.6.1.9-py3-none-manylinux2014_x86_64.whl (127.9 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m127.9/127.9 MB\u001b[0m \u001b[31m7.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading nvidia_cusparse_cu12-12.3.1.170-py3-none-manylinux2014_x86_64.whl (207.5 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m207.5/207.5 MB\u001b[0m \u001b[31m5.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading nvidia_nvjitlink_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl (21.1 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m21.1/21.1 MB\u001b[0m \u001b[31m83.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading ffmpy-0.5.0-py3-none-any.whl (6.0 kB)\n",
            "Downloading pydub-0.25.1-py2.py3-none-any.whl (32 kB)\n",
            "Installing collected packages: pydub, uvicorn, tomlkit, semantic-version, ruff, rank-bm25, python-multipart, python-dotenv, nvidia-nvjitlink-cu12, nvidia-curand-cu12, nvidia-cufft-cu12, nvidia-cuda-runtime-cu12, nvidia-cuda-nvrtc-cu12, nvidia-cuda-cupti-cu12, nvidia-cublas-cu12, groovy, ffmpy, faiss-cpu, aiofiles, starlette, nvidia-cusparse-cu12, nvidia-cudnn-cu12, safehttpx, pdfminer.six, nvidia-cusolver-cu12, gradio-client, fastapi, gradio\n",
            "  Attempting uninstall: nvidia-nvjitlink-cu12\n",
            "    Found existing installation: nvidia-nvjitlink-cu12 12.5.82\n",
            "    Uninstalling nvidia-nvjitlink-cu12-12.5.82:\n",
            "      Successfully uninstalled nvidia-nvjitlink-cu12-12.5.82\n",
            "  Attempting uninstall: nvidia-curand-cu12\n",
            "    Found existing installation: nvidia-curand-cu12 10.3.6.82\n",
            "    Uninstalling nvidia-curand-cu12-10.3.6.82:\n",
            "      Successfully uninstalled nvidia-curand-cu12-10.3.6.82\n",
            "  Attempting uninstall: nvidia-cufft-cu12\n",
            "    Found existing installation: nvidia-cufft-cu12 11.2.3.61\n",
            "    Uninstalling nvidia-cufft-cu12-11.2.3.61:\n",
            "      Successfully uninstalled nvidia-cufft-cu12-11.2.3.61\n",
            "  Attempting uninstall: nvidia-cuda-runtime-cu12\n",
            "    Found existing installation: nvidia-cuda-runtime-cu12 12.5.82\n",
            "    Uninstalling nvidia-cuda-runtime-cu12-12.5.82:\n",
            "      Successfully uninstalled nvidia-cuda-runtime-cu12-12.5.82\n",
            "  Attempting uninstall: nvidia-cuda-nvrtc-cu12\n",
            "    Found existing installation: nvidia-cuda-nvrtc-cu12 12.5.82\n",
            "    Uninstalling nvidia-cuda-nvrtc-cu12-12.5.82:\n",
            "      Successfully uninstalled nvidia-cuda-nvrtc-cu12-12.5.82\n",
            "  Attempting uninstall: nvidia-cuda-cupti-cu12\n",
            "    Found existing installation: nvidia-cuda-cupti-cu12 12.5.82\n",
            "    Uninstalling nvidia-cuda-cupti-cu12-12.5.82:\n",
            "      Successfully uninstalled nvidia-cuda-cupti-cu12-12.5.82\n",
            "  Attempting uninstall: nvidia-cublas-cu12\n",
            "    Found existing installation: nvidia-cublas-cu12 12.5.3.2\n",
            "    Uninstalling nvidia-cublas-cu12-12.5.3.2:\n",
            "      Successfully uninstalled nvidia-cublas-cu12-12.5.3.2\n",
            "  Attempting uninstall: nvidia-cusparse-cu12\n",
            "    Found existing installation: nvidia-cusparse-cu12 12.5.1.3\n",
            "    Uninstalling nvidia-cusparse-cu12-12.5.1.3:\n",
            "      Successfully uninstalled nvidia-cusparse-cu12-12.5.1.3\n",
            "  Attempting uninstall: nvidia-cudnn-cu12\n",
            "    Found existing installation: nvidia-cudnn-cu12 9.3.0.75\n",
            "    Uninstalling nvidia-cudnn-cu12-9.3.0.75:\n",
            "      Successfully uninstalled nvidia-cudnn-cu12-9.3.0.75\n",
            "  Attempting uninstall: nvidia-cusolver-cu12\n",
            "    Found existing installation: nvidia-cusolver-cu12 11.6.3.83\n",
            "    Uninstalling nvidia-cusolver-cu12-11.6.3.83:\n",
            "      Successfully uninstalled nvidia-cusolver-cu12-11.6.3.83\n",
            "Successfully installed aiofiles-24.1.0 faiss-cpu-1.11.0 fastapi-0.115.12 ffmpy-0.5.0 gradio-5.30.0 gradio-client-1.10.1 groovy-0.1.2 nvidia-cublas-cu12-12.4.5.8 nvidia-cuda-cupti-cu12-12.4.127 nvidia-cuda-nvrtc-cu12-12.4.127 nvidia-cuda-runtime-cu12-12.4.127 nvidia-cudnn-cu12-9.1.0.70 nvidia-cufft-cu12-11.2.1.3 nvidia-curand-cu12-10.3.5.147 nvidia-cusolver-cu12-11.6.1.9 nvidia-cusparse-cu12-12.3.1.170 nvidia-nvjitlink-cu12-12.4.127 pdfminer.six-20250506 pydub-0.25.1 python-dotenv-1.1.0 python-multipart-0.0.20 rank-bm25-0.2.2 ruff-0.11.10 safehttpx-0.1.6 semantic-version-2.10.0 starlette-0.46.2 tomlkit-0.13.2 uvicorn-0.34.2\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!python rag_demo_pro.py"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "kDHZxwxutOpE",
        "outputId": "3135b2d4-7416-4e22-d8cf-fe7891f876d4"
      },
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "2025-05-20 03:20:48.416262: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:477] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\n",
            "WARNING: All log messages before absl::InitializeLog() is called are written to STDERR\n",
            "E0000 00:00:1747711248.659046    1805 cuda_dnn.cc:8310] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\n",
            "E0000 00:00:1747711248.730392    1805 cuda_blas.cc:1418] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n",
            "2025-05-20 03:20:49.216330: I tensorflow/core/platform/cpu_feature_guard.cc:210] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\n",
            "To enable the following instructions: AVX2 FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
            "modules.json: 100% 349/349 [00:00<00:00, 1.24MB/s]\n",
            "config_sentence_transformers.json: 100% 116/116 [00:00<00:00, 662kB/s]\n",
            "README.md: 10.5kB [00:00, 1.08MB/s]\n",
            "sentence_bert_config.json: 100% 53.0/53.0 [00:00<00:00, 273kB/s]\n",
            "config.json: 612B [00:00, 2.33MB/s]       \n",
            "Xet Storage is enabled for this repo, but the 'hf_xet' package is not installed. Falling back to regular HTTP download. For better performance, install the package with: `pip install huggingface_hub[hf_xet]` or `pip install hf_xet`\n",
            "WARNING:huggingface_hub.file_download:Xet Storage is enabled for this repo, but the 'hf_xet' package is not installed. Falling back to regular HTTP download. For better performance, install the package with: `pip install huggingface_hub[hf_xet]` or `pip install hf_xet`\n",
            "model.safetensors: 100% 90.9M/90.9M [00:01<00:00, 49.1MB/s]\n",
            "tokenizer_config.json: 100% 350/350 [00:00<00:00, 1.68MB/s]\n",
            "vocab.txt: 232kB [00:00, 793kB/s]\n",
            "tokenizer.json: 466kB [00:00, 3.00MB/s]\n",
            "special_tokens_map.json: 100% 112/112 [00:00<00:00, 741kB/s]\n",
            "config.json: 100% 190/190 [00:00<00:00, 899kB/s]\n",
            "Gradio version: 5.30.0\n",
            "/content/Local_Pdf_Chat_RAG/rag_demo_pro.py:1721: UserWarning: You have not specified a value for the `type` parameter. Defaulting to the 'tuples' format for chatbot messages, but this is deprecated and will be removed in a future version of Gradio. Please set type='messages' instead, which uses openai-style dictionaries with 'role' and 'content' keys.\n",
            "  chatbot = gr.Chatbot(\n",
            "模型未加载！请先执行：\n",
            "ollama pull deepseek-r1:7b\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!nohup ollama serve &\n",
        "!ollama list"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "YtgTyO5fuMb1",
        "outputId": "161c28d0-39fd-4dba-fa00-b5d6b72d9d76"
      },
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "nohup: appending output to 'nohup.out'\n",
            "NAME                ID              SIZE      MODIFIED      \n",
            "deepseek-r1:1.5b    a42b25d8c10a    1.1 GB    5 minutes ago    \n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "import os\n",
        "os.environ['HF_ENDPOINT'] = 'https://hf-mirror.com'\n",
        "# 以上两行添加的Hugging Face镜像设置，是为了解决没有科学上网环境下载向量模型的问题\n",
        "import gradio as gr\n",
        "from pdfminer.high_level import extract_text_to_fp\n",
        "from sentence_transformers import SentenceTransformer\n",
        "# 导入交叉编码器\n",
        "from sentence_transformers import CrossEncoder\n",
        "import faiss # Новый импорт\n",
        "import requests\n",
        "import json\n",
        "from io import StringIO\n",
        "from langchain_text_splitters import RecursiveCharacterTextSplitter\n",
        "import os\n",
        "import socket\n",
        "import webbrowser\n",
        "import logging\n",
        "from requests.adapters import HTTPAdapter\n",
        "from urllib3.util.retry import Retry\n",
        "import time\n",
        "from datetime import datetime\n",
        "import hashlib\n",
        "import re\n",
        "from dotenv import load_dotenv\n",
        "# 导入BM25算法库\n",
        "from rank_bm25 import BM25Okapi\n",
        "import numpy as np # Убедимся, что numpy импортирован\n",
        "import jieba\n",
        "import threading\n",
        "from functools import lru_cache\n",
        "\n",
        "# 加载环境变量\n",
        "load_dotenv()\n",
        "SERPAPI_KEY = os.getenv(\"SERPAPI_KEY\")  # В .env файле установите SERPAPI_KEY\n",
        "SEARCH_ENGINE = \"google\"  # Можно изменить на другую поисковую систему при необходимости\n",
        "# Новое: Конфигурация метода переранжирования (кросс-энкодер или LLM)\n",
        "RERANK_METHOD = os.getenv(\"RERANK_METHOD\", \"cross_encoder\")  # \"cross_encoder\" или \"llm\"\n",
        "# Новое: Конфигурация SiliconFlow API\n",
        "SILICONFLOW_API_KEY = os.getenv(\"SILICONFLOW_API_KEY\")\n",
        "SILICONFLOW_API_URL = os.getenv(\"SILICONFLOW_API_URL\", \"https://api.siliconflow.cn/v1/chat/completions\")\n",
        "\n",
        "# В начале файла добавляем настройки таймаута\n",
        "import requests\n",
        "requests.adapters.DEFAULT_RETRIES = 3  # Увеличиваем количество попыток\n",
        "\n",
        "# В начале файла добавляем настройки переменных окружения\n",
        "os.environ['TF_ENABLE_ONEDNN_OPTS'] = '0'  # Отключаем оптимизацию oneDNN\n",
        "\n",
        "# В самом начале файла добавляем конфигурацию прокси\n",
        "import os\n",
        "os.environ['NO_PROXY'] = 'localhost,127.0.0.1'  # Новая настройка обхода прокси\n",
        "\n",
        "# Инициализация компонентов\n",
        "EMBED_MODEL = SentenceTransformer('all-MiniLM-L6-v2')\n",
        "# Модель для эмбеддингов также можно переключить на модель, оптимизированную для китайского языка, например:\n",
        "# EMBED_MODEL = SentenceTransformer('shibing624/text2vec-base-chinese')\n",
        "\n",
        "# FAISS相关的 глобальные переменные\n",
        "faiss_index = None\n",
        "faiss_contents_map = {}  # original_id -> content\n",
        "faiss_metadatas_map = {} # original_id -> metadata\n",
        "faiss_id_order_for_index = [] # Сохраняет порядок ID, как они были добавлены в FAISS\n",
        "\n",
        "# Новое: Инициализация кросс-энкодера (отложенная загрузка)\n",
        "cross_encoder = None\n",
        "cross_encoder_lock = threading.Lock()\n",
        "\n",
        "def get_cross_encoder():\n",
        "    \"\"\"延迟加载交叉编码器模型\"\"\"\n",
        "    global cross_encoder\n",
        "    if cross_encoder is None:\n",
        "        with cross_encoder_lock:\n",
        "            if cross_encoder is None:\n",
        "                try:\n",
        "                    # 使用多语言交叉编码器，更适合中文\n",
        "                    cross_encoder = CrossEncoder('sentence-transformers/distiluse-base-multilingual-cased-v2')\n",
        "                    logging.info(\"交叉编码器加载成功\")\n",
        "                except Exception as e:\n",
        "                    logging.error(f\"加载交叉编码器失败: {str(e)}\")\n",
        "                    # 设置为None，下次调用会重试\n",
        "                    cross_encoder = None\n",
        "    return cross_encoder\n",
        "\n",
        "# 新增：BM25索引管理\n",
        "def recursive_retrieval(initial_query, max_iterations=3, enable_web_search=False, model_choice=\"ollama\"):\n",
        "    \"\"\"\n",
        "    实现递归检索与迭代查询功能\n",
        "    通过分析当前查询结果，确定是否需要进一步查询\n",
        "\n",
        "    Args:\n",
        "        initial_query: 初始查询\n",
        "        max_iterations: 最大迭代次数\n",
        "        enable_web_search: 是否启用网络搜索\n",
        "        model_choice: 使用的模型选择(\"ollama\"或\"siliconflow\")\n",
        "\n",
        "    Returns:\n",
        "        包含所有检索内容的列表\n",
        "    \"\"\"\n",
        "    query = initial_query\n",
        "    all_contexts = []\n",
        "    all_doc_ids = []  # 使用原始ID\n",
        "    all_metadata = []\n",
        "\n",
        "    global faiss_index, faiss_contents_map, faiss_metadatas_map, faiss_id_order_for_index\n",
        "\n",
        "    for i in range(max_iterations):\n",
        "        logging.info(f\"递归检索迭代 {i+1}/{max_iterations}，当前查询: {query}\")\n",
        "\n",
        "        web_results_texts = [] # Store text from web results for context building\n",
        "        if enable_web_search and check_serpapi_key():\n",
        "            try:\n",
        "                # update_web_results now needs to handle FAISS directly or be adapted\n",
        "                # For now, let's assume it returns texts to be added to context\n",
        "                web_search_raw_results = update_web_results(query) # This function needs to be adapted for FAISS\n",
        "                for res in web_search_raw_results:\n",
        "                    text = f\"标题：{res.get('title', '')}\\\\n摘要：{res.get('snippet', '')}\"\n",
        "                    web_results_texts.append(text)\n",
        "                    # We would also need to add these to faiss_index, faiss_contents_map etc.\n",
        "                    # and get their FAISS indices if we want them to be part of semantic search.\n",
        "                    # This part is complex due to dynamic addition and potential ID clashes.\n",
        "                    # For now, web results are added as pure text context, not searched semantically *again* within this loop's FAISS query.\n",
        "            except Exception as e:\n",
        "                logging.error(f\"网络搜索错误: {str(e)}\")\n",
        "\n",
        "        query_embedding = EMBED_MODEL.encode([query])\n",
        "        query_embedding_np = np.array(query_embedding).astype('float32')\n",
        "\n",
        "        semantic_results_docs = []\n",
        "        semantic_results_metadatas = []\n",
        "        semantic_results_ids = []\n",
        "\n",
        "        if faiss_index and faiss_index.ntotal > 0:\n",
        "            try:\n",
        "                D, I = faiss_index.search(query_embedding_np, k=10) # D: distances, I: indices\n",
        "                # I contains the internal FAISS indices. We need to map them back to original IDs.\n",
        "                for faiss_idx in I[0]: # I[0] because query_embedding_np was a batch of 1\n",
        "                    if faiss_idx != -1 and faiss_idx < len(faiss_id_order_for_index):\n",
        "                        original_id = faiss_id_order_for_index[faiss_idx]\n",
        "                        semantic_results_docs.append(faiss_contents_map.get(original_id, \"\"))\n",
        "                        semantic_results_metadatas.append(faiss_metadatas_map.get(original_id, {}))\n",
        "                        semantic_results_ids.append(original_id)\n",
        "            except Exception as e:\n",
        "                logging.error(f\"FAISS 检索错误: {str(e)}\")\n",
        "\n",
        "        bm25_results = BM25_MANAGER.search(query, top_k=10) # BM25_MANAGER.search returns list of dicts\n",
        "\n",
        "        # Adapt hybrid_merge to work with current data structures\n",
        "        # It expects semantic_results in a specific format if we pass it directly\n",
        "        # For now, prepare a structure similar to old semantic_results for hybrid_merge\n",
        "        prepared_semantic_results_for_hybrid = {\n",
        "            \"ids\": [semantic_results_ids],\n",
        "            \"documents\": [semantic_results_docs],\n",
        "            \"metadatas\": [semantic_results_metadatas]\n",
        "        }\n",
        "\n",
        "        hybrid_results = hybrid_merge(prepared_semantic_results_for_hybrid, bm25_results, alpha=0.7)\n",
        "\n",
        "        doc_ids_current_iter = []\n",
        "        docs_current_iter = []\n",
        "        metadata_list_current_iter = []\n",
        "\n",
        "        if hybrid_results:\n",
        "            for doc_id, result_data in hybrid_results[:10]: # doc_id here is the original_id\n",
        "                doc_ids_current_iter.append(doc_id)\n",
        "                docs_current_iter.append(result_data['content'])\n",
        "                metadata_list_current_iter.append(result_data['metadata'])\n",
        "\n",
        "        if docs_current_iter:\n",
        "            try:\n",
        "                reranked_results = rerank_results(query, docs_current_iter, doc_ids_current_iter, metadata_list_current_iter, top_k=5)\n",
        "            except Exception as e:\n",
        "                logging.error(f\"重排序错误: {str(e)}\")\n",
        "                reranked_results = [(doc_id, {'content': doc, 'metadata': meta, 'score': 1.0})\n",
        "                                  for doc_id, doc, meta in zip(doc_ids_current_iter, docs_current_iter, metadata_list_current_iter)]\n",
        "        else:\n",
        "            reranked_results = []\n",
        "\n",
        "        current_contexts_for_llm = web_results_texts[:] # Start with web results for LLM context\n",
        "        for doc_id, result_data in reranked_results:\n",
        "            doc = result_data['content']\n",
        "            metadata = result_data['metadata']\n",
        "\n",
        "            if doc_id not in all_doc_ids:\n",
        "                all_doc_ids.append(doc_id)\n",
        "                all_contexts.append(doc)\n",
        "                all_metadata.append(metadata)\n",
        "            current_contexts_for_llm.append(doc) # Add reranked local docs for LLM context\n",
        "\n",
        "        if i == max_iterations - 1:\n",
        "            break\n",
        "\n",
        "        if current_contexts_for_llm: # Use combined web and local context for deciding next query\n",
        "            current_summary = \"\\\\n\".join(current_contexts_for_llm[:3]) if current_contexts_for_llm else \"未找到相关信息\"\n",
        "\n",
        "            next_query_prompt = f\"\"\"基于原始问题: {initial_query}\n",
        "以及已检索信息:\n",
        "{current_summary}\n",
        "\n",
        "分析是否需要进一步查询。如果需要，请提供新的查询问题，使用不同角度或更具体的关键词。\n",
        "如果已经有充分信息，请回复'不需要进一步查询'。\n",
        "\n",
        "新查询(如果需要):\"\"\"\n",
        "\n",
        "            try:\n",
        "                if model_choice == \"siliconflow\":\n",
        "                    logging.info(\"使用SiliconFlow API分析是否需要进一步查询\")\n",
        "                    next_query_result = call_siliconflow_api(next_query_prompt, temperature=0.7, max_tokens=256)\n",
        "                    # SiliconFlow API返回格式包含回答和可能的思维链，这里只需要回答部分来判断是否继续\n",
        "                    # 假设call_siliconflow_api返回的是一个元组 (回答, 思维链) 或只是回答字符串\n",
        "                    if isinstance(next_query_result, tuple):\n",
        "                         next_query = next_query_result[0].strip() # 取回答部分\n",
        "                    else:\n",
        "                         next_query = next_query_result.strip() # 如果只返回字符串\n",
        "\n",
        "                    # 移除潜在的思维链标记\n",
        "                    if \"<think>\" in next_query:\n",
        "                        next_query = next_query.split(\"<think>\")[0].strip()\n",
        "\n",
        "\n",
        "                else:\n",
        "                    logging.info(\"使用本地Ollama模型分析是否需要进一步查询\")\n",
        "                    response = session.post(\n",
        "                        \"http://localhost:11434/api/generate\",\n",
        "                        json={\n",
        "                            \"model\": \"deepseek-r1:1.5b\",\n",
        "                            \"prompt\": next_query_prompt,\n",
        "                            \"stream\": False\n",
        "                        },\n",
        "                        timeout=30\n",
        "                    )\n",
        "                    # Ollama 返回格式不同，需要根据实际情况提取\n",
        "                    next_query = response.json().get(\"response\", \"\").strip()\n",
        "\n",
        "                if \"不需要\" in next_query or \"不需要进一步查询\" in next_query or len(next_query) < 5:\n",
        "                    logging.info(\"LLM判断不需要进一步查询，结束递归检索\")\n",
        "                    break\n",
        "\n",
        "                # 使用新查询继续迭代\n",
        "                query = next_query\n",
        "                logging.info(f\"生成新查询: {query}\")\n",
        "            except Exception as e:\n",
        "                logging.error(f\"生成新查询时出错: {str(e)}\")\n",
        "                break\n",
        "        else:\n",
        "            # 如果当前迭代没有检索到内容，结束迭代\n",
        "            break\n",
        "\n",
        "    return all_contexts, all_doc_ids, all_metadata\n",
        "\n",
        "class BM25IndexManager:\n",
        "    def __init__(self):\n",
        "        self.bm25_index = None\n",
        "        self.doc_mapping = {}  # 映射BM25索引位置到文档ID\n",
        "        self.tokenized_corpus = []\n",
        "        self.raw_corpus = []\n",
        "\n",
        "    def build_index(self, documents, doc_ids):\n",
        "        \"\"\"构建BM25索引\"\"\"\n",
        "        self.raw_corpus = documents\n",
        "        self.doc_mapping = {i: doc_id for i, doc_id in enumerate(doc_ids)}\n",
        "\n",
        "        # 对文档进行分词，使用jieba分词器更适合中文\n",
        "        self.tokenized_corpus = []\n",
        "        for doc in documents:\n",
        "            # 对中文文档进行分词\n",
        "            tokens = list(jieba.cut(doc))\n",
        "            self.tokenized_corpus.append(tokens)\n",
        "\n",
        "        # 创建BM25索引\n",
        "        self.bm25_index = BM25Okapi(self.tokenized_corpus)\n",
        "        return True\n",
        "\n",
        "    def search(self, query, top_k=5):\n",
        "        \"\"\"使用BM25检索相关文档\"\"\"\n",
        "        if not self.bm25_index:\n",
        "            return []\n",
        "\n",
        "        # 对查询进行分词\n",
        "        tokenized_query = list(jieba.cut(query))\n",
        "\n",
        "        # 获取BM25得分\n",
        "        bm25_scores = self.bm25_index.get_scores(tokenized_query)\n",
        "\n",
        "        # 获取得分最高的文档索引\n",
        "        top_indices = np.argsort(bm25_scores)[-top_k:][::-1]\n",
        "\n",
        "        # 返回结果\n",
        "        results = []\n",
        "        for idx in top_indices:\n",
        "            if bm25_scores[idx] > 0:  # 只返回有相关性的结果\n",
        "                results.append({\n",
        "                    'id': self.doc_mapping[idx],\n",
        "                    'score': float(bm25_scores[idx]),\n",
        "                    'content': self.raw_corpus[idx]\n",
        "                })\n",
        "\n",
        "        return results\n",
        "\n",
        "    def clear(self):\n",
        "        \"\"\"清空索引\"\"\"\n",
        "        self.bm25_index = None\n",
        "        self.doc_mapping = {}\n",
        "        self.tokenized_corpus = []\n",
        "        self.raw_corpus = []\n",
        "\n",
        "# 初始化BM25索引管理器\n",
        "BM25_MANAGER = BM25IndexManager()\n",
        "\n",
        "logging.basicConfig(level=logging.INFO)\n",
        "\n",
        "print(\"Gradio version:\", gr.__version__)  # 添加版本输出\n",
        "\n",
        "# 在初始化组件后添加：\n",
        "session = requests.Session()\n",
        "retries = Retry(\n",
        "    total=3,\n",
        "    backoff_factor=0.1,\n",
        "    status_forcelist=[500, 502, 503, 504]\n",
        ")\n",
        "session.mount('http://', HTTPAdapter(max_retries=retries))\n",
        "\n",
        "#########################################\n",
        "# SerpAPI 网络查询及向量化处理函数\n",
        "#########################################\n",
        "def serpapi_search(query: str, num_results: int = 5) -> list:\n",
        "    \"\"\"\n",
        "    执行 SerpAPI 搜索，并返回解析后的结构化结果\n",
        "    \"\"\"\n",
        "    if not SERPAPI_KEY:\n",
        "        raise ValueError(\"未设置 SERPAPI_KEY 环境变量。请在.env文件中设置您的 API 密钥。\")\n",
        "    try:\n",
        "        params = {\n",
        "            \"engine\": SEARCH_ENGINE,\n",
        "            \"q\": query,\n",
        "            \"api_key\": SERPAPI_KEY,\n",
        "            \"num\": num_results,\n",
        "            \"hl\": \"zh-CN\",  # 中文界面\n",
        "            \"gl\": \"cn\"\n",
        "        }\n",
        "        response = requests.get(\"https://serpapi.com/search\", params=params, timeout=15)\n",
        "        response.raise_for_status()\n",
        "        search_data = response.json()\n",
        "        return _parse_serpapi_results(search_data)\n",
        "    except Exception as e:\n",
        "        logging.error(f\"网络搜索失败: {str(e)}\")\n",
        "        return []\n",
        "\n",
        "def _parse_serpapi_results(data: dict) -> list:\n",
        "    \"\"\"解析 SerpAPI 返回的原始数据\"\"\"\n",
        "    results = []\n",
        "    if \"organic_results\" in data:\n",
        "        for item in data[\"organic_results\"]:\n",
        "            result = {\n",
        "                \"title\": item.get(\"title\"),\n",
        "                \"url\": item.get(\"link\"),\n",
        "                \"snippet\": item.get(\"snippet\"),\n",
        "                \"timestamp\": item.get(\"date\")  # 若有时间信息，可选\n",
        "            }\n",
        "            results.append(result)\n",
        "    # 如果有知识图谱信息，也可以添加置顶（可选）\n",
        "    if \"knowledge_graph\" in data:\n",
        "        kg = data[\"knowledge_graph\"]\n",
        "        results.insert(0, {\n",
        "            \"title\": kg.get(\"title\"),\n",
        "            \"url\": kg.get(\"source\", {}).get(\"link\", \"\"),\n",
        "            \"snippet\": kg.get(\"description\"),\n",
        "            \"source\": \"knowledge_graph\"\n",
        "        })\n",
        "    return results\n",
        "\n",
        "def update_web_results(query: str, num_results: int = 5) -> list:\n",
        "    \"\"\"\n",
        "    基于 SerpAPI 搜索结果。注意：此版本不将结果存入FAISS。\n",
        "    它仅返回原始搜索结果。\n",
        "    \"\"\"\n",
        "    results = serpapi_search(query, num_results)\n",
        "    if not results:\n",
        "        logging.info(\"网络搜索没有返回结果或发生错误\")\n",
        "        return []\n",
        "\n",
        "    # 之前这里有删除旧网络结果和添加到ChromaDB的逻辑。\n",
        "    # 由于FAISS IndexFlatL2不支持按ID删除，并且动态添加涉及复杂ID管理，\n",
        "    # 此简化版本不将网络结果添加到FAISS索引。\n",
        "    # 返回原始结果，供调用者决定如何使用（例如，仅作为文本上下文）。\n",
        "    logging.info(f\"网络搜索返回 {len(results)} 条结果，这些结果不会被添加到FAISS索引中。\")\n",
        "    return results # 返回原始SerpAPI结果列表\n",
        "\n",
        "# 检查是否配置了SERPAPI_KEY\n",
        "def check_serpapi_key():\n",
        "    \"\"\"检查是否配置了SERPAPI_KEY\"\"\"\n",
        "    return SERPAPI_KEY is not None and SERPAPI_KEY.strip() != \"\"\n",
        "\n",
        "# 添加文件处理状态跟踪\n",
        "class FileProcessor:\n",
        "    def __init__(self):\n",
        "        self.processed_files = {}  # 存储已处理文件的状态\n",
        "\n",
        "    def clear_files(self):\n",
        "        \"\"\"清空所有文件记录\"\"\"\n",
        "        self.processed_files = {}\n",
        "\n",
        "    def add_file(self, file_name):\n",
        "        self.processed_files[file_name] = {\n",
        "            'status': '等待处理',\n",
        "            'timestamp': datetime.now().strftime('%Y-%m-%d %H:%M:%S'),\n",
        "            'chunks': 0\n",
        "        }\n",
        "\n",
        "    def update_status(self, file_name, status, chunks=None):\n",
        "        if file_name in self.processed_files:\n",
        "            self.processed_files[file_name]['status'] = status\n",
        "            if chunks is not None:\n",
        "                self.processed_files[file_name]['chunks'] = chunks\n",
        "\n",
        "    def get_file_list(self):\n",
        "        return [\n",
        "            f\"📄 {fname} | {info['status']}\"\n",
        "            for fname, info in self.processed_files.items()\n",
        "        ]\n",
        "\n",
        "file_processor = FileProcessor()\n",
        "\n",
        "#########################################\n",
        "# 矛盾检测函数\n",
        "#########################################\n",
        "def detect_conflicts(sources):\n",
        "    \"\"\"精准矛盾检测算法\"\"\"\n",
        "    key_facts = {}\n",
        "    for item in sources:\n",
        "        facts = extract_facts(item['text'] if 'text' in item else item.get('excerpt', ''))\n",
        "        for fact, value in facts.items():\n",
        "            if fact in key_facts:\n",
        "                if key_facts[fact] != value:\n",
        "                    return True\n",
        "            else:\n",
        "                key_facts[fact] = value\n",
        "    return False\n",
        "\n",
        "def extract_facts(text):\n",
        "    \"\"\"从文本提取关键事实（示例逻辑）\"\"\"\n",
        "    facts = {}\n",
        "    # 提取数值型事实\n",
        "    numbers = re.findall(r'\\b\\d{4}年|\\b\\d+%', text)\n",
        "    if numbers:\n",
        "        facts['关键数值'] = numbers\n",
        "    # 提取技术术语\n",
        "    if \"产业图谱\" in text:\n",
        "        facts['技术方法'] = list(set(re.findall(r'[A-Za-z]+模型|[A-Z]{2,}算法', text)))\n",
        "    return facts\n",
        "\n",
        "def evaluate_source_credibility(source):\n",
        "    \"\"\"评估来源可信度\"\"\"\n",
        "    credibility_scores = {\n",
        "        \"gov.cn\": 0.9,\n",
        "        \"edu.cn\": 0.85,\n",
        "        \"weixin\": 0.7,\n",
        "        \"zhihu\": 0.6,\n",
        "        \"baidu\": 0.5\n",
        "    }\n",
        "\n",
        "    url = source.get('url', '')\n",
        "    if not url:\n",
        "        return 0.5  # 默认中等可信度\n",
        "\n",
        "    domain_match = re.search(r'//([^/]+)', url)\n",
        "    if not domain_match:\n",
        "        return 0.5\n",
        "\n",
        "    domain = domain_match.group(1)\n",
        "\n",
        "    # 检查是否匹配任何已知域名\n",
        "    for known_domain, score in credibility_scores.items():\n",
        "        if known_domain in domain:\n",
        "            return score\n",
        "\n",
        "    return 0.5  # 默认中等可信度\n",
        "\n",
        "def extract_text(filepath):\n",
        "    \"\"\"改进的PDF文本提取方法\"\"\"\n",
        "    output = StringIO()\n",
        "    with open(filepath, 'rb') as file:\n",
        "        extract_text_to_fp(file, output)\n",
        "    return output.getvalue()\n",
        "\n",
        "def process_multiple_pdfs(files, progress=gr.Progress()):\n",
        "    \"\"\"处理多个PDF文件\"\"\"\n",
        "    if not files:\n",
        "        return \"请选择要上传的PDF文件\", []\n",
        "\n",
        "    try:\n",
        "        # 清空向量数据库和相关存储\n",
        "        progress(0.1, desc=\"清理历史数据...\")\n",
        "        global faiss_index, faiss_contents_map, faiss_metadatas_map, faiss_id_order_for_index\n",
        "        faiss_index = None\n",
        "        faiss_contents_map = {}\n",
        "        faiss_metadatas_map = {}\n",
        "        faiss_id_order_for_index = []\n",
        "\n",
        "        # 清空BM25索引\n",
        "        BM25_MANAGER.clear()\n",
        "        logging.info(\"成功清理历史FAISS数据和BM25索引\")\n",
        "\n",
        "        # 清空文件处理状态\n",
        "        file_processor.clear_files()\n",
        "\n",
        "        total_files = len(files)\n",
        "        processed_results = []\n",
        "        total_chunks = 0\n",
        "\n",
        "        all_new_chunks = []\n",
        "        all_new_metadatas = []\n",
        "        all_new_original_ids = []\n",
        "\n",
        "        for idx, file in enumerate(files, 1):\n",
        "            try:\n",
        "                file_name = os.path.basename(file.name)\n",
        "                progress((idx-1)/total_files, desc=f\"处理文件 {idx}/{total_files}: {file_name}\")\n",
        "\n",
        "                file_processor.add_file(file_name)\n",
        "                text = extract_text(file.name)\n",
        "\n",
        "                text_splitter = RecursiveCharacterTextSplitter(\n",
        "                    chunk_size=400,\n",
        "                    chunk_overlap=40,\n",
        "                    separators=[\"\\n\\n\", \"\\n\", \"。\", \"，\", \"；\", \"：\", \" \", \"\"]\n",
        "                )\n",
        "                chunks = text_splitter.split_text(text)\n",
        "\n",
        "                if not chunks:\n",
        "                    raise ValueError(\"文档内容为空或无法提取文本\")\n",
        "\n",
        "                doc_id = f\"doc_{int(time.time())}_{idx}\"\n",
        "\n",
        "                # Store chunks and metadatas temporarily before batch embedding\n",
        "                current_file_ids = [f\"{doc_id}_chunk_{i}\" for i in range(len(chunks))]\n",
        "                current_file_metadatas = [{\"source\": file_name, \"doc_id\": doc_id} for _ in chunks]\n",
        "\n",
        "                all_new_chunks.extend(chunks)\n",
        "                all_new_metadatas.extend(current_file_metadatas)\n",
        "                all_new_original_ids.extend(current_file_ids)\n",
        "\n",
        "                total_chunks += len(chunks)\n",
        "                file_processor.update_status(file_name, \"处理完成\", len(chunks))\n",
        "                processed_results.append(f\"✅ {file_name}: 成功处理 {len(chunks)} 个文本块\")\n",
        "\n",
        "            except Exception as e:\n",
        "                error_msg = str(e)\n",
        "                logging.error(f\"处理文件 {file_name} 时出错: {error_msg}\")\n",
        "                file_processor.update_status(file_name, f\"处理失败: {error_msg}\")\n",
        "                processed_results.append(f\"❌ {file_name}: 处理失败 - {error_msg}\")\n",
        "\n",
        "        if all_new_chunks:\n",
        "            progress(0.8, desc=\"生成文本嵌入...\")\n",
        "            embeddings = EMBED_MODEL.encode(all_new_chunks, show_progress_bar=True)\n",
        "            embeddings_np = np.array(embeddings).astype('float32')\n",
        "\n",
        "            progress(0.9, desc=\"构建FAISS索引...\")\n",
        "            if faiss_index is None: # Should always be None here due to clearing\n",
        "                dimension = embeddings_np.shape[1]\n",
        "                faiss_index = faiss.IndexFlatL2(dimension)\n",
        "\n",
        "            faiss_index.add(embeddings_np)\n",
        "\n",
        "            for i, original_id in enumerate(all_new_original_ids):\n",
        "                faiss_contents_map[original_id] = all_new_chunks[i]\n",
        "                faiss_metadatas_map[original_id] = all_new_metadatas[i]\n",
        "            faiss_id_order_for_index.extend(all_new_original_ids) # Keep track of order for FAISS indices\n",
        "            logging.info(f\"FAISS索引构建完成，共索引 {faiss_index.ntotal} 个文本块\")\n",
        "\n",
        "        summary = f\"\\n总计处理 {total_files} 个文件，{total_chunks} 个文本块\"\n",
        "        processed_results.append(summary)\n",
        "\n",
        "        progress(0.95, desc=\"构建BM25检索索引...\")\n",
        "        update_bm25_index() # This will need to use faiss_contents_map\n",
        "\n",
        "        file_list = file_processor.get_file_list()\n",
        "\n",
        "        return \"\\n\".join(processed_results), file_list\n",
        "\n",
        "    except Exception as e:\n",
        "        error_msg = str(e)\n",
        "        logging.error(f\"整体处理过程出错: {error_msg}\")\n",
        "        return f\"处理过程出错: {error_msg}\", []\n",
        "\n",
        "# 新增：交叉编码器重排序函数\n",
        "def rerank_with_cross_encoder(query, docs, doc_ids, metadata_list, top_k=5):\n",
        "    \"\"\"\n",
        "    使用交叉编码器对检索结果进行重排序\n",
        "\n",
        "    参数:\n",
        "        query: 查询字符串\n",
        "        docs: 文档内容列表\n",
        "        doc_ids: 文档ID列表\n",
        "        metadata_list: 元数据列表\n",
        "        top_k: 返回结果数量\n",
        "\n",
        "    返回:\n",
        "        重排序后的结果列表 [(doc_id, {'content': doc, 'metadata': metadata, 'score': score}), ...]\n",
        "    \"\"\"\n",
        "    if not docs:\n",
        "        return []\n",
        "\n",
        "    encoder = get_cross_encoder()\n",
        "    if encoder is None:\n",
        "        logging.warning(\"交叉编码器不可用，跳过重排序\")\n",
        "        # 返回原始顺序（按索引排序）\n",
        "        return [(doc_id, {'content': doc, 'metadata': meta, 'score': 1.0 - idx/len(docs)})\n",
        "                for idx, (doc_id, doc, meta) in enumerate(zip(doc_ids, docs, metadata_list))]\n",
        "\n",
        "    # 准备交叉编码器输入\n",
        "    cross_inputs = [[query, doc] for doc in docs]\n",
        "\n",
        "    try:\n",
        "        # 计算相关性得分\n",
        "        scores = encoder.predict(cross_inputs)\n",
        "\n",
        "        # 组合结果\n",
        "        results = [\n",
        "            (doc_id, {\n",
        "                'content': doc,\n",
        "                'metadata': meta,\n",
        "                'score': float(score)  # 确保是Python原生类型\n",
        "            })\n",
        "            for doc_id, doc, meta, score in zip(doc_ids, docs, metadata_list, scores)\n",
        "        ]\n",
        "\n",
        "        # 按得分排序\n",
        "        results = sorted(results, key=lambda x: x[1]['score'], reverse=True)\n",
        "\n",
        "        # 返回前K个结果\n",
        "        return results[:top_k]\n",
        "    except Exception as e:\n",
        "        logging.error(f\"交叉编码器重排序失败: {str(e)}\")\n",
        "        # 出错时返回原始顺序\n",
        "        return [(doc_id, {'content': doc, 'metadata': meta, 'score': 1.0 - idx/len(docs)})\n",
        "                for idx, (doc_id, doc, meta) in enumerate(zip(doc_ids, docs, metadata_list))]\n",
        "\n",
        "# 新增：LLM相关性评分函数\n",
        "@lru_cache(maxsize=32)\n",
        "def get_llm_relevance_score(query, doc):\n",
        "    \"\"\"\n",
        "    使用LLM对查询和文档的相关性进行评分（带缓存）\n",
        "\n",
        "    参数:\n",
        "        query: 查询字符串\n",
        "        doc: 文档内容\n",
        "\n",
        "    返回:\n",
        "        相关性得分 (0-10)\n",
        "    \"\"\"\n",
        "    try:\n",
        "        # 构建评分提示词\n",
        "        prompt = f\"\"\"给定以下查询和文档片段，评估它们的相关性。\n",
        "        评分标准：0分表示完全不相关，10分表示高度相关。\n",
        "        只需返回一个0-10之间的整数分数，不要有任何其他解释。\n",
        "\n",
        "        查询: {query}\n",
        "\n",
        "        文档片段: {doc}\n",
        "\n",
        "        相关性分数(0-10):\"\"\"\n",
        "\n",
        "        # 调用本地LLM\n",
        "        response = session.post(\n",
        "            \"http://localhost:11434/api/generate\",\n",
        "            json={\n",
        "                \"model\": \"deepseek-r1:1.5b\",  # 使用较小模型进行评分\n",
        "                \"prompt\": prompt,\n",
        "                \"stream\": False\n",
        "            },\n",
        "            timeout=30\n",
        "        )\n",
        "\n",
        "        # 提取得分\n",
        "        result = response.json().get(\"response\", \"\").strip()\n",
        "\n",
        "        # 尝试解析为数字\n",
        "        try:\n",
        "            score = float(result)\n",
        "            # 确保分数在0-10范围内\n",
        "            score = max(0, min(10, score))\n",
        "            return score\n",
        "        except ValueError:\n",
        "            # 如果无法解析为数字，尝试从文本中提取数字\n",
        "            match = re.search(r'\\b([0-9]|10)\\b', result)\n",
        "            if match:\n",
        "                return float(match.group(1))\n",
        "            else:\n",
        "                # 默认返回中等相关性\n",
        "                return 5.0\n",
        "\n",
        "    except Exception as e:\n",
        "        logging.error(f\"LLM评分失败: {str(e)}\")\n",
        "        # 默认返回中等相关性\n",
        "        return 5.0\n",
        "\n",
        "def rerank_with_llm(query, docs, doc_ids, metadata_list, top_k=5):\n",
        "    \"\"\"\n",
        "    使用LLM对检索结果进行重排序\n",
        "\n",
        "    参数:\n",
        "        query: 查询字符串\n",
        "        docs: 文档内容列表\n",
        "        doc_ids: 文档ID列表\n",
        "        metadata_list: 元数据列表\n",
        "        top_k: 返回结果数量\n",
        "\n",
        "    返回:\n",
        "        重排序后的结果列表\n",
        "    \"\"\"\n",
        "    if not docs:\n",
        "        return []\n",
        "\n",
        "    results = []\n",
        "\n",
        "    # 对每个文档进行评分\n",
        "    for doc_id, doc, meta in zip(doc_ids, docs, metadata_list):\n",
        "        # 获取LLM评分\n",
        "        score = get_llm_relevance_score(query, doc)\n",
        "\n",
        "        # 添加到结果列表\n",
        "        results.append((doc_id, {\n",
        "            'content': doc,\n",
        "            'metadata': meta,\n",
        "            'score': score / 10.0  # 归一化到0-1\n",
        "        }))\n",
        "\n",
        "    # 按得分排序\n",
        "    results = sorted(results, key=lambda x: x[1]['score'], reverse=True)\n",
        "\n",
        "    # 返回前K个结果\n",
        "    return results[:top_k]\n",
        "\n",
        "# 新增：通用重排序函数\n",
        "def rerank_results(query, docs, doc_ids, metadata_list, method=None, top_k=5):\n",
        "    \"\"\"\n",
        "    对检索结果进行重排序\n",
        "\n",
        "    参数:\n",
        "        query: 查询字符串\n",
        "        docs: 文档内容列表\n",
        "        doc_ids: 文档ID列表\n",
        "        metadata_list: 元数据列表\n",
        "        method: 重排序方法 (\"cross_encoder\", \"llm\" 或 None)\n",
        "        top_k: 返回结果数量\n",
        "\n",
        "    返回:\n",
        "        重排序后的结果\n",
        "    \"\"\"\n",
        "    # 如果未指定方法，使用全局配置\n",
        "    if method is None:\n",
        "        method = RERANK_METHOD\n",
        "\n",
        "    # 根据方法选择重排序函数\n",
        "    if method == \"llm\":\n",
        "        return rerank_with_llm(query, docs, doc_ids, metadata_list, top_k)\n",
        "    elif method == \"cross_encoder\":\n",
        "        return rerank_with_cross_encoder(query, docs, doc_ids, metadata_list, top_k)\n",
        "    else:\n",
        "        # 默认不进行重排序，按原始顺序返回\n",
        "        return [(doc_id, {'content': doc, 'metadata': meta, 'score': 1.0 - idx/len(docs)})\n",
        "                for idx, (doc_id, doc, meta) in enumerate(zip(doc_ids, docs, metadata_list))]\n",
        "\n",
        "def stream_answer(question, enable_web_search=False, model_choice=\"ollama\", progress=gr.Progress()):\n",
        "    \"\"\"改进的流式问答处理流程，支持联网搜索、混合检索和重排序，以及多种模型选择\"\"\"\n",
        "    global faiss_index # 确保可以访问\n",
        "    try:\n",
        "        # 检查向量数据库是否为空\n",
        "        knowledge_base_exists = faiss_index is not None and faiss_index.ntotal > 0\n",
        "        if not knowledge_base_exists:\n",
        "                if not enable_web_search:\n",
        "                    yield \"⚠️ 知识库为空，请先上传文档。\", \"遇到错误\"\n",
        "                    return\n",
        "                else:\n",
        "                    logging.warning(\"知识库为空，将仅使用网络搜索结果\")\n",
        "\n",
        "        progress(0.3, desc=\"执行递归检索...\")\n",
        "        # 使用递归检索获取更全面的答案上下文\n",
        "        all_contexts, all_doc_ids, all_metadata = recursive_retrieval(\n",
        "            initial_query=question,\n",
        "            max_iterations=3,\n",
        "            enable_web_search=enable_web_search,\n",
        "            model_choice=model_choice\n",
        "        )\n",
        "\n",
        "        # 组合上下文，包含来源信息\n",
        "        context_with_sources = []\n",
        "        sources_for_conflict_detection = []\n",
        "\n",
        "        # 使用检索到的结果构建上下文\n",
        "        for doc, doc_id, metadata in zip(all_contexts, all_doc_ids, all_metadata):\n",
        "            source_type = metadata.get('source', '本地文档')\n",
        "\n",
        "            source_item = {\n",
        "                'text': doc,\n",
        "                'type': source_type\n",
        "            }\n",
        "\n",
        "            if source_type == 'web':\n",
        "                url = metadata.get('url', '未知URL')\n",
        "                title = metadata.get('title', '未知标题')\n",
        "                context_with_sources.append(f\"[网络来源: {title}] (URL: {url})\\n{doc}\")\n",
        "                source_item['url'] = url\n",
        "                source_item['title'] = title\n",
        "            else:\n",
        "                source = metadata.get('source', '未知来源')\n",
        "                context_with_sources.append(f\"[本地文档: {source}]\\n{doc}\")\n",
        "                source_item['source'] = source\n",
        "\n",
        "            sources_for_conflict_detection.append(source_item)\n",
        "\n",
        "        # 检测矛盾\n",
        "        conflict_detected = detect_conflicts(sources_for_conflict_detection)\n",
        "\n",
        "        # 获取可信源\n",
        "        if conflict_detected:\n",
        "            credible_sources = [s for s in sources_for_conflict_detection\n",
        "                               if s['type'] == 'web' and evaluate_source_credibility(s) > 0.7]\n",
        "\n",
        "        context = \"\\n\\n\".join(context_with_sources)\n",
        "\n",
        "        # 添加时间敏感检测\n",
        "        time_sensitive = any(word in question for word in [\"最新\", \"今年\", \"当前\", \"最近\", \"刚刚\"])\n",
        "\n",
        "        # 改进提示词模板，提高回答质量\n",
        "        prompt_template = \"\"\"作为一个专业的问答助手，你需要基于以下{context_type}回答用户问题。\n",
        "\n",
        "提供的参考内容：\n",
        "{context}\n",
        "\n",
        "用户问题：{question}\n",
        "\n",
        "请遵循以下回答原则：\n",
        "1. 仅基于提供的参考内容回答问题，不要使用你自己的知识\n",
        "2. 如果参考内容中没有足够信息，请坦诚告知你无法回答\n",
        "3. 回答应该全面、准确、有条理，并使用适当的段落和结构\n",
        "4. 请用中文回答\n",
        "5. 在回答末尾标注信息来源{time_instruction}{conflict_instruction}\n",
        "\n",
        "请现在开始回答：\"\"\"\n",
        "\n",
        "        prompt = prompt_template.format(\n",
        "            context_type=\"本地文档和网络搜索结果\" if enable_web_search and knowledge_base_exists else (\"网络搜索结果\" if enable_web_search else \"本地文档\"),\n",
        "            context=context if context else (\"网络搜索结果将用于回答。\" if enable_web_search and not knowledge_base_exists else \"知识库为空或未找到相关内容。\"),\n",
        "            question=question,\n",
        "            time_instruction=\"，优先使用最新的信息\" if time_sensitive and enable_web_search else \"\",\n",
        "            conflict_instruction=\"，并明确指出不同来源的差异\" if conflict_detected else \"\"\n",
        "        )\n",
        "\n",
        "        progress(0.7, desc=\"生成回答...\")\n",
        "        full_answer = \"\"\n",
        "\n",
        "        # 根据模型选择使用不同的API\n",
        "        if model_choice == \"siliconflow\":\n",
        "            # 对于SiliconFlow API，不支持流式响应，所以一次性获取\n",
        "            progress(0.8, desc=\"通过SiliconFlow API生成回答...\")\n",
        "            full_answer = call_siliconflow_api(prompt, temperature=0.7, max_tokens=1536)\n",
        "\n",
        "            # 处理思维链\n",
        "            if \"<think>\" in full_answer and \"</think>\" in full_answer:\n",
        "                processed_answer = process_thinking_content(full_answer)\n",
        "            else:\n",
        "                processed_answer = full_answer\n",
        "\n",
        "            yield processed_answer, \"完成!\"\n",
        "        else:\n",
        "            # 使用本地Ollama模型的流式响应\n",
        "            response = session.post(\n",
        "                \"http://localhost:11434/api/generate\",\n",
        "                json={\n",
        "                    \"model\": \"deepseek-r1:1.5b\",\n",
        "                    \"prompt\": prompt,\n",
        "                    \"stream\": True\n",
        "                },\n",
        "                timeout=120,\n",
        "                stream=True\n",
        "            )\n",
        "\n",
        "            for line in response.iter_lines():\n",
        "                if line:\n",
        "                    chunk = json.loads(line.decode()).get(\"response\", \"\")\n",
        "                    full_answer += chunk\n",
        "\n",
        "                    # 检查是否有完整的思维链标签可以处理\n",
        "                    if \"<think>\" in full_answer and \"</think>\" in full_answer:\n",
        "                        # 需要确保完整收集一个思维链片段后再显示\n",
        "                        processed_answer = process_thinking_content(full_answer)\n",
        "                    else:\n",
        "                        processed_answer = full_answer\n",
        "\n",
        "                    yield processed_answer, \"生成回答中...\"\n",
        "\n",
        "            # 处理最终输出，确保应用思维链处理\n",
        "            final_answer = process_thinking_content(full_answer)\n",
        "            yield final_answer, \"完成!\"\n",
        "\n",
        "    except Exception as e:\n",
        "        yield f\"系统错误: {str(e)}\", \"遇到错误\"\n",
        "\n",
        "def query_answer(question, enable_web_search=False, model_choice=\"ollama\", progress=gr.Progress()):\n",
        "    \"\"\"问答处理流程，支持联网搜索、混合检索和重排序，以及多种模型选择\"\"\"\n",
        "    global faiss_index # 确保可以访问\n",
        "    try:\n",
        "        logging.info(f\"收到问题：{question}，联网状态：{enable_web_search}，模型选择：{model_choice}\")\n",
        "\n",
        "        # 检查向量数据库是否为空\n",
        "        knowledge_base_exists = faiss_index is not None and faiss_index.ntotal > 0\n",
        "        if not knowledge_base_exists:\n",
        "                if not enable_web_search:\n",
        "                    return \"⚠️ 知识库为空，请先上传文档。\"\n",
        "                else:\n",
        "                    logging.warning(\"知识库为空，将仅使用网络搜索结果\")\n",
        "\n",
        "        progress(0.3, desc=\"执行递归检索...\")\n",
        "        # 使用递归检索获取更全面的答案上下文\n",
        "        all_contexts, all_doc_ids, all_metadata = recursive_retrieval(\n",
        "            initial_query=question,\n",
        "            max_iterations=3,\n",
        "            enable_web_search=enable_web_search,\n",
        "            model_choice=model_choice\n",
        "        )\n",
        "\n",
        "        # 组合上下文，包含来源信息\n",
        "        context_with_sources = []\n",
        "        sources_for_conflict_detection = []\n",
        "\n",
        "        # 使用检索到的结果构建上下文\n",
        "        for doc, doc_id, metadata in zip(all_contexts, all_doc_ids, all_metadata):\n",
        "            source_type = metadata.get('source', '本地文档')\n",
        "\n",
        "            source_item = {\n",
        "                'text': doc,\n",
        "                'type': source_type\n",
        "            }\n",
        "\n",
        "            if source_type == 'web':\n",
        "                url = metadata.get('url', '未知URL')\n",
        "                title = metadata.get('title', '未知标题')\n",
        "                context_with_sources.append(f\"[网络来源: {title}] (URL: {url})\\n{doc}\")\n",
        "                source_item['url'] = url\n",
        "                source_item['title'] = title\n",
        "            else:\n",
        "                source = metadata.get('source', '未知来源')\n",
        "                context_with_sources.append(f\"[本地文档: {source}]\\n{doc}\")\n",
        "                source_item['source'] = source\n",
        "\n",
        "            sources_for_conflict_detection.append(source_item)\n",
        "\n",
        "        # 检测矛盾\n",
        "        conflict_detected = detect_conflicts(sources_for_conflict_detection)\n",
        "\n",
        "        # 获取可信源\n",
        "        if conflict_detected:\n",
        "            credible_sources = [s for s in sources_for_conflict_detection\n",
        "                              if s['type'] == 'web' and evaluate_source_credibility(s) > 0.7]\n",
        "\n",
        "        context = \"\\n\\n\".join(context_with_sources)\n",
        "\n",
        "        # 添加时间敏感检测\n",
        "        time_sensitive = any(word in question for word in [\"最新\", \"今年\", \"当前\", \"最近\", \"刚刚\"])\n",
        "\n",
        "        # 改进提示词模板，提高回答质量\n",
        "        prompt_template = \"\"\"作为一个专业的问答助手，你需要基于以下{context_type}回答用户问题。\n",
        "\n",
        "提供的参考内容：\n",
        "{context}\n",
        "\n",
        "用户问题：{question}\n",
        "\n",
        "请遵循以下回答原则：\n",
        "1. 仅基于提供的参考内容回答问题，不要使用你自己的知识\n",
        "2. 如果参考内容中没有足够信息，请坦诚告知你无法回答\n",
        "3. 回答应该全面、准确、有条理，并使用适当的段落和结构\n",
        "4. 请用中文回答\n",
        "5. 在回答末尾标注信息来源{time_instruction}{conflict_instruction}\n",
        "\n",
        "请现在开始回答：\"\"\"\n",
        "\n",
        "        prompt = prompt_template.format(\n",
        "            context_type=\"本地文档和网络搜索结果\" if enable_web_search and knowledge_base_exists else (\"网络搜索结果\" if enable_web_search else \"本地文档\"),\n",
        "            context=context if context else (\"网络搜索结果将用于回答。\" if enable_web_search and not knowledge_base_exists else \"知识库为空或未找到相关内容。\"),\n",
        "            question=question,\n",
        "            time_instruction=\"，优先使用最新的信息\" if time_sensitive and enable_web_search else \"\",\n",
        "            conflict_instruction=\"，并明确指出不同来源的差异\" if conflict_detected else \"\"\n",
        "        )\n",
        "\n",
        "        progress(0.8, desc=\"生成回答...\")\n",
        "\n",
        "        # 根据模型选择使用不同的API\n",
        "        if model_choice == \"siliconflow\":\n",
        "            # 使用SiliconFlow API\n",
        "            result = call_siliconflow_api(prompt, temperature=0.7, max_tokens=1536)\n",
        "\n",
        "            # 处理思维链\n",
        "            processed_result = process_thinking_content(result)\n",
        "            return processed_result\n",
        "        else:\n",
        "            # 使用本地Ollama\n",
        "            response = session.post(\n",
        "                \"http://localhost:11434/api/generate\",\n",
        "                json={\n",
        "                    \"model\": \"deepseek-r1:7b\",\n",
        "                    \"prompt\": prompt,\n",
        "                    \"stream\": False\n",
        "                },\n",
        "                timeout=120,  # 延长到2分钟\n",
        "                headers={'Connection': 'close'}  # 添加连接头\n",
        "            )\n",
        "            response.raise_for_status()  # 检查HTTP状态码\n",
        "\n",
        "            progress(1.0, desc=\"完成!\")\n",
        "            # 确保返回字符串并处理空值\n",
        "            result = response.json()\n",
        "            return process_thinking_content(str(result.get(\"response\", \"未获取到有效回答\")))\n",
        "\n",
        "    except json.JSONDecodeError:\n",
        "        return \"响应解析失败，请重试\"\n",
        "    except KeyError:\n",
        "        return \"响应格式异常，请检查模型服务\"\n",
        "    except Exception as e:\n",
        "        progress(1.0, desc=\"遇到错误\")  # 确保进度条完成\n",
        "        return f\"系统错误: {str(e)}\"\n",
        "\n",
        "def process_thinking_content(text):\n",
        "    \"\"\"处理包含<think>标签的内容，将其转换为Markdown格式\"\"\"\n",
        "    # 检查输入是否为有效文本\n",
        "    if text is None:\n",
        "        return \"\"\n",
        "\n",
        "    # 确保输入是字符串\n",
        "    if not isinstance(text, str):\n",
        "        try:\n",
        "            processed_text = str(text)\n",
        "        except:\n",
        "            return \"无法处理的内容格式\"\n",
        "    else:\n",
        "        processed_text = text\n",
        "\n",
        "    # 处理思维链标签\n",
        "    try:\n",
        "        while \"<think>\" in processed_text and \"</think>\" in processed_text:\n",
        "            start_idx = processed_text.find(\"<think>\")\n",
        "            end_idx = processed_text.find(\"</think>\")\n",
        "            if start_idx != -1 and end_idx != -1 and end_idx > start_idx:\n",
        "                thinking_content = processed_text[start_idx + 7:end_idx]\n",
        "                before_think = processed_text[:start_idx]\n",
        "                after_think = processed_text[end_idx + 8:]\n",
        "\n",
        "                # 使用可折叠详情框显示思维链\n",
        "                processed_text = before_think + \"\\n\\n<details>\\n<summary>思考过程（点击展开）</summary>\\n\\n\" + thinking_content + \"\\n\\n</details>\\n\\n\" + after_think\n",
        "\n",
        "        # 处理其他HTML标签，但保留details和summary标签\n",
        "        processed_html = []\n",
        "        i = 0\n",
        "        while i < len(processed_text):\n",
        "            if processed_text[i:i+8] == \"<details\" or processed_text[i:i+9] == \"</details\" or \\\n",
        "               processed_text[i:i+8] == \"<summary\" or processed_text[i:i+9] == \"</summary\":\n",
        "                # 保留这些标签\n",
        "                tag_end = processed_text.find(\">\", i)\n",
        "                if tag_end != -1:\n",
        "                    processed_html.append(processed_text[i:tag_end+1])\n",
        "                    i = tag_end + 1\n",
        "                    continue\n",
        "\n",
        "            if processed_text[i] == \"<\":\n",
        "                processed_html.append(\"&lt;\")\n",
        "            elif processed_text[i] == \">\":\n",
        "                processed_html.append(\"&gt;\")\n",
        "            else:\n",
        "                processed_html.append(processed_text[i])\n",
        "            i += 1\n",
        "\n",
        "        processed_text = \"\".join(processed_html)\n",
        "    except Exception as e:\n",
        "        logging.error(f\"处理思维链内容时出错: {str(e)}\")\n",
        "        # 出错时至少返回原始文本，但确保安全处理HTML标签\n",
        "        try:\n",
        "            return text.replace(\"<\", \"&lt;\").replace(\">\", \"&gt;\")\n",
        "        except:\n",
        "            return \"处理内容时出错\"\n",
        "\n",
        "    return processed_text\n",
        "\n",
        "def call_siliconflow_api(prompt, temperature=0.7, max_tokens=1024):\n",
        "    \"\"\"\n",
        "    调用SiliconFlow API获取回答\n",
        "\n",
        "    Args:\n",
        "        prompt: 提示词\n",
        "        temperature: 温度参数\n",
        "        max_tokens: 最大生成token数\n",
        "\n",
        "    Returns:\n",
        "        生成的回答文本和思维链内容\n",
        "    \"\"\"\n",
        "    # 检查是否配置了SiliconFlow API密钥\n",
        "    if not SILICONFLOW_API_KEY:\n",
        "        logging.error(\"未设置 SILICONFLOW_API_KEY 环境变量。请在.env文件中设置您的 API 密钥。\")\n",
        "        return \"错误：未配置 SiliconFlow API 密钥。\", \"\"\n",
        "\n",
        "    try:\n",
        "        payload = {\n",
        "            \"model\": \"Pro/deepseek-ai/DeepSeek-R1\",\n",
        "            \"messages\": [\n",
        "                {\n",
        "                    \"role\": \"user\",\n",
        "                    \"content\": prompt\n",
        "                }\n",
        "            ],\n",
        "            \"stream\": False,\n",
        "            \"max_tokens\": max_tokens,\n",
        "            \"stop\": None,\n",
        "            \"temperature\": temperature,\n",
        "            \"top_p\": 0.7,\n",
        "            \"top_k\": 50,\n",
        "            \"frequency_penalty\": 0.5,\n",
        "            \"n\": 1,\n",
        "            \"response_format\": {\"type\": \"text\"}\n",
        "        }\n",
        "\n",
        "        headers = {\n",
        "            \"Authorization\": f\"Bearer {SILICONFLOW_API_KEY}\", # 从环境变量获取密钥\n",
        "            \"Content-Type\": \"application/json; charset=utf-8\" # 明确指定编码\n",
        "        }\n",
        "\n",
        "        # 手动将payload编码为UTF-8 JSON字符串\n",
        "        json_payload = json.dumps(payload, ensure_ascii=False).encode('utf-8')\n",
        "\n",
        "        response = requests.post(\n",
        "            SILICONFLOW_API_URL,\n",
        "            data=json_payload, # 通过data参数发送编码后的JSON\n",
        "            headers=headers,\n",
        "            timeout=60  # 延长超时时间\n",
        "        )\n",
        "\n",
        "        response.raise_for_status()\n",
        "        result = response.json()\n",
        "\n",
        "        # 提取回答内容和思维链\n",
        "        if \"choices\" in result and len(result[\"choices\"]) > 0:\n",
        "            message = result[\"choices\"][0][\"message\"]\n",
        "            content = message.get(\"content\", \"\")\n",
        "            reasoning = message.get(\"reasoning_content\", \"\")\n",
        "\n",
        "            # 如果有思维链，则添加特殊标记，以便前端处理\n",
        "            if reasoning:\n",
        "                # 添加思维链标记\n",
        "                full_response = f\"{content}<think>{reasoning}</think>\"\n",
        "                return full_response\n",
        "            else:\n",
        "                return content\n",
        "        else:\n",
        "            return \"API返回结果格式异常，请检查\"\n",
        "\n",
        "    except requests.exceptions.RequestException as e:\n",
        "        logging.error(f\"调用SiliconFlow API时出错: {str(e)}\")\n",
        "        return f\"调用API时出错: {str(e)}\"\n",
        "    except json.JSONDecodeError:\n",
        "        logging.error(\"SiliconFlow API返回非JSON响应\")\n",
        "        return \"API响应解析失败\"\n",
        "    except Exception as e:\n",
        "        logging.error(f\"调用SiliconFlow API时发生未知错误: {str(e)}\")\n",
        "        return f\"发生未知错误: {str(e)}\"\n",
        "\n",
        "def hybrid_merge(semantic_results, bm25_results, alpha=0.7):\n",
        "    \"\"\"\n",
        "    合并语义搜索和BM25搜索结果\n",
        "\n",
        "    参数:\n",
        "        semantic_results: 向量检索结果 (字典格式，包含ids, documents, metadatas)\n",
        "        bm25_results: BM25检索结果 (字典列表，包含id, score, content)\n",
        "        alpha: 语义搜索权重 (0-1)\n",
        "\n",
        "    返回:\n",
        "        合并后的结果列表 [(doc_id, {'score': score, 'content': content, 'metadata': metadata}), ...]\n",
        "    \"\"\"\n",
        "    merged_dict = {}\n",
        "    global faiss_metadatas_map # Ensure we can access the global map\n",
        "\n",
        "    # 处理语义搜索结果\n",
        "    if (semantic_results and\n",
        "        isinstance(semantic_results.get('documents'), list) and len(semantic_results['documents']) > 0 and\n",
        "        isinstance(semantic_results.get('metadatas'), list) and len(semantic_results['metadatas']) > 0 and\n",
        "        isinstance(semantic_results.get('ids'), list) and len(semantic_results['ids']) > 0 and\n",
        "        isinstance(semantic_results['documents'][0], list) and\n",
        "        isinstance(semantic_results['metadatas'][0], list) and\n",
        "        isinstance(semantic_results['ids'][0], list) and\n",
        "        len(semantic_results['documents'][0]) == len(semantic_results['metadatas'][0]) == len(semantic_results['ids'][0])):\n",
        "\n",
        "        num_results = len(semantic_results['documents'][0])\n",
        "        # Assuming semantic_results are already ordered by relevance (higher is better)\n",
        "        # A simple rank-based score, can be replaced if actual scores/distances are available and preferred\n",
        "        for i, (doc_id, doc, meta) in enumerate(zip(semantic_results['ids'][0], semantic_results['documents'][0], semantic_results['metadatas'][0])):\n",
        "            score = 1.0 - (i / max(1, num_results)) # Higher rank (smaller i) gets higher score\n",
        "            merged_dict[doc_id] = {\n",
        "                'score': alpha * score,\n",
        "                'content': doc,\n",
        "                'metadata': meta\n",
        "            }\n",
        "    else:\n",
        "        logging.warning(\"Semantic results are missing, have an unexpected format, or are empty. Skipping semantic part in hybrid merge.\")\n",
        "\n",
        "    # 处理BM25结果\n",
        "    if not bm25_results:\n",
        "        return sorted(merged_dict.items(), key=lambda x: x[1]['score'], reverse=True)\n",
        "\n",
        "    valid_bm25_scores = [r['score'] for r in bm25_results if isinstance(r, dict) and 'score' in r]\n",
        "    max_bm25_score = max(valid_bm25_scores) if valid_bm25_scores else 1.0\n",
        "\n",
        "    for result in bm25_results:\n",
        "        if not (isinstance(result, dict) and 'id' in result and 'score' in result and 'content' in result):\n",
        "            logging.warning(f\"Skipping invalid BM25 result item: {result}\")\n",
        "            continue\n",
        "\n",
        "        doc_id = result['id']\n",
        "        # Normalize BM25 score\n",
        "        normalized_score = result['score'] / max_bm25_score if max_bm25_score > 0 else 0\n",
        "\n",
        "        if doc_id in merged_dict:\n",
        "            merged_dict[doc_id]['score'] += (1 - alpha) * normalized_score\n",
        "        else:\n",
        "            metadata = faiss_metadatas_map.get(doc_id, {}) # Get metadata from our global map\n",
        "            merged_dict[doc_id] = {\n",
        "                'score': (1 - alpha) * normalized_score,\n",
        "                'content': result['content'],\n",
        "                'metadata': metadata\n",
        "            }\n",
        "\n",
        "    merged_results = sorted(merged_dict.items(), key=lambda x: x[1]['score'], reverse=True)\n",
        "    return merged_results\n",
        "\n",
        "# 新增：更新本地文档的BM25索引\n",
        "def update_bm25_index():\n",
        "    \"\"\"更新BM25索引，从内存中的映射加载所有文档\"\"\"\n",
        "    global faiss_contents_map, faiss_id_order_for_index\n",
        "    try:\n",
        "        # Use the ordered list of IDs to ensure consistency\n",
        "        doc_ids = faiss_id_order_for_index\n",
        "        if not doc_ids:\n",
        "            logging.warning(\"没有可索引的文档 (FAISS ID列表为空)\")\n",
        "            BM25_MANAGER.clear()\n",
        "            return False\n",
        "\n",
        "        # Retrieve documents in the correct order\n",
        "        documents = [faiss_contents_map.get(doc_id, \"\") for doc_id in doc_ids]\n",
        "\n",
        "        # Filter out any potential empty documents if necessary, though map access should be safe\n",
        "        valid_docs_with_ids = [(doc_id, doc) for doc_id, doc in zip(doc_ids, documents) if doc]\n",
        "        if not valid_docs_with_ids:\n",
        "            logging.warning(\"没有有效的文档内容可用于BM25索引\")\n",
        "            BM25_MANAGER.clear()\n",
        "            return False\n",
        "\n",
        "        # Separate IDs and documents again for building the index\n",
        "        final_doc_ids = [item[0] for item in valid_docs_with_ids]\n",
        "        final_documents = [item[1] for item in valid_docs_with_ids]\n",
        "\n",
        "        BM25_MANAGER.build_index(final_documents, final_doc_ids)\n",
        "        logging.info(f\"BM25索引更新完成，共索引 {len(final_doc_ids)} 个文档\")\n",
        "        return True\n",
        "    except Exception as e:\n",
        "        logging.error(f\"更新BM25索引失败: {str(e)}\")\n",
        "        return False\n",
        "\n",
        "# 新增函数：获取系统使用的模型信息\n",
        "def get_system_models_info():\n",
        "    \"\"\"返回系统使用的各种模型信息\"\"\"\n",
        "    models_info = {\n",
        "        \"嵌入模型\": \"all-MiniLM-L6-v2\",\n",
        "        \"分块方法\": \"RecursiveCharacterTextSplitter (chunk_size=800, overlap=150)\",\n",
        "        \"检索方法\": \"向量检索 + BM25混合检索 (α=0.7)\",\n",
        "        \"重排序模型\": \"交叉编码器 (sentence-transformers/distiluse-base-multilingual-cased-v2)\",\n",
        "        \"生成模型\": \"deepseek-r1 (7B/1.5B)\",\n",
        "        \"分词工具\": \"jieba (中文分词)\"\n",
        "    }\n",
        "    return models_info\n",
        "\n",
        "# 新增函数：获取文档分块可视化数据\n",
        "def get_document_chunks(progress=gr.Progress()):\n",
        "    \"\"\"获取文档分块结果用于可视化\"\"\"\n",
        "    global faiss_contents_map, faiss_metadatas_map, faiss_id_order_for_index\n",
        "    global chunk_data_cache # Ensure we can update the global cache\n",
        "    try:\n",
        "        progress(0.1, desc=\"正在从内存加载数据...\")\n",
        "\n",
        "        if not faiss_id_order_for_index:\n",
        "            chunk_data_cache = []\n",
        "            return [], \"知识库中没有文档，请先上传并处理文档。\"\n",
        "\n",
        "        progress(0.5, desc=\"正在组织分块数据...\")\n",
        "\n",
        "        doc_groups = {}\n",
        "        # Iterate using the ordered IDs to reflect the FAISS index order conceptually\n",
        "        for doc_id in faiss_id_order_for_index:\n",
        "            doc = faiss_contents_map.get(doc_id, \"\")\n",
        "            meta = faiss_metadatas_map.get(doc_id, {})\n",
        "            if not doc: # Skip if content is somehow empty\n",
        "                continue\n",
        "\n",
        "            source = meta.get('source', '未知来源')\n",
        "            if source not in doc_groups:\n",
        "                doc_groups[source] = []\n",
        "\n",
        "            doc_id_meta = meta.get('doc_id', '未知ID') # Get the original document ID from meta\n",
        "            chunk_info = {\n",
        "                \"original_id\": doc_id, # Keep the chunk-specific ID\n",
        "                \"doc_id\": doc_id_meta,\n",
        "                \"content\": doc[:200] + \"...\" if len(doc) > 200 else doc,\n",
        "                \"full_content\": doc,\n",
        "                \"token_count\": len(list(jieba.cut(doc))),\n",
        "                \"char_count\": len(doc)\n",
        "            }\n",
        "            doc_groups[source].append(chunk_info)\n",
        "\n",
        "        result_dicts = []\n",
        "        result_lists = []\n",
        "\n",
        "        # Keep track of chunks per source for indexing\n",
        "        source_chunk_counters = {source: 0 for source in doc_groups.keys()}\n",
        "        total_chunks = 0\n",
        "\n",
        "        for source, chunks in doc_groups.items():\n",
        "            num_chunks_in_source = len(chunks)\n",
        "            for chunk in chunks:\n",
        "                source_chunk_counters[source] += 1\n",
        "                total_chunks += 1\n",
        "                result_dict = {\n",
        "                    \"来源\": source,\n",
        "                    \"序号\": f\"{source_chunk_counters[source]}/{num_chunks_in_source}\",\n",
        "                    \"字符数\": chunk[\"char_count\"],\n",
        "                    \"分词数\": chunk[\"token_count\"],\n",
        "                    \"内容预览\": chunk[\"content\"],\n",
        "                    \"完整内容\": chunk[\"full_content\"],\n",
        "                    \"原始分块ID\": chunk[\"original_id\"] # Add original ID for potential debugging\n",
        "                }\n",
        "                result_dicts.append(result_dict)\n",
        "\n",
        "                result_lists.append([\n",
        "                    source,\n",
        "                    f\"{source_chunk_counters[source]}/{num_chunks_in_source}\",\n",
        "                    chunk[\"char_count\"],\n",
        "                    chunk[\"token_count\"],\n",
        "                    chunk[\"content\"]\n",
        "                ])\n",
        "\n",
        "        progress(1.0, desc=\"数据加载完成!\")\n",
        "\n",
        "        chunk_data_cache = result_dicts # Update the global cache\n",
        "        summary = f\"总计 {total_chunks} 个文本块，来自 {len(doc_groups)} 个不同来源。\"\n",
        "\n",
        "        return result_lists, summary\n",
        "    except Exception as e:\n",
        "        chunk_data_cache = []\n",
        "        return [], f\"获取分块数据失败: {str(e)}\"\n",
        "\n",
        "# 添加全局缓存变量\n",
        "chunk_data_cache = []\n",
        "\n",
        "# 新增函数：显示分块详情\n",
        "def show_chunk_details(evt: gr.SelectData, chunks):\n",
        "    \"\"\"显示选中分块的详细内容\"\"\"\n",
        "    try:\n",
        "        if evt.index[0] < len(chunk_data_cache):\n",
        "            selected_chunk = chunk_data_cache[evt.index[0]]\n",
        "            return selected_chunk.get(\"完整内容\", \"内容加载失败\")\n",
        "        return \"未找到选中的分块\"\n",
        "    except Exception as e:\n",
        "        return f\"加载分块详情失败: {str(e)}\"\n",
        "\n",
        "# 修改布局部分，添加一个新的标签页\n",
        "with gr.Blocks(\n",
        "    title=\"本地RAG问答系统\",\n",
        "    css=\"\"\"\n",
        "    /* 全局主题变量 */\n",
        "    :root[data-theme=\"light\"] {\n",
        "        --text-color: #2c3e50;\n",
        "        --bg-color: #ffffff;\n",
        "        --panel-bg: #f8f9fa;\n",
        "        --border-color: #e9ecef;\n",
        "        --success-color: #4CAF50;\n",
        "        --error-color: #f44336;\n",
        "        --primary-color: #2196F3;\n",
        "        --secondary-bg: #ffffff;\n",
        "        --hover-color: #e9ecef;\n",
        "        --chat-user-bg: #e3f2fd;\n",
        "        --chat-assistant-bg: #f5f5f5;\n",
        "    }\n",
        "\n",
        "    :root[data-theme=\"dark\"] {\n",
        "        --text-color: #e0e0e0;\n",
        "        --bg-color: #1a1a1a;\n",
        "        --panel-bg: #2d2d2d;\n",
        "        --border-color: #404040;\n",
        "        --success-color: #81c784;\n",
        "        --error-color: #e57373;\n",
        "        --primary-color: #64b5f6;\n",
        "        --secondary-bg: #2d2d2d;\n",
        "        --hover-color: #404040;\n",
        "        --chat-user-bg: #1e3a5f;\n",
        "        --chat-assistant-bg: #2d2d2d;\n",
        "    }\n",
        "\n",
        "    /* 全局样式 */\n",
        "    body {\n",
        "        font-family: -apple-system, BlinkMacSystemFont, \"Segoe UI\", Roboto, \"Helvetica Neue\", Arial, sans-serif;\n",
        "        margin: 0;\n",
        "        padding: 0;\n",
        "        overflow-x: hidden;\n",
        "        width: 100vw;\n",
        "        height: 100vh;\n",
        "    }\n",
        "\n",
        "    .gradio-container {\n",
        "        max-width: 100% !important;\n",
        "        width: 100% !important;\n",
        "        margin: 0 !important;\n",
        "        padding: 0 1% !important;\n",
        "        color: var(--text-color);\n",
        "        background-color: var(--bg-color);\n",
        "        min-height: 100vh;\n",
        "    }\n",
        "\n",
        "    /* 确保标签内容撑满 */\n",
        "    .tabs.svelte-710i53 {\n",
        "        margin: 0 !important;\n",
        "        padding: 0 !important;\n",
        "        width: 100% !important;\n",
        "    }\n",
        "\n",
        "    /* 主题切换按钮 */\n",
        "    .theme-toggle {\n",
        "        position: fixed;\n",
        "        top: 20px;\n",
        "        right: 20px;\n",
        "        z-index: 1000;\n",
        "        padding: 8px 16px;\n",
        "        border-radius: 20px;\n",
        "        border: 1px solid var(--border-color);\n",
        "        background: var(--panel-bg);\n",
        "        color: var(--text-color);\n",
        "        cursor: pointer;\n",
        "        transition: all 0.3s ease;\n",
        "        font-size: 14px;\n",
        "        display: flex;\n",
        "        align-items: center;\n",
        "        gap: 8px;\n",
        "    }\n",
        "\n",
        "    .theme-toggle:hover {\n",
        "        background: var(--hover-color);\n",
        "    }\n",
        "\n",
        "    /* 面板样式 */\n",
        "    .left-panel {\n",
        "        padding-right: 20px;\n",
        "        border-right: 1px solid var(--border-color);\n",
        "        background: var(--bg-color);\n",
        "        width: 100%;\n",
        "    }\n",
        "\n",
        "    .right-panel {\n",
        "        height: 100vh;\n",
        "        background: var(--bg-color);\n",
        "        width: 100%;\n",
        "    }\n",
        "\n",
        "    /* 文件列表样式 */\n",
        "    .file-list {\n",
        "        margin-top: 10px;\n",
        "        padding: 12px;\n",
        "        background: var(--panel-bg);\n",
        "        border-radius: 8px;\n",
        "        font-size: 14px;\n",
        "        line-height: 1.6;\n",
        "        border: 1px solid var(--border-color);\n",
        "    }\n",
        "\n",
        "    /* 答案框样式 */\n",
        "    .answer-box {\n",
        "        min-height: 500px !important;\n",
        "        background: var(--panel-bg);\n",
        "        border-radius: 8px;\n",
        "        padding: 16px;\n",
        "        font-size: 15px;\n",
        "        line-height: 1.6;\n",
        "        border: 1px solid var(--border-color);\n",
        "    }\n",
        "\n",
        "    /* 输入框样式 */\n",
        "    textarea {\n",
        "        background: var(--panel-bg) !important;\n",
        "        color: var(--text-color) !important;\n",
        "        border: 1px solid var(--border-color) !important;\n",
        "        border-radius: 8px !important;\n",
        "        padding: 12px !important;\n",
        "        font-size: 14px !important;\n",
        "    }\n",
        "\n",
        "    /* 按钮样式 */\n",
        "    button.primary {\n",
        "        background: var(--primary-color) !important;\n",
        "        color: white !important;\n",
        "        border-radius: 8px !important;\n",
        "        padding: 8px 16px !important;\n",
        "        font-weight: 500 !important;\n",
        "        transition: all 0.3s ease !important;\n",
        "    }\n",
        "\n",
        "    button.primary:hover {\n",
        "        opacity: 0.9;\n",
        "        transform: translateY(-1px);\n",
        "    }\n",
        "\n",
        "    /* 标题和文本样式 */\n",
        "    h1, h2, h3 {\n",
        "        color: var(--text-color) !important;\n",
        "        font-weight: 600 !important;\n",
        "    }\n",
        "\n",
        "    .footer-note {\n",
        "        color: var(--text-color);\n",
        "        opacity: 0.8;\n",
        "        font-size: 13px;\n",
        "        margin-top: 12px;\n",
        "    }\n",
        "\n",
        "    /* 加载和进度样式 */\n",
        "    #loading, .progress-text {\n",
        "        color: var(--text-color);\n",
        "    }\n",
        "\n",
        "    /* 聊天记录样式 */\n",
        "    .chat-container {\n",
        "        border: 1px solid var(--border-color);\n",
        "        border-radius: 8px;\n",
        "        margin-bottom: 16px;\n",
        "        max-height: 80vh;\n",
        "        height: 80vh !important;\n",
        "        overflow-y: auto;\n",
        "        background: var(--bg-color);\n",
        "    }\n",
        "\n",
        "    .chat-message {\n",
        "        padding: 12px 16px;\n",
        "        margin: 8px;\n",
        "        border-radius: 8px;\n",
        "        font-size: 14px;\n",
        "        line-height: 1.5;\n",
        "    }\n",
        "\n",
        "    .chat-message.user {\n",
        "        background: var(--chat-user-bg);\n",
        "        margin-left: 32px;\n",
        "        border-top-right-radius: 4px;\n",
        "    }\n",
        "\n",
        "    .chat-message.assistant {\n",
        "        background: var(--chat-assistant-bg);\n",
        "        margin-right: 32px;\n",
        "        border-top-left-radius: 4px;\n",
        "    }\n",
        "\n",
        "    .chat-message .timestamp {\n",
        "        font-size: 12px;\n",
        "        color: var(--text-color);\n",
        "        opacity: 0.7;\n",
        "        margin-bottom: 4px;\n",
        "    }\n",
        "\n",
        "    .chat-message .content {\n",
        "        white-space: pre-wrap;\n",
        "    }\n",
        "\n",
        "    /* 按钮组样式 */\n",
        "    .button-row {\n",
        "        display: flex;\n",
        "        gap: 8px;\n",
        "        margin-top: 8px;\n",
        "    }\n",
        "\n",
        "    .clear-button {\n",
        "        background: var(--error-color) !important;\n",
        "    }\n",
        "\n",
        "    /* API配置提示样式 */\n",
        "    .api-info {\n",
        "        margin-top: 10px;\n",
        "        padding: 10px;\n",
        "        border-radius: 5px;\n",
        "        background: var(--panel-bg);\n",
        "        border: 1px solid var(--border-color);\n",
        "    }\n",
        "\n",
        "    /* 新增: 数据可视化卡片样式 */\n",
        "    .model-card {\n",
        "        background: var(--panel-bg);\n",
        "        border-radius: 8px;\n",
        "        padding: 16px;\n",
        "        border: 1px solid var(--border-color);\n",
        "        margin-bottom: 16px;\n",
        "    }\n",
        "\n",
        "    .model-card h3 {\n",
        "        margin-top: 0;\n",
        "        border-bottom: 1px solid var(--border-color);\n",
        "        padding-bottom: 8px;\n",
        "    }\n",
        "\n",
        "    .model-item {\n",
        "        display: flex;\n",
        "        margin-bottom: 8px;\n",
        "    }\n",
        "\n",
        "    .model-item .label {\n",
        "        flex: 1;\n",
        "        font-weight: 500;\n",
        "    }\n",
        "\n",
        "    .model-item .value {\n",
        "        flex: 2;\n",
        "    }\n",
        "\n",
        "    /* 数据表格样式 */\n",
        "    .chunk-table {\n",
        "        border-radius: 8px;\n",
        "        overflow: hidden;\n",
        "        border: 1px solid var(--border-color);\n",
        "    }\n",
        "\n",
        "    .chunk-table th, .chunk-table td {\n",
        "        border: 1px solid var(--border-color);\n",
        "        padding: 8px;\n",
        "    }\n",
        "\n",
        "    .chunk-detail-box {\n",
        "        min-height: 200px;\n",
        "        padding: 16px;\n",
        "        background: var(--panel-bg);\n",
        "        border-radius: 8px;\n",
        "        border: 1px solid var(--border-color);\n",
        "        font-family: monospace;\n",
        "        white-space: pre-wrap;\n",
        "        overflow-y: auto;\n",
        "    }\n",
        "    \"\"\"\n",
        ") as demo:\n",
        "    gr.Markdown(\"# 🧠 智能文档问答系统\")\n",
        "\n",
        "    with gr.Tabs() as tabs:\n",
        "        # 第一个选项卡：问答对话\n",
        "        with gr.TabItem(\"💬 问答对话\"):\n",
        "            with gr.Row(equal_height=True):\n",
        "                # 左侧操作面板 - 调整比例为合适的大小\n",
        "                with gr.Column(scale=5, elem_classes=\"left-panel\"):\n",
        "                    gr.Markdown(\"## 📂 文档处理区\")\n",
        "                    with gr.Group():\n",
        "                        file_input = gr.File(\n",
        "                            label=\"上传PDF文档\",\n",
        "                            file_types=[\".pdf\"],\n",
        "                            file_count=\"multiple\"\n",
        "                        )\n",
        "                        upload_btn = gr.Button(\"🚀 开始处理\", variant=\"primary\")\n",
        "                        upload_status = gr.Textbox(\n",
        "                            label=\"处理状态\",\n",
        "                            interactive=False,\n",
        "                            lines=2\n",
        "                        )\n",
        "                        file_list = gr.Textbox(\n",
        "                            label=\"已处理文件\",\n",
        "                            interactive=False,\n",
        "                            lines=3,\n",
        "                            elem_classes=\"file-list\"\n",
        "                        )\n",
        "\n",
        "                    # 将问题输入区移至左侧面板底部\n",
        "                    gr.Markdown(\"## ❓ 输入问题\")\n",
        "                    with gr.Group():\n",
        "                        question_input = gr.Textbox(\n",
        "                            label=\"输入问题\",\n",
        "                            lines=3,\n",
        "                            placeholder=\"请输入您的问题...\",\n",
        "                            elem_id=\"question-input\"\n",
        "                        )\n",
        "                        with gr.Row():\n",
        "                            # 添加联网开关\n",
        "                            web_search_checkbox = gr.Checkbox(\n",
        "                                label=\"启用联网搜索\",\n",
        "                                value=False,\n",
        "                                info=\"打开后将同时搜索网络内容（需配置SERPAPI_KEY）\"\n",
        "                            )\n",
        "\n",
        "                            # 添加模型选择下拉框\n",
        "                            model_choice = gr.Dropdown(\n",
        "                                choices=[\"ollama\", \"siliconflow\"],\n",
        "                                value=\"ollama\",\n",
        "                                label=\"模型选择\",\n",
        "                                info=\"选择使用本地模型或云端模型\"\n",
        "                            )\n",
        "\n",
        "                        with gr.Row():\n",
        "                            ask_btn = gr.Button(\"🔍 开始提问\", variant=\"primary\", scale=2)\n",
        "                            clear_btn = gr.Button(\"🗑️ 清空对话\", variant=\"secondary\", elem_classes=\"clear-button\", scale=1)\n",
        "\n",
        "                    # 添加API配置提示信息\n",
        "                    api_info = gr.HTML(\n",
        "                        \"\"\"\n",
        "                        <div class=\"api-info\" style=\"margin-top:10px;padding:10px;border-radius:5px;background:var(--panel-bg);border:1px solid var(--border-color);\">\n",
        "                            <p>📢 <strong>功能说明：</strong></p>\n",
        "                            <p>1. <strong>联网搜索</strong>：%s</p>\n",
        "                            <p>2. <strong>模型选择</strong>：当前使用 <strong>%s</strong> %s</p>\n",
        "                        </div>\n",
        "                        \"\"\"\n",
        "                    )\n",
        "\n",
        "                # 右侧对话区 - 调整比例\n",
        "                with gr.Column(scale=7, elem_classes=\"right-panel\"):\n",
        "                    gr.Markdown(\"## 📝 对话记录\")\n",
        "\n",
        "                    # 对话记录显示区\n",
        "                    chatbot = gr.Chatbot(\n",
        "                        label=\"对话历史\",\n",
        "                        height=600,  # 增加高度\n",
        "                        elem_classes=\"chat-container\",\n",
        "                        show_label=False\n",
        "                    )\n",
        "\n",
        "                    status_display = gr.HTML(\"\", elem_id=\"status-display\")\n",
        "                    gr.Markdown(\"\"\"\n",
        "                    <div class=\"footer-note\">\n",
        "                        *回答生成可能需要1-2分钟，请耐心等待<br>\n",
        "                        *支持多轮对话，可基于前文继续提问\n",
        "                    </div>\n",
        "                    \"\"\")\n",
        "\n",
        "        # 第二个选项卡：分块可视化\n",
        "        with gr.TabItem(\"📊 分块可视化\"):\n",
        "            with gr.Row():\n",
        "                with gr.Column(scale=1):\n",
        "                    gr.Markdown(\"## 💡 系统模型信息\")\n",
        "\n",
        "                    # 显示系统模型信息卡片\n",
        "                    models_info = get_system_models_info()\n",
        "                    with gr.Group(elem_classes=\"model-card\"):\n",
        "                        gr.Markdown(\"### 核心模型与技术\")\n",
        "\n",
        "                        for key, value in models_info.items():\n",
        "                            with gr.Row():\n",
        "                                gr.Markdown(f\"**{key}**:\", elem_classes=\"label\")\n",
        "                                gr.Markdown(f\"{value}\", elem_classes=\"value\")\n",
        "\n",
        "                with gr.Column(scale=2):\n",
        "                    gr.Markdown(\"## 📄 文档分块统计\")\n",
        "                    refresh_chunks_btn = gr.Button(\"🔄 刷新分块数据\", variant=\"primary\")\n",
        "                    chunks_status = gr.Markdown(\"点击按钮查看分块统计\")\n",
        "\n",
        "            # 分块数据表格和详情\n",
        "            with gr.Row():\n",
        "                chunks_data = gr.Dataframe(\n",
        "                    headers=[\"来源\", \"序号\", \"字符数\", \"分词数\", \"内容预览\"],\n",
        "                    elem_classes=\"chunk-table\",\n",
        "                    interactive=False,\n",
        "                    wrap=True,\n",
        "                    row_count=(10, \"dynamic\")\n",
        "                )\n",
        "\n",
        "            with gr.Row():\n",
        "                chunk_detail_text = gr.Textbox(\n",
        "                    label=\"分块详情\",\n",
        "                    placeholder=\"点击表格中的行查看完整内容...\",\n",
        "                    lines=8,\n",
        "                    elem_classes=\"chunk-detail-box\"\n",
        "                )\n",
        "\n",
        "            gr.Markdown(\"\"\"\n",
        "            <div class=\"footer-note\">\n",
        "                * 点击表格中的行可查看该分块的完整内容<br>\n",
        "                * 分词数表示使用jieba分词后的token数量\n",
        "            </div>\n",
        "            \"\"\")\n",
        "\n",
        "\n",
        "    # 进度显示组件调整到左侧面板下方\n",
        "    with gr.Row(visible=False) as progress_row:\n",
        "        gr.HTML(\"\"\"\n",
        "        <div class=\"progress-text\">\n",
        "            <span>当前进度：</span>\n",
        "            <span id=\"current-step\" style=\"color: #2b6de3;\">初始化...</span>\n",
        "            <span id=\"progress-percent\" style=\"margin-left:15px;color: #e32b2b;\">0%</span>\n",
        "        </div>\n",
        "        \"\"\")\n",
        "\n",
        "    # 定义函数处理事件\n",
        "    def clear_chat_history():\n",
        "        return None, \"对话已清空\"\n",
        "\n",
        "    def process_chat(question, history, enable_web_search, model_choice):\n",
        "        if history is None:\n",
        "            history = []\n",
        "\n",
        "        # 更新模型选择信息的显示\n",
        "        api_text = \"\"\"\n",
        "        <div class=\"api-info\" style=\"margin-top:10px;padding:10px;border-radius:5px;background:var(--panel-bg);border:1px solid var(--border-color);\">\n",
        "            <p>📢 <strong>功能说明：</strong></p>\n",
        "            <p>1. <strong>联网搜索</strong>：%s</p>\n",
        "            <p>2. <strong>模型选择</strong>：当前使用 <strong>%s</strong> %s</p>\n",
        "        </div>\n",
        "        \"\"\" % (\n",
        "            \"已启用\" if enable_web_search else \"未启用\",\n",
        "            \"Cloud DeepSeek-R1 模型\" if model_choice == \"siliconflow\" else \"本地 Ollama 模型\",\n",
        "            \"(需要在.env文件中配置SERPAPI_KEY)\" if enable_web_search else \"\"\n",
        "        )\n",
        "\n",
        "        # 如果问题为空，不处理\n",
        "        if not question or question.strip() == \"\":\n",
        "            history.append((\"\", \"问题不能为空，请输入有效问题。\"))\n",
        "            return history, \"\", api_text\n",
        "\n",
        "        # 添加用户问题到历史\n",
        "        history.append((question, \"\"))\n",
        "\n",
        "        # 创建生成器\n",
        "        resp_generator = stream_answer(question, enable_web_search, model_choice)\n",
        "\n",
        "        # 流式更新回答\n",
        "        for response, status in resp_generator:\n",
        "            history[-1] = (question, response)\n",
        "            yield history, \"\", api_text\n",
        "\n",
        "    def update_api_info(enable_web_search, model_choice):\n",
        "        api_text = \"\"\"\n",
        "        <div class=\"api-info\" style=\"margin-top:10px;padding:10px;border-radius:5px;background:var(--panel-bg);border:1px solid var(--border-color);\">\n",
        "            <p>📢 <strong>功能说明：</strong></p>\n",
        "            <p>1. <strong>联网搜索</strong>：%s</p>\n",
        "            <p>2. <strong>模型选择</strong>：当前使用 <strong>%s</strong> %s</p>\n",
        "        </div>\n",
        "        \"\"\" % (\n",
        "            \"已启用\" if enable_web_search else \"未启用\",\n",
        "            \"Cloud DeepSeek-R1 模型\" if model_choice == \"siliconflow\" else \"本地 Ollama 模型\",\n",
        "            \"(需要在.env文件中配置SERPAPI_KEY)\" if enable_web_search else \"\"\n",
        "        )\n",
        "        return api_text\n",
        "\n",
        "    # 绑定UI事件\n",
        "    upload_btn.click(\n",
        "        process_multiple_pdfs,\n",
        "        inputs=[file_input],\n",
        "        outputs=[upload_status, file_list],\n",
        "        show_progress=True\n",
        "    )\n",
        "\n",
        "    # 绑定提问按钮\n",
        "    ask_btn.click(\n",
        "        process_chat,\n",
        "        inputs=[question_input, chatbot, web_search_checkbox, model_choice],\n",
        "        outputs=[chatbot, question_input, api_info]\n",
        "    )\n",
        "\n",
        "    # 绑定清空按钮\n",
        "    clear_btn.click(\n",
        "        clear_chat_history,\n",
        "        inputs=[],\n",
        "        outputs=[chatbot, status_display]\n",
        "    )\n",
        "\n",
        "    # 当切换联网搜索或模型选择时更新API信息\n",
        "    web_search_checkbox.change(\n",
        "        update_api_info,\n",
        "        inputs=[web_search_checkbox, model_choice],\n",
        "        outputs=[api_info]\n",
        "    )\n",
        "\n",
        "    model_choice.change(\n",
        "        update_api_info,\n",
        "        inputs=[web_search_checkbox, model_choice],\n",
        "        outputs=[api_info]\n",
        "    )\n",
        "\n",
        "    # 新增：分块可视化刷新按钮事件\n",
        "    refresh_chunks_btn.click(\n",
        "        fn=get_document_chunks,\n",
        "        outputs=[chunks_data, chunks_status]\n",
        "    )\n",
        "\n",
        "    # 新增：分块表格点击事件\n",
        "    chunks_data.select(\n",
        "        fn=show_chunk_details,\n",
        "        inputs=chunks_data,\n",
        "        outputs=chunk_detail_text\n",
        "    )\n",
        "\n",
        "# 修改JavaScript注入部分\n",
        "demo._js = \"\"\"\n",
        "function gradioApp() {\n",
        "    // 设置默认主题为暗色\n",
        "    document.documentElement.setAttribute('data-theme', 'dark');\n",
        "\n",
        "    const observer = new MutationObserver((mutations) => {\n",
        "        document.getElementById(\"loading\").style.display = \"none\";\n",
        "        const progress = document.querySelector('.progress-text');\n",
        "        if (progress) {\n",
        "            const percent = document.querySelector('.progress > div')?.innerText || '';\n",
        "            const step = document.querySelector('.progress-description')?.innerText || '';\n",
        "            document.getElementById('current-step').innerText = step;\n",
        "            document.getElementById('progress-percent').innerText = percent;\n",
        "        }\n",
        "    });\n",
        "    observer.observe(document.body, {childList: true, subtree: true});\n",
        "}\n",
        "\n",
        "function toggleTheme() {\n",
        "    const root = document.documentElement;\n",
        "    const currentTheme = root.getAttribute('data-theme');\n",
        "    const newTheme = currentTheme === 'light' ? 'dark' : 'light';\n",
        "    root.setAttribute('data-theme', newTheme);\n",
        "}\n",
        "\n",
        "// 初始化主题\n",
        "document.addEventListener('DOMContentLoaded', () => {\n",
        "    document.documentElement.setAttribute('data-theme', 'dark');\n",
        "});\n",
        "\"\"\"\n",
        "\n",
        "# 修改端口检查函数\n",
        "def is_port_available(port):\n",
        "    with socket.socket(socket.AF_INET, socket.SOCK_STREAM) as s:\n",
        "        s.settimeout(1)\n",
        "        return s.connect_ex(('127.0.0.1', port)) != 0  # 更可靠的检测方式\n",
        "\n",
        "def check_environment():\n",
        "    \"\"\"环境依赖检查\"\"\"\n",
        "    try:\n",
        "        # 添加模型存在性检查\n",
        "        model_check = session.post(\n",
        "            \"http://localhost:11434/api/show\",\n",
        "            json={\"name\": \"deepseek-r1:7b\"},\n",
        "            timeout=10\n",
        "        )\n",
        "        if model_check.status_code != 200:\n",
        "            print(\"模型未加载！请先执行：\")\n",
        "            print(\"ollama pull deepseek-r1:7b\")\n",
        "            return False\n",
        "\n",
        "        # 原有检查保持不变...\n",
        "        response = session.get(\n",
        "            \"http://localhost:11434/api/tags\",\n",
        "            proxies={\"http\": None, \"https\": None},  # 禁用代理\n",
        "            timeout=5\n",
        "        )\n",
        "        if response.status_code != 200:\n",
        "            print(\"Ollama服务异常，返回状态码:\", response.status_code)\n",
        "            return False\n",
        "        return True\n",
        "    except Exception as e:\n",
        "        print(\"Ollama连接失败:\", str(e))\n",
        "        return False\n",
        "\n",
        "# 方案2：禁用浏览器缓存（添加meta标签）\n",
        "gr.HTML(\"\"\"\n",
        "<meta http-equiv=\"Cache-Control\" content=\"no-cache, no-store, must-revalidate\">\n",
        "<meta http-equiv=\"Pragma\" content=\"no-cache\">\n",
        "<meta http-equiv=\"Expires\" content=\"0\">\n",
        "\"\"\")\n",
        "\n",
        "# 恢复主程序启动部分\n",
        "if __name__ == \"__main__\":\n",
        "    if not check_environment():\n",
        "        exit(1)\n",
        "    ports = [17995, 17996, 17997, 17998, 17999]\n",
        "    selected_port = next((p for p in ports if is_port_available(p)), None)\n",
        "\n",
        "    if not selected_port:\n",
        "        print(\"所有端口都被占用，请手动释放端口\")\n",
        "        exit(1)\n",
        "\n",
        "    try:\n",
        "        ollama_check = session.get(\"http://localhost:11434\", timeout=5)\n",
        "        if ollama_check.status_code != 200:\n",
        "            print(\"Ollama服务未正常启动！\")\n",
        "            print(\"请先执行：ollama serve 启动服务\")\n",
        "            exit(1)\n",
        "\n",
        "        webbrowser.open(f\"http://127.0.0.1:{selected_port}\")\n",
        "        demo.launch(\n",
        "            server_port=selected_port,\n",
        "            server_name=\"0.0.0.0\",\n",
        "            show_error=True,\n",
        "            ssl_verify=False,\n",
        "            height=900\n",
        "        )\n",
        "    except Exception as e:\n",
        "        print(f\"启动失败: {str(e)}\")\n",
        "\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "id": "vOinlXnItPJk",
        "outputId": "48693fba-ce61-4516-ef7c-12bd7f9f517c"
      },
      "execution_count": 11,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.11/dist-packages/huggingface_hub/utils/_auth.py:94: UserWarning: \n",
            "The secret `HF_TOKEN` does not exist in your Colab secrets.\n",
            "To authenticate with the Hugging Face Hub, create a token in your settings tab (https://huggingface.co/settings/tokens), set it as secret in your Google Colab and restart your session.\n",
            "You will be able to reuse this secret in all of your notebooks.\n",
            "Please note that authentication is recommended but still optional to access public models or datasets.\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Gradio version: 5.30.0\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "<ipython-input-11-609fbca34b2a>:1720: UserWarning: You have not specified a value for the `type` parameter. Defaulting to the 'tuples' format for chatbot messages, but this is deprecated and will be removed in a future version of Gradio. Please set type='messages' instead, which uses openai-style dictionaries with 'role' and 'content' keys.\n",
            "  chatbot = gr.Chatbot(\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "模型未加载！请先执行：\n",
            "ollama pull deepseek-r1:7b\n",
            "It looks like you are running Gradio on a hosted a Jupyter notebook. For the Gradio app to work, sharing must be enabled. Automatically setting `share=True` (you can turn this off by setting `share=False` in `launch()` explicitly).\n",
            "\n",
            "Colab notebook detected. To show errors in colab notebook, set debug=True in launch()\n",
            "* Running on public URL: https://e4f418e45eb6d79f65.gradio.live\n",
            "\n",
            "This share link expires in 1 week. For free permanent hosting and GPU upgrades, run `gradio deploy` from the terminal in the working directory to deploy to Hugging Face Spaces (https://huggingface.co/spaces)\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "<div><iframe src=\"https://e4f418e45eb6d79f65.gradio.live\" width=\"100%\" height=\"900\" allow=\"autoplay; camera; microphone; clipboard-read; clipboard-write;\" frameborder=\"0\" allowfullscreen></iframe></div>"
            ]
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "/content/Local_Pdf_Chat_RAG/rag_demo_pro.py\n",
        "\n",
        "import os\n",
        "os.environ['HF_ENDPOINT'] = 'https://hf-mirror.com'\n",
        "# 以上两行添加的Hugging Face镜像设置，是为了解决没有科学上网环境下载向量模型的问题\n",
        "import gradio as gr\n",
        "from pdfminer.high_level import extract_text_to_fp\n",
        "from sentence_transformers import SentenceTransformer\n",
        "# 导入交叉编码器\n",
        "from sentence_transformers import CrossEncoder\n",
        "import faiss # Новый импорт\n",
        "import requests\n",
        "import json\n",
        "from io import StringIO\n",
        "from langchain_text_splitters import RecursiveCharacterTextSplitter\n",
        "import os\n",
        "import socket\n",
        "import webbrowser\n",
        "import logging\n",
        "from requests.adapters import HTTPAdapter\n",
        "from urllib3.util.retry import Retry\n",
        "import time\n",
        "from datetime import datetime\n",
        "import hashlib\n",
        "import re\n",
        "from dotenv import load_dotenv\n",
        "# 导入BM25算法库\n",
        "from rank_bm25 import BM25Okapi\n",
        "import numpy as np # Убедимся, что numpy импортирован\n",
        "import jieba\n",
        "import threading\n",
        "from functools import lru_cache\n",
        "\n",
        "# 加载环境变量\n",
        "load_dotenv()\n",
        "SERPAPI_KEY = os.getenv(\"SERPAPI_KEY\")  # В .env файле установите SERPAPI_KEY\n",
        "SEARCH_ENGINE = \"google\"  # Можно изменить на другую поисковую систему при необходимости\n",
        "# Новое: Конфигурация метода переранжирования (кросс-энкодер или LLM)\n",
        "RERANK_METHOD = os.getenv(\"RERANK_METHOD\", \"cross_encoder\")  # \"cross_encoder\" или \"llm\"\n",
        "# Новое: Конфигурация SiliconFlow API\n",
        "SILICONFLOW_API_KEY = os.getenv(\"SILICONFLOW_API_KEY\")\n",
        "SILICONFLOW_API_URL = os.getenv(\"SILICONFLOW_API_URL\", \"https://api.siliconflow.cn/v1/chat/completions\")\n",
        "\n",
        "# В начале файла добавляем настройки таймаута\n",
        "import requests\n",
        "requests.adapters.DEFAULT_RETRIES = 3  # Увеличиваем количество попыток\n",
        "\n",
        "# В начале файла добавляем настройки переменных окружения\n",
        "os.environ['TF_ENABLE_ONEDNN_OPTS'] = '0'  # Отключаем оптимизацию oneDNN\n",
        "\n",
        "# В самом начале файла добавляем конфигурацию прокси\n",
        "import os\n",
        "os.environ['NO_PROXY'] = 'localhost,127.0.0.1'  # Новая настройка обхода прокси\n",
        "\n",
        "# Инициализация компонентов\n",
        "EMBED_MODEL = SentenceTransformer('all-MiniLM-L6-v2')\n",
        "# Модель для эмбеддингов также можно переключить на модель, оптимизированную для китайского языка, например:\n",
        "# EMBED_MODEL = SentenceTransformer('shibing624/text2vec-base-chinese')\n",
        "\n",
        "# FAISS相关的 глобальные переменные\n",
        "faiss_index = None\n",
        "faiss_contents_map = {}  # original_id -> content\n",
        "faiss_metadatas_map = {} # original_id -> metadata\n",
        "faiss_id_order_for_index = [] # Сохраняет порядок ID, как они были добавлены в FAISS\n",
        "\n",
        "# Новое: Инициализация кросс-энкодера (отложенная загрузка)\n",
        "cross_encoder = None\n",
        "cross_encoder_lock = threading.Lock()\n",
        "\n",
        "def get_cross_encoder():\n",
        "    \"\"\"延迟加载交叉编码器模型\"\"\"\n",
        "    global cross_encoder\n",
        "    if cross_encoder is None:\n",
        "        with cross_encoder_lock:\n",
        "            if cross_encoder is None:\n",
        "                try:\n",
        "                    # 使用多语言交叉编码器，更适合中文\n",
        "                    cross_encoder = CrossEncoder('sentence-transformers/distiluse-base-multilingual-cased-v2')\n",
        "                    logging.info(\"交叉编码器加载成功\")\n",
        "                except Exception as e:\n",
        "                    logging.error(f\"加载交叉编码器失败: {str(e)}\")\n",
        "                    # 设置为None，下次调用会重试\n",
        "                    cross_encoder = None\n",
        "    return cross_encoder\n",
        "\n",
        "# 新增：BM25索引管理\n",
        "def recursive_retrieval(initial_query, max_iterations=3, enable_web_search=False, model_choice=\"ollama\"):\n",
        "    \"\"\"\n",
        "    实现递归检索与迭代查询功能\n",
        "    通过分析当前查询结果，确定是否需要进一步查询\n",
        "\n",
        "    Args:\n",
        "        initial_query: 初始查询\n",
        "        max_iterations: 最大迭代次数\n",
        "        enable_web_search: 是否启用网络搜索\n",
        "        model_choice: 使用的模型选择(\"ollama\"或\"siliconflow\")\n",
        "\n",
        "    Returns:\n",
        "        包含所有检索内容的列表\n",
        "    \"\"\"\n",
        "    query = initial_query\n",
        "    all_contexts = []\n",
        "    all_doc_ids = []  # 使用原始ID\n",
        "    all_metadata = []\n",
        "\n",
        "    global faiss_index, faiss_contents_map, faiss_metadatas_map, faiss_id_order_for_index\n",
        "\n",
        "    for i in range(max_iterations):\n",
        "        logging.info(f\"递归检索迭代 {i+1}/{max_iterations}，当前查询: {query}\")\n",
        "\n",
        "        web_results_texts = [] # Store text from web results for context building\n",
        "        if enable_web_search and check_serpapi_key():\n",
        "            try:\n",
        "                # update_web_results now needs to handle FAISS directly or be adapted\n",
        "                # For now, let's assume it returns texts to be added to context\n",
        "                web_search_raw_results = update_web_results(query) # This function needs to be adapted for FAISS\n",
        "                for res in web_search_raw_results:\n",
        "                    text = f\"标题：{res.get('title', '')}\\\\n摘要：{res.get('snippet', '')}\"\n",
        "                    web_results_texts.append(text)\n",
        "                    # We would also need to add these to faiss_index, faiss_contents_map etc.\n",
        "                    # and get their FAISS indices if we want them to be part of semantic search.\n",
        "                    # This part is complex due to dynamic addition and potential ID clashes.\n",
        "                    # For now, web results are added as pure text context, not searched semantically *again* within this loop's FAISS query.\n",
        "            except Exception as e:\n",
        "                logging.error(f\"网络搜索错误: {str(e)}\")\n",
        "\n",
        "        query_embedding = EMBED_MODEL.encode([query])\n",
        "        query_embedding_np = np.array(query_embedding).astype('float32')\n",
        "\n",
        "        semantic_results_docs = []\n",
        "        semantic_results_metadatas = []\n",
        "        semantic_results_ids = []\n",
        "\n",
        "        if faiss_index and faiss_index.ntotal > 0:\n",
        "            try:\n",
        "                D, I = faiss_index.search(query_embedding_np, k=10) # D: distances, I: indices\n",
        "                # I contains the internal FAISS indices. We need to map them back to original IDs.\n",
        "                for faiss_idx in I[0]: # I[0] because query_embedding_np was a batch of 1\n",
        "                    if faiss_idx != -1 and faiss_idx < len(faiss_id_order_for_index):\n",
        "                        original_id = faiss_id_order_for_index[faiss_idx]\n",
        "                        semantic_results_docs.append(faiss_contents_map.get(original_id, \"\"))\n",
        "                        semantic_results_metadatas.append(faiss_metadatas_map.get(original_id, {}))\n",
        "                        semantic_results_ids.append(original_id)\n",
        "            except Exception as e:\n",
        "                logging.error(f\"FAISS 检索错误: {str(e)}\")\n",
        "\n",
        "        bm25_results = BM25_MANAGER.search(query, top_k=10) # BM25_MANAGER.search returns list of dicts\n",
        "\n",
        "        # Adapt hybrid_merge to work with current data structures\n",
        "        # It expects semantic_results in a specific format if we pass it directly\n",
        "        # For now, prepare a structure similar to old semantic_results for hybrid_merge\n",
        "        prepared_semantic_results_for_hybrid = {\n",
        "            \"ids\": [semantic_results_ids],\n",
        "            \"documents\": [semantic_results_docs],\n",
        "            \"metadatas\": [semantic_results_metadatas]\n",
        "        }\n",
        "\n",
        "        hybrid_results = hybrid_merge(prepared_semantic_results_for_hybrid, bm25_results, alpha=0.7)\n",
        "\n",
        "        doc_ids_current_iter = []\n",
        "        docs_current_iter = []\n",
        "        metadata_list_current_iter = []\n",
        "\n",
        "        if hybrid_results:\n",
        "            for doc_id, result_data in hybrid_results[:10]: # doc_id here is the original_id\n",
        "                doc_ids_current_iter.append(doc_id)\n",
        "                docs_current_iter.append(result_data['content'])\n",
        "                metadata_list_current_iter.append(result_data['metadata'])\n",
        "\n",
        "        if docs_current_iter:\n",
        "            try:\n",
        "                reranked_results = rerank_results(query, docs_current_iter, doc_ids_current_iter, metadata_list_current_iter, top_k=5)\n",
        "            except Exception as e:\n",
        "                logging.error(f\"重排序错误: {str(e)}\")\n",
        "                reranked_results = [(doc_id, {'content': doc, 'metadata': meta, 'score': 1.0})\n",
        "                                  for doc_id, doc, meta in zip(doc_ids_current_iter, docs_current_iter, metadata_list_current_iter)]\n",
        "        else:\n",
        "            reranked_results = []\n",
        "\n",
        "        current_contexts_for_llm = web_results_texts[:] # Start with web results for LLM context\n",
        "        for doc_id, result_data in reranked_results:\n",
        "            doc = result_data['content']\n",
        "            metadata = result_data['metadata']\n",
        "\n",
        "            if doc_id not in all_doc_ids:\n",
        "                all_doc_ids.append(doc_id)\n",
        "                all_contexts.append(doc)\n",
        "                all_metadata.append(metadata)\n",
        "            current_contexts_for_llm.append(doc) # Add reranked local docs for LLM context\n",
        "\n",
        "        if i == max_iterations - 1:\n",
        "            break\n",
        "\n",
        "        if current_contexts_for_llm: # Use combined web and local context for deciding next query\n",
        "            current_summary = \"\\\\n\".join(current_contexts_for_llm[:3]) if current_contexts_for_llm else \"未找到相关信息\"\n",
        "\n",
        "            next_query_prompt = f\"\"\"基于原始问题: {initial_query}\n",
        "以及已检索信息:\n",
        "{current_summary}\n",
        "\n",
        "分析是否需要进一步查询。如果需要，请提供新的查询问题，使用不同角度或更具体的关键词。\n",
        "如果已经有充分信息，请回复'不需要进一步查询'。\n",
        "\n",
        "新查询(如果需要):\"\"\"\n",
        "\n",
        "            try:\n",
        "                if model_choice == \"siliconflow\":\n",
        "                    logging.info(\"使用SiliconFlow API分析是否需要进一步查询\")\n",
        "                    next_query_result = call_siliconflow_api(next_query_prompt, temperature=0.7, max_tokens=256)\n",
        "                    # SiliconFlow API返回格式包含回答和可能的思维链，这里只需要回答部分来判断是否继续\n",
        "                    # 假设call_siliconflow_api返回的是一个元组 (回答, 思维链) 或只是回答字符串\n",
        "                    if isinstance(next_query_result, tuple):\n",
        "                         next_query = next_query_result[0].strip() # 取回答部分\n",
        "                    else:\n",
        "                         next_query = next_query_result.strip() # 如果只返回字符串\n",
        "\n",
        "                    # 移除潜在的思维链标记\n",
        "                    if \"<think>\" in next_query:\n",
        "                        next_query = next_query.split(\"<think>\")[0].strip()\n",
        "\n",
        "\n",
        "                else:\n",
        "                    logging.info(\"使用本地Ollama模型分析是否需要进一步查询\")\n",
        "                    response = session.post(\n",
        "                        \"http://localhost:11434/api/generate\",\n",
        "                        json={\n",
        "                            \"model\": \"deepseek-r1:1.5b\",\n",
        "                            \"prompt\": next_query_prompt,\n",
        "                            \"stream\": False\n",
        "                        },\n",
        "                        timeout=30\n",
        "                    )\n",
        "                    # Ollama 返回格式不同，需要根据实际情况提取\n",
        "                    next_query = response.json().get(\"response\", \"\").strip()\n",
        "\n",
        "                if \"不需要\" in next_query or \"不需要进一步查询\" in next_query or len(next_query) < 5:\n",
        "                    logging.info(\"LLM判断不需要进一步查询，结束递归检索\")\n",
        "                    break\n",
        "\n",
        "                # 使用新查询继续迭代\n",
        "                query = next_query\n",
        "                logging.info(f\"生成新查询: {query}\")\n",
        "            except Exception as e:\n",
        "                logging.error(f\"生成新查询时出错: {str(e)}\")\n",
        "                break\n",
        "        else:\n",
        "            # 如果当前迭代没有检索到内容，结束迭代\n",
        "            break\n",
        "\n",
        "    return all_contexts, all_doc_ids, all_metadata\n",
        "\n",
        "class BM25IndexManager:\n",
        "    def __init__(self):\n",
        "        self.bm25_index = None\n",
        "        self.doc_mapping = {}  # 映射BM25索引位置到文档ID\n",
        "        self.tokenized_corpus = []\n",
        "        self.raw_corpus = []\n",
        "\n",
        "    def build_index(self, documents, doc_ids):\n",
        "        \"\"\"构建BM25索引\"\"\"\n",
        "        self.raw_corpus = documents\n",
        "        self.doc_mapping = {i: doc_id for i, doc_id in enumerate(doc_ids)}\n",
        "\n",
        "        # 对文档进行分词，使用jieba分词器更适合中文\n",
        "        self.tokenized_corpus = []\n",
        "        for doc in documents:\n",
        "            # 对中文文档进行分词\n",
        "            tokens = list(jieba.cut(doc))\n",
        "            self.tokenized_corpus.append(tokens)\n",
        "\n",
        "        # 创建BM25索引\n",
        "        self.bm25_index = BM25Okapi(self.tokenized_corpus)\n",
        "        return True\n",
        "\n",
        "    def search(self, query, top_k=5):\n",
        "        \"\"\"使用BM25检索相关文档\"\"\"\n",
        "        if not self.bm25_index:\n",
        "            return []\n",
        "\n",
        "        # 对查询进行分词\n",
        "        tokenized_query = list(jieba.cut(query))\n",
        "\n",
        "        # 获取BM25得分\n",
        "        bm25_scores = self.bm25_index.get_scores(tokenized_query)\n",
        "\n",
        "        # 获取得分最高的文档索引\n",
        "        top_indices = np.argsort(bm25_scores)[-top_k:][::-1]\n",
        "\n",
        "        # 返回结果\n",
        "        results = []\n",
        "        for idx in top_indices:\n",
        "            if bm25_scores[idx] > 0:  # 只返回有相关性的结果\n",
        "                results.append({\n",
        "                    'id': self.doc_mapping[idx],\n",
        "                    'score': float(bm25_scores[idx]),\n",
        "                    'content': self.raw_corpus[idx]\n",
        "                })\n",
        "\n",
        "        return results\n",
        "\n",
        "    def clear(self):\n",
        "        \"\"\"清空索引\"\"\"\n",
        "        self.bm25_index = None\n",
        "        self.doc_mapping = {}\n",
        "        self.tokenized_corpus = []\n",
        "        self.raw_corpus = []\n",
        "\n",
        "# 初始化BM25索引管理器\n",
        "BM25_MANAGER = BM25IndexManager()\n",
        "\n",
        "logging.basicConfig(level=logging.INFO)\n",
        "\n",
        "print(\"Gradio version:\", gr.__version__)  # 添加版本输出\n",
        "\n",
        "# 在初始化组件后添加：\n",
        "session = requests.Session()\n",
        "retries = Retry(\n",
        "    total=3,\n",
        "    backoff_factor=0.1,\n",
        "    status_forcelist=[500, 502, 503, 504]\n",
        ")\n",
        "session.mount('http://', HTTPAdapter(max_retries=retries))\n",
        "\n",
        "#########################################\n",
        "# SerpAPI 网络查询及向量化处理函数\n",
        "#########################################\n",
        "def serpapi_search(query: str, num_results: int = 5) -> list:\n",
        "    \"\"\"\n",
        "    执行 SerpAPI 搜索，并返回解析后的结构化结果\n",
        "    \"\"\"\n",
        "    if not SERPAPI_KEY:\n",
        "        raise ValueError(\"未设置 SERPAPI_KEY 环境变量。请在.env文件中设置您的 API 密钥。\")\n",
        "    try:\n",
        "        params = {\n",
        "            \"engine\": SEARCH_ENGINE,\n",
        "            \"q\": query,\n",
        "            \"api_key\": SERPAPI_KEY,\n",
        "            \"num\": num_results,\n",
        "            \"hl\": \"zh-CN\",  # 中文界面\n",
        "            \"gl\": \"cn\"\n",
        "        }\n",
        "        response = requests.get(\"https://serpapi.com/search\", params=params, timeout=15)\n",
        "        response.raise_for_status()\n",
        "        search_data = response.json()\n",
        "        return _parse_serpapi_results(search_data)\n",
        "    except Exception as e:\n",
        "        logging.error(f\"网络搜索失败: {str(e)}\")\n",
        "        return []\n",
        "\n",
        "def _parse_serpapi_results(data: dict) -> list:\n",
        "    \"\"\"解析 SerpAPI 返回的原始数据\"\"\"\n",
        "    results = []\n",
        "    if \"organic_results\" in data:\n",
        "        for item in data[\"organic_results\"]:\n",
        "            result = {\n",
        "                \"title\": item.get(\"title\"),\n",
        "                \"url\": item.get(\"link\"),\n",
        "                \"snippet\": item.get(\"snippet\"),\n",
        "                \"timestamp\": item.get(\"date\")  # 若有时间信息，可选\n",
        "            }\n",
        "            results.append(result)\n",
        "    # 如果有知识图谱信息，也可以添加置顶（可选）\n",
        "    if \"knowledge_graph\" in data:\n",
        "        kg = data[\"knowledge_graph\"]\n",
        "        results.insert(0, {\n",
        "            \"title\": kg.get(\"title\"),\n",
        "            \"url\": kg.get(\"source\", {}).get(\"link\", \"\"),\n",
        "            \"snippet\": kg.get(\"description\"),\n",
        "            \"source\": \"knowledge_graph\"\n",
        "        })\n",
        "    return results\n",
        "\n",
        "def update_web_results(query: str, num_results: int = 5) -> list:\n",
        "    \"\"\"\n",
        "    基于 SerpAPI 搜索结果。注意：此版本不将结果存入FAISS。\n",
        "    它仅返回原始搜索结果。\n",
        "    \"\"\"\n",
        "    results = serpapi_search(query, num_results)\n",
        "    if not results:\n",
        "        logging.info(\"网络搜索没有返回结果或发生错误\")\n",
        "        return []\n",
        "\n",
        "    # 之前这里有删除旧网络结果和添加到ChromaDB的逻辑。\n",
        "    # 由于FAISS IndexFlatL2不支持按ID删除，并且动态添加涉及复杂ID管理，\n",
        "    # 此简化版本不将网络结果添加到FAISS索引。\n",
        "    # 返回原始结果，供调用者决定如何使用（例如，仅作为文本上下文）。\n",
        "    logging.info(f\"网络搜索返回 {len(results)} 条结果，这些结果不会被添加到FAISS索引中。\")\n",
        "    return results # 返回原始SerpAPI结果列表\n",
        "\n",
        "# 检查是否配置了SERPAPI_KEY\n",
        "def check_serpapi_key():\n",
        "    \"\"\"检查是否配置了SERPAPI_KEY\"\"\"\n",
        "    return SERPAPI_KEY is not None and SERPAPI_KEY.strip() != \"\"\n",
        "\n",
        "# 添加文件处理状态跟踪\n",
        "class FileProcessor:\n",
        "    def __init__(self):\n",
        "        self.processed_files = {}  # 存储已处理文件的状态\n",
        "\n",
        "    def clear_files(self):\n",
        "        \"\"\"清空所有文件记录\"\"\"\n",
        "        self.processed_files = {}\n",
        "\n",
        "    def add_file(self, file_name):\n",
        "        self.processed_files[file_name] = {\n",
        "            'status': '等待处理',\n",
        "            'timestamp': datetime.now().strftime('%Y-%m-%d %H:%M:%S'),\n",
        "            'chunks': 0\n",
        "        }\n",
        "\n",
        "    def update_status(self, file_name, status, chunks=None):\n",
        "        if file_name in self.processed_files:\n",
        "            self.processed_files[file_name]['status'] = status\n",
        "            if chunks is not None:\n",
        "                self.processed_files[file_name]['chunks'] = chunks\n",
        "\n",
        "    def get_file_list(self):\n",
        "        return [\n",
        "            f\"📄 {fname} | {info['status']}\"\n",
        "            for fname, info in self.processed_files.items()\n",
        "        ]\n",
        "\n",
        "file_processor = FileProcessor()\n",
        "\n",
        "#########################################\n",
        "# 矛盾检测函数\n",
        "#########################################\n",
        "def detect_conflicts(sources):\n",
        "    \"\"\"精准矛盾检测算法\"\"\"\n",
        "    key_facts = {}\n",
        "    for item in sources:\n",
        "        facts = extract_facts(item['text'] if 'text' in item else item.get('excerpt', ''))\n",
        "        for fact, value in facts.items():\n",
        "            if fact in key_facts:\n",
        "                if key_facts[fact] != value:\n",
        "                    return True\n",
        "            else:\n",
        "                key_facts[fact] = value\n",
        "    return False\n",
        "\n",
        "def extract_facts(text):\n",
        "    \"\"\"从文本提取关键事实（示例逻辑）\"\"\"\n",
        "    facts = {}\n",
        "    # 提取数值型事实\n",
        "    numbers = re.findall(r'\\b\\d{4}年|\\b\\d+%', text)\n",
        "    if numbers:\n",
        "        facts['关键数值'] = numbers\n",
        "    # 提取技术术语\n",
        "    if \"产业图谱\" in text:\n",
        "        facts['技术方法'] = list(set(re.findall(r'[A-Za-z]+模型|[A-Z]{2,}算法', text)))\n",
        "    return facts\n",
        "\n",
        "def evaluate_source_credibility(source):\n",
        "    \"\"\"评估来源可信度\"\"\"\n",
        "    credibility_scores = {\n",
        "        \"gov.cn\": 0.9,\n",
        "        \"edu.cn\": 0.85,\n",
        "        \"weixin\": 0.7,\n",
        "        \"zhihu\": 0.6,\n",
        "        \"baidu\": 0.5\n",
        "    }\n",
        "\n",
        "    url = source.get('url', '')\n",
        "    if not url:\n",
        "        return 0.5  # 默认中等可信度\n",
        "\n",
        "    domain_match = re.search(r'//([^/]+)', url)\n",
        "    if not domain_match:\n",
        "        return 0.5\n",
        "\n",
        "    domain = domain_match.group(1)\n",
        "\n",
        "    # 检查是否匹配任何已知域名\n",
        "    for known_domain, score in credibility_scores.items():\n",
        "        if known_domain in domain:\n",
        "            return score\n",
        "\n",
        "    return 0.5  # 默认中等可信度\n",
        "\n",
        "def extract_text(filepath):\n",
        "    \"\"\"改进的PDF文本提取方法\"\"\"\n",
        "    output = StringIO()\n",
        "    with open(filepath, 'rb') as file:\n",
        "        extract_text_to_fp(file, output)\n",
        "    return output.getvalue()\n",
        "\n",
        "def process_multiple_pdfs(files, progress=gr.Progress()):\n",
        "    \"\"\"处理多个PDF文件\"\"\"\n",
        "    if not files:\n",
        "        return \"请选择要上传的PDF文件\", []\n",
        "\n",
        "    try:\n",
        "        # 清空向量数据库和相关存储\n",
        "        progress(0.1, desc=\"清理历史数据...\")\n",
        "        global faiss_index, faiss_contents_map, faiss_metadatas_map, faiss_id_order_for_index\n",
        "        faiss_index = None\n",
        "        faiss_contents_map = {}\n",
        "        faiss_metadatas_map = {}\n",
        "        faiss_id_order_for_index = []\n",
        "\n",
        "        # 清空BM25索引\n",
        "        BM25_MANAGER.clear()\n",
        "        logging.info(\"成功清理历史FAISS数据和BM25索引\")\n",
        "\n",
        "        # 清空文件处理状态\n",
        "        file_processor.clear_files()\n",
        "\n",
        "        total_files = len(files)\n",
        "        processed_results = []\n",
        "        total_chunks = 0\n",
        "\n",
        "        all_new_chunks = []\n",
        "        all_new_metadatas = []\n",
        "        all_new_original_ids = []\n",
        "\n",
        "        for idx, file in enumerate(files, 1):\n",
        "            try:\n",
        "                file_name = os.path.basename(file.name)\n",
        "                progress((idx-1)/total_files, desc=f\"处理文件 {idx}/{total_files}: {file_name}\")\n",
        "\n",
        "                file_processor.add_file(file_name)\n",
        "                text = extract_text(file.name)\n",
        "\n",
        "                text_splitter = RecursiveCharacterTextSplitter(\n",
        "                    chunk_size=400,\n",
        "                    chunk_overlap=40,\n",
        "                    separators=[\"\\n\\n\", \"\\n\", \"。\", \"，\", \"；\", \"：\", \" \", \"\"]\n",
        "                )\n",
        "                chunks = text_splitter.split_text(text)\n",
        "\n",
        "                if not chunks:\n",
        "                    raise ValueError(\"文档内容为空或无法提取文本\")\n",
        "\n",
        "                doc_id = f\"doc_{int(time.time())}_{idx}\"\n",
        "\n",
        "                # Store chunks and metadatas temporarily before batch embedding\n",
        "                current_file_ids = [f\"{doc_id}_chunk_{i}\" for i in range(len(chunks))]\n",
        "                current_file_metadatas = [{\"source\": file_name, \"doc_id\": doc_id} for _ in chunks]\n",
        "\n",
        "                all_new_chunks.extend(chunks)\n",
        "                all_new_metadatas.extend(current_file_metadatas)\n",
        "                all_new_original_ids.extend(current_file_ids)\n",
        "\n",
        "                total_chunks += len(chunks)\n",
        "                file_processor.update_status(file_name, \"处理完成\", len(chunks))\n",
        "                processed_results.append(f\"✅ {file_name}: 成功处理 {len(chunks)} 个文本块\")\n",
        "\n",
        "            except Exception as e:\n",
        "                error_msg = str(e)\n",
        "                logging.error(f\"处理文件 {file_name} 时出错: {error_msg}\")\n",
        "                file_processor.update_status(file_name, f\"处理失败: {error_msg}\")\n",
        "                processed_results.append(f\"❌ {file_name}: 处理失败 - {error_msg}\")\n",
        "\n",
        "        if all_new_chunks:\n",
        "            progress(0.8, desc=\"生成文本嵌入...\")\n",
        "            embeddings = EMBED_MODEL.encode(all_new_chunks, show_progress_bar=True)\n",
        "            embeddings_np = np.array(embeddings).astype('float32')\n",
        "\n",
        "            progress(0.9, desc=\"构建FAISS索引...\")\n",
        "            if faiss_index is None: # Should always be None here due to clearing\n",
        "                dimension = embeddings_np.shape[1]\n",
        "                faiss_index = faiss.IndexFlatL2(dimension)\n",
        "\n",
        "            faiss_index.add(embeddings_np)\n",
        "\n",
        "            for i, original_id in enumerate(all_new_original_ids):\n",
        "                faiss_contents_map[original_id] = all_new_chunks[i]\n",
        "                faiss_metadatas_map[original_id] = all_new_metadatas[i]\n",
        "            faiss_id_order_for_index.extend(all_new_original_ids) # Keep track of order for FAISS indices\n",
        "            logging.info(f\"FAISS索引构建完成，共索引 {faiss_index.ntotal} 个文本块\")\n",
        "\n",
        "        summary = f\"\\n总计处理 {total_files} 个文件，{total_chunks} 个文本块\"\n",
        "        processed_results.append(summary)\n",
        "\n",
        "        progress(0.95, desc=\"构建BM25检索索引...\")\n",
        "        update_bm25_index() # This will need to use faiss_contents_map\n",
        "\n",
        "        file_list = file_processor.get_file_list()\n",
        "\n",
        "        return \"\\n\".join(processed_results), file_list\n",
        "\n",
        "    except Exception as e:\n",
        "        error_msg = str(e)\n",
        "        logging.error(f\"整体处理过程出错: {error_msg}\")\n",
        "        return f\"处理过程出错: {error_msg}\", []\n",
        "\n",
        "# 新增：交叉编码器重排序函数\n",
        "def rerank_with_cross_encoder(query, docs, doc_ids, metadata_list, top_k=5):\n",
        "    \"\"\"\n",
        "    使用交叉编码器对检索结果进行重排序\n",
        "\n",
        "    参数:\n",
        "        query: 查询字符串\n",
        "        docs: 文档内容列表\n",
        "        doc_ids: 文档ID列表\n",
        "        metadata_list: 元数据列表\n",
        "        top_k: 返回结果数量\n",
        "\n",
        "    返回:\n",
        "        重排序后的结果列表 [(doc_id, {'content': doc, 'metadata': metadata, 'score': score}), ...]\n",
        "    \"\"\"\n",
        "    if not docs:\n",
        "        return []\n",
        "\n",
        "    encoder = get_cross_encoder()\n",
        "    if encoder is None:\n",
        "        logging.warning(\"交叉编码器不可用，跳过重排序\")\n",
        "        # 返回原始顺序（按索引排序）\n",
        "        return [(doc_id, {'content': doc, 'metadata': meta, 'score': 1.0 - idx/len(docs)})\n",
        "                for idx, (doc_id, doc, meta) in enumerate(zip(doc_ids, docs, metadata_list))]\n",
        "\n",
        "    # 准备交叉编码器输入\n",
        "    cross_inputs = [[query, doc] for doc in docs]\n",
        "\n",
        "    try:\n",
        "        # 计算相关性得分\n",
        "        scores = encoder.predict(cross_inputs)\n",
        "\n",
        "        # 组合结果\n",
        "        results = [\n",
        "            (doc_id, {\n",
        "                'content': doc,\n",
        "                'metadata': meta,\n",
        "                'score': float(score)  # 确保是Python原生类型\n",
        "            })\n",
        "            for doc_id, doc, meta, score in zip(doc_ids, docs, metadata_list, scores)\n",
        "        ]\n",
        "\n",
        "        # 按得分排序\n",
        "        results = sorted(results, key=lambda x: x[1]['score'], reverse=True)\n",
        "\n",
        "        # 返回前K个结果\n",
        "        return results[:top_k]\n",
        "    except Exception as e:\n",
        "        logging.error(f\"交叉编码器重排序失败: {str(e)}\")\n",
        "        # 出错时返回原始顺序\n",
        "        return [(doc_id, {'content': doc, 'metadata': meta, 'score': 1.0 - idx/len(docs)})\n",
        "                for idx, (doc_id, doc, meta) in enumerate(zip(doc_ids, docs, metadata_list))]\n",
        "\n",
        "# 新增：LLM相关性评分函数\n",
        "@lru_cache(maxsize=32)\n",
        "def get_llm_relevance_score(query, doc):\n",
        "    \"\"\"\n",
        "    使用LLM对查询和文档的相关性进行评分（带缓存）\n",
        "\n",
        "    参数:\n",
        "        query: 查询字符串\n",
        "        doc: 文档内容\n",
        "\n",
        "    返回:\n",
        "        相关性得分 (0-10)\n",
        "    \"\"\"\n",
        "    try:\n",
        "        # 构建评分提示词\n",
        "        prompt = f\"\"\"给定以下查询和文档片段，评估它们的相关性。\n",
        "        评分标准：0分表示完全不相关，10分表示高度相关。\n",
        "        只需返回一个0-10之间的整数分数，不要有任何其他解释。\n",
        "\n",
        "        查询: {query}\n",
        "\n",
        "        文档片段: {doc}\n",
        "\n",
        "        相关性分数(0-10):\"\"\"\n",
        "\n",
        "        # 调用本地LLM\n",
        "        response = session.post(\n",
        "            \"http://localhost:11434/api/generate\",\n",
        "            json={\n",
        "                \"model\": \"deepseek-r1:1.5b\",  # 使用较小模型进行评分\n",
        "                \"prompt\": prompt,\n",
        "                \"stream\": False\n",
        "            },\n",
        "            timeout=30\n",
        "        )\n",
        "\n",
        "        # 提取得分\n",
        "        result = response.json().get(\"response\", \"\").strip()\n",
        "\n",
        "        # 尝试解析为数字\n",
        "        try:\n",
        "            score = float(result)\n",
        "            # 确保分数在0-10范围内\n",
        "            score = max(0, min(10, score))\n",
        "            return score\n",
        "        except ValueError:\n",
        "            # 如果无法解析为数字，尝试从文本中提取数字\n",
        "            match = re.search(r'\\b([0-9]|10)\\b', result)\n",
        "            if match:\n",
        "                return float(match.group(1))\n",
        "            else:\n",
        "                # 默认返回中等相关性\n",
        "                return 5.0\n",
        "\n",
        "    except Exception as e:\n",
        "        logging.error(f\"LLM评分失败: {str(e)}\")\n",
        "        # 默认返回中等相关性\n",
        "        return 5.0\n",
        "\n",
        "def rerank_with_llm(query, docs, doc_ids, metadata_list, top_k=5):\n",
        "    \"\"\"\n",
        "    使用LLM对检索结果进行重排序\n",
        "\n",
        "    参数:\n",
        "        query: 查询字符串\n",
        "        docs: 文档内容列表\n",
        "        doc_ids: 文档ID列表\n",
        "        metadata_list: 元数据列表\n",
        "        top_k: 返回结果数量\n",
        "\n",
        "    返回:\n",
        "        重排序后的结果列表\n",
        "    \"\"\"\n",
        "    if not docs:\n",
        "        return []\n",
        "\n",
        "    results = []\n",
        "\n",
        "    # 对每个文档进行评分\n",
        "    for doc_id, doc, meta in zip(doc_ids, docs, metadata_list):\n",
        "        # 获取LLM评分\n",
        "        score = get_llm_relevance_score(query, doc)\n",
        "\n",
        "        # 添加到结果列表\n",
        "        results.append((doc_id, {\n",
        "            'content': doc,\n",
        "            'metadata': meta,\n",
        "            'score': score / 10.0  # 归一化到0-1\n",
        "        }))\n",
        "\n",
        "    # 按得分排序\n",
        "    results = sorted(results, key=lambda x: x[1]['score'], reverse=True)\n",
        "\n",
        "    # 返回前K个结果\n",
        "    return results[:top_k]\n",
        "\n",
        "# 新增：通用重排序函数\n",
        "def rerank_results(query, docs, doc_ids, metadata_list, method=None, top_k=5):\n",
        "    \"\"\"\n",
        "    对检索结果进行重排序\n",
        "\n",
        "    参数:\n",
        "        query: 查询字符串\n",
        "        docs: 文档内容列表\n",
        "        doc_ids: 文档ID列表\n",
        "        metadata_list: 元数据列表\n",
        "        method: 重排序方法 (\"cross_encoder\", \"llm\" 或 None)\n",
        "        top_k: 返回结果数量\n",
        "\n",
        "    返回:\n",
        "        重排序后的结果\n",
        "    \"\"\"\n",
        "    # 如果未指定方法，使用全局配置\n",
        "    if method is None:\n",
        "        method = RERANK_METHOD\n",
        "\n",
        "    # 根据方法选择重排序函数\n",
        "    if method == \"llm\":\n",
        "        return rerank_with_llm(query, docs, doc_ids, metadata_list, top_k)\n",
        "    elif method == \"cross_encoder\":\n",
        "        return rerank_with_cross_encoder(query, docs, doc_ids, metadata_list, top_k)\n",
        "    else:\n",
        "        # 默认不进行重排序，按原始顺序返回\n",
        "        return [(doc_id, {'content': doc, 'metadata': meta, 'score': 1.0 - idx/len(docs)})\n",
        "                for idx, (doc_id, doc, meta) in enumerate(zip(doc_ids, docs, metadata_list))]\n",
        "\n",
        "def stream_answer(question, enable_web_search=False, model_choice=\"ollama\", progress=gr.Progress()):\n",
        "    \"\"\"改进的流式问答处理流程，支持联网搜索、混合检索和重排序，以及多种模型选择\"\"\"\n",
        "    global faiss_index # 确保可以访问\n",
        "    try:\n",
        "        # 检查向量数据库是否为空\n",
        "        knowledge_base_exists = faiss_index is not None and faiss_index.ntotal > 0\n",
        "        if not knowledge_base_exists:\n",
        "                if not enable_web_search:\n",
        "                    yield \"⚠️ 知识库为空，请先上传文档。\", \"遇到错误\"\n",
        "                    return\n",
        "                else:\n",
        "                    logging.warning(\"知识库为空，将仅使用网络搜索结果\")\n",
        "\n",
        "        progress(0.3, desc=\"执行递归检索...\")\n",
        "        # 使用递归检索获取更全面的答案上下文\n",
        "        all_contexts, all_doc_ids, all_metadata = recursive_retrieval(\n",
        "            initial_query=question,\n",
        "            max_iterations=3,\n",
        "            enable_web_search=enable_web_search,\n",
        "            model_choice=model_choice\n",
        "        )\n",
        "\n",
        "        # 组合上下文，包含来源信息\n",
        "        context_with_sources = []\n",
        "        sources_for_conflict_detection = []\n",
        "\n",
        "        # 使用检索到的结果构建上下文\n",
        "        for doc, doc_id, metadata in zip(all_contexts, all_doc_ids, all_metadata):\n",
        "            source_type = metadata.get('source', '本地文档')\n",
        "\n",
        "            source_item = {\n",
        "                'text': doc,\n",
        "                'type': source_type\n",
        "            }\n",
        "\n",
        "            if source_type == 'web':\n",
        "                url = metadata.get('url', '未知URL')\n",
        "                title = metadata.get('title', '未知标题')\n",
        "                context_with_sources.append(f\"[网络来源: {title}] (URL: {url})\\n{doc}\")\n",
        "                source_item['url'] = url\n",
        "                source_item['title'] = title\n",
        "            else:\n",
        "                source = metadata.get('source', '未知来源')\n",
        "                context_with_sources.append(f\"[本地文档: {source}]\\n{doc}\")\n",
        "                source_item['source'] = source\n",
        "\n",
        "            sources_for_conflict_detection.append(source_item)\n",
        "\n",
        "        # 检测矛盾\n",
        "        conflict_detected = detect_conflicts(sources_for_conflict_detection)\n",
        "\n",
        "        # 获取可信源\n",
        "        if conflict_detected:\n",
        "            credible_sources = [s for s in sources_for_conflict_detection\n",
        "                               if s['type'] == 'web' and evaluate_source_credibility(s) > 0.7]\n",
        "\n",
        "        context = \"\\n\\n\".join(context_with_sources)\n",
        "\n",
        "        # 添加时间敏感检测\n",
        "        time_sensitive = any(word in question for word in [\"最新\", \"今年\", \"当前\", \"最近\", \"刚刚\"])\n",
        "\n",
        "        # 改进提示词模板，提高回答质量\n",
        "        prompt_template = \"\"\"作为一个专业的问答助手，你需要基于以下{context_type}回答用户问题。\n",
        "\n",
        "提供的参考内容：\n",
        "{context}\n",
        "\n",
        "用户问题：{question}\n",
        "\n",
        "请遵循以下回答原则：\n",
        "1. 仅基于提供的参考内容回答问题，不要使用你自己的知识\n",
        "2. 如果参考内容中没有足够信息，请坦诚告知你无法回答\n",
        "3. 回答应该全面、准确、有条理，并使用适当的段落和结构\n",
        "4. 请用中文回答\n",
        "5. 在回答末尾标注信息来源{time_instruction}{conflict_instruction}\n",
        "\n",
        "请现在开始回答：\"\"\"\n",
        "\n",
        "        prompt = prompt_template.format(\n",
        "            context_type=\"本地文档和网络搜索结果\" if enable_web_search and knowledge_base_exists else (\"网络搜索结果\" if enable_web_search else \"本地文档\"),\n",
        "            context=context if context else (\"网络搜索结果将用于回答。\" if enable_web_search and not knowledge_base_exists else \"知识库为空或未找到相关内容。\"),\n",
        "            question=question,\n",
        "            time_instruction=\"，优先使用最新的信息\" if time_sensitive and enable_web_search else \"\",\n",
        "            conflict_instruction=\"，并明确指出不同来源的差异\" if conflict_detected else \"\"\n",
        "        )\n",
        "\n",
        "        progress(0.7, desc=\"生成回答...\")\n",
        "        full_answer = \"\"\n",
        "\n",
        "        # 根据模型选择使用不同的API\n",
        "        if model_choice == \"siliconflow\":\n",
        "            # 对于SiliconFlow API，不支持流式响应，所以一次性获取\n",
        "            progress(0.8, desc=\"通过SiliconFlow API生成回答...\")\n",
        "            full_answer = call_siliconflow_api(prompt, temperature=0.7, max_tokens=1536)\n",
        "\n",
        "            # 处理思维链\n",
        "            if \"<think>\" in full_answer and \"</think>\" in full_answer:\n",
        "                processed_answer = process_thinking_content(full_answer)\n",
        "            else:\n",
        "                processed_answer = full_answer\n",
        "\n",
        "            yield processed_answer, \"完成!\"\n",
        "        else:\n",
        "            # 使用本地Ollama模型的流式响应\n",
        "            response = session.post(\n",
        "                \"http://localhost:11434/api/generate\",\n",
        "                json={\n",
        "                    \"model\": \"deepseek-r1:1.5b\",\n",
        "                    \"prompt\": prompt,\n",
        "                    \"stream\": True\n",
        "                },\n",
        "                timeout=120,\n",
        "                stream=True\n",
        "            )\n",
        "\n",
        "            for line in response.iter_lines():\n",
        "                if line:\n",
        "                    chunk = json.loads(line.decode()).get(\"response\", \"\")\n",
        "                    full_answer += chunk\n",
        "\n",
        "                    # 检查是否有完整的思维链标签可以处理\n",
        "                    if \"<think>\" in full_answer and \"</think>\" in full_answer:\n",
        "                        # 需要确保完整收集一个思维链片段后再显示\n",
        "                        processed_answer = process_thinking_content(full_answer)\n",
        "                    else:\n",
        "                        processed_answer = full_answer\n",
        "\n",
        "                    yield processed_answer, \"生成回答中...\"\n",
        "\n",
        "            # 处理最终输出，确保应用思维链处理\n",
        "            final_answer = process_thinking_content(full_answer)\n",
        "            yield final_answer, \"完成!\"\n",
        "\n",
        "    except Exception as e:\n",
        "        yield f\"系统错误: {str(e)}\", \"遇到错误\"\n",
        "\n",
        "def query_answer(question, enable_web_search=False, model_choice=\"ollama\", progress=gr.Progress()):\n",
        "    \"\"\"问答处理流程，支持联网搜索、混合检索和重排序，以及多种模型选择\"\"\"\n",
        "    global faiss_index # 确保可以访问\n",
        "    try:\n",
        "        logging.info(f\"收到问题：{question}，联网状态：{enable_web_search}，模型选择：{model_choice}\")\n",
        "\n",
        "        # 检查向量数据库是否为空\n",
        "        knowledge_base_exists = faiss_index is not None and faiss_index.ntotal > 0\n",
        "        if not knowledge_base_exists:\n",
        "                if not enable_web_search:\n",
        "                    return \"⚠️ 知识库为空，请先上传文档。\"\n",
        "                else:\n",
        "                    logging.warning(\"知识库为空，将仅使用网络搜索结果\")\n",
        "\n",
        "        progress(0.3, desc=\"执行递归检索...\")\n",
        "        # 使用递归检索获取更全面的答案上下文\n",
        "        all_contexts, all_doc_ids, all_metadata = recursive_retrieval(\n",
        "            initial_query=question,\n",
        "            max_iterations=3,\n",
        "            enable_web_search=enable_web_search,\n",
        "            model_choice=model_choice\n",
        "        )\n",
        "\n",
        "        # 组合上下文，包含来源信息\n",
        "        context_with_sources = []\n",
        "        sources_for_conflict_detection = []\n",
        "\n",
        "        # 使用检索到的结果构建上下文\n",
        "        for doc, doc_id, metadata in zip(all_contexts, all_doc_ids, all_metadata):\n",
        "            source_type = metadata.get('source', '本地文档')\n",
        "\n",
        "            source_item = {\n",
        "                'text': doc,\n",
        "                'type': source_type\n",
        "            }\n",
        "\n",
        "            if source_type == 'web':\n",
        "                url = metadata.get('url', '未知URL')\n",
        "                title = metadata.get('title', '未知标题')\n",
        "                context_with_sources.append(f\"[网络来源: {title}] (URL: {url})\\n{doc}\")\n",
        "                source_item['url'] = url\n",
        "                source_item['title'] = title\n",
        "            else:\n",
        "                source = metadata.get('source', '未知来源')\n",
        "                context_with_sources.append(f\"[本地文档: {source}]\\n{doc}\")\n",
        "                source_item['source'] = source\n",
        "\n",
        "            sources_for_conflict_detection.append(source_item)\n",
        "\n",
        "        # 检测矛盾\n",
        "        conflict_detected = detect_conflicts(sources_for_conflict_detection)\n",
        "\n",
        "        # 获取可信源\n",
        "        if conflict_detected:\n",
        "            credible_sources = [s for s in sources_for_conflict_detection\n",
        "                              if s['type'] == 'web' and evaluate_source_credibility(s) > 0.7]\n",
        "\n",
        "        context = \"\\n\\n\".join(context_with_sources)\n",
        "\n",
        "        # 添加时间敏感检测\n",
        "        time_sensitive = any(word in question for word in [\"最新\", \"今年\", \"当前\", \"最近\", \"刚刚\"])\n",
        "\n",
        "        # 改进提示词模板，提高回答质量\n",
        "        prompt_template = \"\"\"作为一个专业的问答助手，你需要基于以下{context_type}回答用户问题。\n",
        "\n",
        "提供的参考内容：\n",
        "{context}\n",
        "\n",
        "用户问题：{question}\n",
        "\n",
        "请遵循以下回答原则：\n",
        "1. 仅基于提供的参考内容回答问题，不要使用你自己的知识\n",
        "2. 如果参考内容中没有足够信息，请坦诚告知你无法回答\n",
        "3. 回答应该全面、准确、有条理，并使用适当的段落和结构\n",
        "4. 请用中文回答\n",
        "5. 在回答末尾标注信息来源{time_instruction}{conflict_instruction}\n",
        "\n",
        "请现在开始回答：\"\"\"\n",
        "\n",
        "        prompt = prompt_template.format(\n",
        "            context_type=\"本地文档和网络搜索结果\" if enable_web_search and knowledge_base_exists else (\"网络搜索结果\" if enable_web_search else \"本地文档\"),\n",
        "            context=context if context else (\"网络搜索结果将用于回答。\" if enable_web_search and not knowledge_base_exists else \"知识库为空或未找到相关内容。\"),\n",
        "            question=question,\n",
        "            time_instruction=\"，优先使用最新的信息\" if time_sensitive and enable_web_search else \"\",\n",
        "            conflict_instruction=\"，并明确指出不同来源的差异\" if conflict_detected else \"\"\n",
        "        )\n",
        "\n",
        "        progress(0.8, desc=\"生成回答...\")\n",
        "\n",
        "        # 根据模型选择使用不同的API\n",
        "        if model_choice == \"siliconflow\":\n",
        "            # 使用SiliconFlow API\n",
        "            result = call_siliconflow_api(prompt, temperature=0.7, max_tokens=1536)\n",
        "\n",
        "            # 处理思维链\n",
        "            processed_result = process_thinking_content(result)\n",
        "            return processed_result\n",
        "        else:\n",
        "            # 使用本地Ollama\n",
        "            response = session.post(\n",
        "                \"http://localhost:11434/api/generate\",\n",
        "                json={\n",
        "                    \"model\": \"deepseek-r1:7b\",\n",
        "                    \"prompt\": prompt,\n",
        "                    \"stream\": False\n",
        "                },\n",
        "                timeout=120,  # 延长到2分钟\n",
        "                headers={'Connection': 'close'}  # 添加连接头\n",
        "            )\n",
        "            response.raise_for_status()  # 检查HTTP状态码\n",
        "\n",
        "            progress(1.0, desc=\"完成!\")\n",
        "            # 确保返回字符串并处理空值\n",
        "            result = response.json()\n",
        "            return process_thinking_content(str(result.get(\"response\", \"未获取到有效回答\")))\n",
        "\n",
        "    except json.JSONDecodeError:\n",
        "        return \"响应解析失败，请重试\"\n",
        "    except KeyError:\n",
        "        return \"响应格式异常，请检查模型服务\"\n",
        "    except Exception as e:\n",
        "        progress(1.0, desc=\"遇到错误\")  # 确保进度条完成\n",
        "        return f\"系统错误: {str(e)}\"\n",
        "\n",
        "def process_thinking_content(text):\n",
        "    \"\"\"处理包含<think>标签的内容，将其转换为Markdown格式\"\"\"\n",
        "    # 检查输入是否为有效文本\n",
        "    if text is None:\n",
        "        return \"\"\n",
        "\n",
        "    # 确保输入是字符串\n",
        "    if not isinstance(text, str):\n",
        "        try:\n",
        "            processed_text = str(text)\n",
        "        except:\n",
        "            return \"无法处理的内容格式\"\n",
        "    else:\n",
        "        processed_text = text\n",
        "\n",
        "    # 处理思维链标签\n",
        "    try:\n",
        "        while \"<think>\" in processed_text and \"</think>\" in processed_text:\n",
        "            start_idx = processed_text.find(\"<think>\")\n",
        "            end_idx = processed_text.find(\"</think>\")\n",
        "            if start_idx != -1 and end_idx != -1 and end_idx > start_idx:\n",
        "                thinking_content = processed_text[start_idx + 7:end_idx]\n",
        "                before_think = processed_text[:start_idx]\n",
        "                after_think = processed_text[end_idx + 8:]\n",
        "\n",
        "                # 使用可折叠详情框显示思维链\n",
        "                processed_text = before_think + \"\\n\\n<details>\\n<summary>思考过程（点击展开）</summary>\\n\\n\" + thinking_content + \"\\n\\n</details>\\n\\n\" + after_think\n",
        "\n",
        "        # 处理其他HTML标签，但保留details和summary标签\n",
        "        processed_html = []\n",
        "        i = 0\n",
        "        while i < len(processed_text):\n",
        "            if processed_text[i:i+8] == \"<details\" or processed_text[i:i+9] == \"</details\" or \\\n",
        "               processed_text[i:i+8] == \"<summary\" or processed_text[i:i+9] == \"</summary\":\n",
        "                # 保留这些标签\n",
        "                tag_end = processed_text.find(\">\", i)\n",
        "                if tag_end != -1:\n",
        "                    processed_html.append(processed_text[i:tag_end+1])\n",
        "                    i = tag_end + 1\n",
        "                    continue\n",
        "\n",
        "            if processed_text[i] == \"<\":\n",
        "                processed_html.append(\"&lt;\")\n",
        "            elif processed_text[i] == \">\":\n",
        "                processed_html.append(\"&gt;\")\n",
        "            else:\n",
        "                processed_html.append(processed_text[i])\n",
        "            i += 1\n",
        "\n",
        "        processed_text = \"\".join(processed_html)\n",
        "    except Exception as e:\n",
        "        logging.error(f\"处理思维链内容时出错: {str(e)}\")\n",
        "        # 出错时至少返回原始文本，但确保安全处理HTML标签\n",
        "        try:\n",
        "            return text.replace(\"<\", \"&lt;\").replace(\">\", \"&gt;\")\n",
        "        except:\n",
        "            return \"处理内容时出错\"\n",
        "\n",
        "    return processed_text\n",
        "\n",
        "def call_siliconflow_api(prompt, temperature=0.7, max_tokens=1024):\n",
        "    \"\"\"\n",
        "    调用SiliconFlow API获取回答\n",
        "\n",
        "    Args:\n",
        "        prompt: 提示词\n",
        "        temperature: 温度参数\n",
        "        max_tokens: 最大生成token数\n",
        "\n",
        "    Returns:\n",
        "        生成的回答文本和思维链内容\n",
        "    \"\"\"\n",
        "    # 检查是否配置了SiliconFlow API密钥\n",
        "    if not SILICONFLOW_API_KEY:\n",
        "        logging.error(\"未设置 SILICONFLOW_API_KEY 环境变量。请在.env文件中设置您的 API 密钥。\")\n",
        "        return \"错误：未配置 SiliconFlow API 密钥。\", \"\"\n",
        "\n",
        "    try:\n",
        "        payload = {\n",
        "            \"model\": \"Pro/deepseek-ai/DeepSeek-R1\",\n",
        "            \"messages\": [\n",
        "                {\n",
        "                    \"role\": \"user\",\n",
        "                    \"content\": prompt\n",
        "                }\n",
        "            ],\n",
        "            \"stream\": False,\n",
        "            \"max_tokens\": max_tokens,\n",
        "            \"stop\": None,\n",
        "            \"temperature\": temperature,\n",
        "            \"top_p\": 0.7,\n",
        "            \"top_k\": 50,\n",
        "            \"frequency_penalty\": 0.5,\n",
        "            \"n\": 1,\n",
        "            \"response_format\": {\"type\": \"text\"}\n",
        "        }\n",
        "\n",
        "        headers = {\n",
        "            \"Authorization\": f\"Bearer {SILICONFLOW_API_KEY}\", # 从环境变量获取密钥\n",
        "            \"Content-Type\": \"application/json; charset=utf-8\" # 明确指定编码\n",
        "        }\n",
        "\n",
        "        # 手动将payload编码为UTF-8 JSON字符串\n",
        "        json_payload = json.dumps(payload, ensure_ascii=False).encode('utf-8')\n",
        "\n",
        "        response = requests.post(\n",
        "            SILICONFLOW_API_URL,\n",
        "            data=json_payload, # 通过data参数发送编码后的JSON\n",
        "            headers=headers,\n",
        "            timeout=60  # 延长超时时间\n",
        "        )\n",
        "\n",
        "        response.raise_for_status()\n",
        "        result = response.json()\n",
        "\n",
        "        # 提取回答内容和思维链\n",
        "        if \"choices\" in result and len(result[\"choices\"]) > 0:\n",
        "            message = result[\"choices\"][0][\"message\"]\n",
        "            content = message.get(\"content\", \"\")\n",
        "            reasoning = message.get(\"reasoning_content\", \"\")\n",
        "\n",
        "            # 如果有思维链，则添加特殊标记，以便前端处理\n",
        "            if reasoning:\n",
        "                # 添加思维链标记\n",
        "                full_response = f\"{content}<think>{reasoning}</think>\"\n",
        "                return full_response\n",
        "            else:\n",
        "                return content\n",
        "        else:\n",
        "            return \"API返回结果格式异常，请检查\"\n",
        "\n",
        "    except requests.exceptions.RequestException as e:\n",
        "        logging.error(f\"调用SiliconFlow API时出错: {str(e)}\")\n",
        "        return f\"调用API时出错: {str(e)}\"\n",
        "    except json.JSONDecodeError:\n",
        "        logging.error(\"SiliconFlow API返回非JSON响应\")\n",
        "        return \"API响应解析失败\"\n",
        "    except Exception as e:\n",
        "        logging.error(f\"调用SiliconFlow API时发生未知错误: {str(e)}\")\n",
        "        return f\"发生未知错误: {str(e)}\"\n",
        "\n",
        "def hybrid_merge(semantic_results, bm25_results, alpha=0.7):\n",
        "    \"\"\"\n",
        "    合并语义搜索和BM25搜索结果\n",
        "\n",
        "    参数:\n",
        "        semantic_results: 向量检索结果 (字典格式，包含ids, documents, metadatas)\n",
        "        bm25_results: BM25检索结果 (字典列表，包含id, score, content)\n",
        "        alpha: 语义搜索权重 (0-1)\n",
        "\n",
        "    返回:\n",
        "        合并后的结果列表 [(doc_id, {'score': score, 'content': content, 'metadata': metadata}), ...]\n",
        "    \"\"\"\n",
        "    merged_dict = {}\n",
        "    global faiss_metadatas_map # Ensure we can access the global map\n",
        "\n",
        "    # 处理语义搜索结果\n",
        "    if (semantic_results and\n",
        "        isinstance(semantic_results.get('documents'), list) and len(semantic_results['documents']) > 0 and\n",
        "        isinstance(semantic_results.get('metadatas'), list) and len(semantic_results['metadatas']) > 0 and\n",
        "        isinstance(semantic_results.get('ids'), list) and len(semantic_results['ids']) > 0 and\n",
        "        isinstance(semantic_results['documents'][0], list) and\n",
        "        isinstance(semantic_results['metadatas'][0], list) and\n",
        "        isinstance(semantic_results['ids'][0], list) and\n",
        "        len(semantic_results['documents'][0]) == len(semantic_results['metadatas'][0]) == len(semantic_results['ids'][0])):\n",
        "\n",
        "        num_results = len(semantic_results['documents'][0])\n",
        "        # Assuming semantic_results are already ordered by relevance (higher is better)\n",
        "        # A simple rank-based score, can be replaced if actual scores/distances are available and preferred\n",
        "        for i, (doc_id, doc, meta) in enumerate(zip(semantic_results['ids'][0], semantic_results['documents'][0], semantic_results['metadatas'][0])):\n",
        "            score = 1.0 - (i / max(1, num_results)) # Higher rank (smaller i) gets higher score\n",
        "            merged_dict[doc_id] = {\n",
        "                'score': alpha * score,\n",
        "                'content': doc,\n",
        "                'metadata': meta\n",
        "            }\n",
        "    else:\n",
        "        logging.warning(\"Semantic results are missing, have an unexpected format, or are empty. Skipping semantic part in hybrid merge.\")\n",
        "\n",
        "    # 处理BM25结果\n",
        "    if not bm25_results:\n",
        "        return sorted(merged_dict.items(), key=lambda x: x[1]['score'], reverse=True)\n",
        "\n",
        "    valid_bm25_scores = [r['score'] for r in bm25_results if isinstance(r, dict) and 'score' in r]\n",
        "    max_bm25_score = max(valid_bm25_scores) if valid_bm25_scores else 1.0\n",
        "\n",
        "    for result in bm25_results:\n",
        "        if not (isinstance(result, dict) and 'id' in result and 'score' in result and 'content' in result):\n",
        "            logging.warning(f\"Skipping invalid BM25 result item: {result}\")\n",
        "            continue\n",
        "\n",
        "        doc_id = result['id']\n",
        "        # Normalize BM25 score\n",
        "        normalized_score = result['score'] / max_bm25_score if max_bm25_score > 0 else 0\n",
        "\n",
        "        if doc_id in merged_dict:\n",
        "            merged_dict[doc_id]['score'] += (1 - alpha) * normalized_score\n",
        "        else:\n",
        "            metadata = faiss_metadatas_map.get(doc_id, {}) # Get metadata from our global map\n",
        "            merged_dict[doc_id] = {\n",
        "                'score': (1 - alpha) * normalized_score,\n",
        "                'content': result['content'],\n",
        "                'metadata': metadata\n",
        "            }\n",
        "\n",
        "    merged_results = sorted(merged_dict.items(), key=lambda x: x[1]['score'], reverse=True)\n",
        "    return merged_results\n",
        "\n",
        "# 新增：更新本地文档的BM25索引\n",
        "def update_bm25_index():\n",
        "    \"\"\"更新BM25索引，从内存中的映射加载所有文档\"\"\"\n",
        "    global faiss_contents_map, faiss_id_order_for_index\n",
        "    try:\n",
        "        # Use the ordered list of IDs to ensure consistency\n",
        "        doc_ids = faiss_id_order_for_index\n",
        "        if not doc_ids:\n",
        "            logging.warning(\"没有可索引的文档 (FAISS ID列表为空)\")\n",
        "            BM25_MANAGER.clear()\n",
        "            return False\n",
        "\n",
        "        # Retrieve documents in the correct order\n",
        "        documents = [faiss_contents_map.get(doc_id, \"\") for doc_id in doc_ids]\n",
        "\n",
        "        # Filter out any potential empty documents if necessary, though map access should be safe\n",
        "        valid_docs_with_ids = [(doc_id, doc) for doc_id, doc in zip(doc_ids, documents) if doc]\n",
        "        if not valid_docs_with_ids:\n",
        "            logging.warning(\"没有有效的文档内容可用于BM25索引\")\n",
        "            BM25_MANAGER.clear()\n",
        "            return False\n",
        "\n",
        "        # Separate IDs and documents again for building the index\n",
        "        final_doc_ids = [item[0] for item in valid_docs_with_ids]\n",
        "        final_documents = [item[1] for item in valid_docs_with_ids]\n",
        "\n",
        "        BM25_MANAGER.build_index(final_documents, final_doc_ids)\n",
        "        logging.info(f\"BM25索引更新完成，共索引 {len(final_doc_ids)} 个文档\")\n",
        "        return True\n",
        "    except Exception as e:\n",
        "        logging.error(f\"更新BM25索引失败: {str(e)}\")\n",
        "        return False\n",
        "\n",
        "# 新增函数：获取系统使用的模型信息\n",
        "def get_system_models_info():\n",
        "    \"\"\"返回系统使用的各种模型信息\"\"\"\n",
        "    models_info = {\n",
        "        \"嵌入模型\": \"all-MiniLM-L6-v2\",\n",
        "        \"分块方法\": \"RecursiveCharacterTextSplitter (chunk_size=800, overlap=150)\",\n",
        "        \"检索方法\": \"向量检索 + BM25混合检索 (α=0.7)\",\n",
        "        \"重排序模型\": \"交叉编码器 (sentence-transformers/distiluse-base-multilingual-cased-v2)\",\n",
        "        \"生成模型\": \"deepseek-r1 (7B/1.5B)\",\n",
        "        \"分词工具\": \"jieba (中文分词)\"\n",
        "    }\n",
        "    return models_info\n",
        "\n",
        "# 新增函数：获取文档分块可视化数据\n",
        "def get_document_chunks(progress=gr.Progress()):\n",
        "    \"\"\"获取文档分块结果用于可视化\"\"\"\n",
        "    global faiss_contents_map, faiss_metadatas_map, faiss_id_order_for_index\n",
        "    global chunk_data_cache # Ensure we can update the global cache\n",
        "    try:\n",
        "        progress(0.1, desc=\"正在从内存加载数据...\")\n",
        "\n",
        "        if not faiss_id_order_for_index:\n",
        "            chunk_data_cache = []\n",
        "            return [], \"知识库中没有文档，请先上传并处理文档。\"\n",
        "\n",
        "        progress(0.5, desc=\"正在组织分块数据...\")\n",
        "\n",
        "        doc_groups = {}\n",
        "        # Iterate using the ordered IDs to reflect the FAISS index order conceptually\n",
        "        for doc_id in faiss_id_order_for_index:\n",
        "            doc = faiss_contents_map.get(doc_id, \"\")\n",
        "            meta = faiss_metadatas_map.get(doc_id, {})\n",
        "            if not doc: # Skip if content is somehow empty\n",
        "                continue\n",
        "\n",
        "            source = meta.get('source', '未知来源')\n",
        "            if source not in doc_groups:\n",
        "                doc_groups[source] = []\n",
        "\n",
        "            doc_id_meta = meta.get('doc_id', '未知ID') # Get the original document ID from meta\n",
        "            chunk_info = {\n",
        "                \"original_id\": doc_id, # Keep the chunk-specific ID\n",
        "                \"doc_id\": doc_id_meta,\n",
        "                \"content\": doc[:200] + \"...\" if len(doc) > 200 else doc,\n",
        "                \"full_content\": doc,\n",
        "                \"token_count\": len(list(jieba.cut(doc))),\n",
        "                \"char_count\": len(doc)\n",
        "            }\n",
        "            doc_groups[source].append(chunk_info)\n",
        "\n",
        "        result_dicts = []\n",
        "        result_lists = []\n",
        "\n",
        "        # Keep track of chunks per source for indexing\n",
        "        source_chunk_counters = {source: 0 for source in doc_groups.keys()}\n",
        "        total_chunks = 0\n",
        "\n",
        "        for source, chunks in doc_groups.items():\n",
        "            num_chunks_in_source = len(chunks)\n",
        "            for chunk in chunks:\n",
        "                source_chunk_counters[source] += 1\n",
        "                total_chunks += 1\n",
        "                result_dict = {\n",
        "                    \"来源\": source,\n",
        "                    \"序号\": f\"{source_chunk_counters[source]}/{num_chunks_in_source}\",\n",
        "                    \"字符数\": chunk[\"char_count\"],\n",
        "                    \"分词数\": chunk[\"token_count\"],\n",
        "                    \"内容预览\": chunk[\"content\"],\n",
        "                    \"完整内容\": chunk[\"full_content\"],\n",
        "                    \"原始分块ID\": chunk[\"original_id\"] # Add original ID for potential debugging\n",
        "                }\n",
        "                result_dicts.append(result_dict)\n",
        "\n",
        "                result_lists.append([\n",
        "                    source,\n",
        "                    f\"{source_chunk_counters[source]}/{num_chunks_in_source}\",\n",
        "                    chunk[\"char_count\"],\n",
        "                    chunk[\"token_count\"],\n",
        "                    chunk[\"content\"]\n",
        "                ])\n",
        "\n",
        "        progress(1.0, desc=\"数据加载完成!\")\n",
        "\n",
        "        chunk_data_cache = result_dicts # Update the global cache\n",
        "        summary = f\"总计 {total_chunks} 个文本块，来自 {len(doc_groups)} 个不同来源。\"\n",
        "\n",
        "        return result_lists, summary\n",
        "    except Exception as e:\n",
        "        chunk_data_cache = []\n",
        "        return [], f\"获取分块数据失败: {str(e)}\"\n",
        "\n",
        "# 添加全局缓存变量\n",
        "chunk_data_cache = []\n",
        "\n",
        "# 新增函数：显示分块详情\n",
        "def show_chunk_details(evt: gr.SelectData, chunks):\n",
        "    \"\"\"显示选中分块的详细内容\"\"\"\n",
        "    try:\n",
        "        if evt.index[0] < len(chunk_data_cache):\n",
        "            selected_chunk = chunk_data_cache[evt.index[0]]\n",
        "            return selected_chunk.get(\"完整内容\", \"内容加载失败\")\n",
        "        return \"未找到选中的分块\"\n",
        "    except Exception as e:\n",
        "        return f\"加载分块详情失败: {str(e)}\"\n",
        "\n",
        "# 修改布局部分，添加一个新的标签页\n",
        "with gr.Blocks(\n",
        "    title=\"本地RAG问答系统\",\n",
        "    css=\"\"\"\n",
        "    /* 全局主题变量 */\n",
        "    :root[data-theme=\"light\"] {\n",
        "        --text-color: #2c3e50;\n",
        "        --bg-color: #ffffff;\n",
        "        --panel-bg: #f8f9fa;\n",
        "        --border-color: #e9ecef;\n",
        "        --success-color: #4CAF50;\n",
        "        --error-color: #f44336;\n",
        "        --primary-color: #2196F3;\n",
        "        --secondary-bg: #ffffff;\n",
        "        --hover-color: #e9ecef;\n",
        "        --chat-user-bg: #e3f2fd;\n",
        "        --chat-assistant-bg: #f5f5f5;\n",
        "    }\n",
        "\n",
        "    :root[data-theme=\"dark\"] {\n",
        "        --text-color: #e0e0e0;\n",
        "        --bg-color: #1a1a1a;\n",
        "        --panel-bg: #2d2d2d;\n",
        "        --border-color: #404040;\n",
        "        --success-color: #81c784;\n",
        "        --error-color: #e57373;\n",
        "        --primary-color: #64b5f6;\n",
        "        --secondary-bg: #2d2d2d;\n",
        "        --hover-color: #404040;\n",
        "        --chat-user-bg: #1e3a5f;\n",
        "        --chat-assistant-bg: #2d2d2d;\n",
        "    }\n",
        "\n",
        "    /* 全局样式 */\n",
        "    body {\n",
        "        font-family: -apple-system, BlinkMacSystemFont, \"Segoe UI\", Roboto, \"Helvetica Neue\", Arial, sans-serif;\n",
        "        margin: 0;\n",
        "        padding: 0;\n",
        "        overflow-x: hidden;\n",
        "        width: 100vw;\n",
        "        height: 100vh;\n",
        "    }\n",
        "\n",
        "    .gradio-container {\n",
        "        max-width: 100% !important;\n",
        "        width: 100% !important;\n",
        "        margin: 0 !important;\n",
        "        padding: 0 1% !important;\n",
        "        color: var(--text-color);\n",
        "        background-color: var(--bg-color);\n",
        "        min-height: 100vh;\n",
        "    }\n",
        "\n",
        "    /* 确保标签内容撑满 */\n",
        "    .tabs.svelte-710i53 {\n",
        "        margin: 0 !important;\n",
        "        padding: 0 !important;\n",
        "        width: 100% !important;\n",
        "    }\n",
        "\n",
        "    /* 主题切换按钮 */\n",
        "    .theme-toggle {\n",
        "        position: fixed;\n",
        "        top: 20px;\n",
        "        right: 20px;\n",
        "        z-index: 1000;\n",
        "        padding: 8px 16px;\n",
        "        border-radius: 20px;\n",
        "        border: 1px solid var(--border-color);\n",
        "        background: var(--panel-bg);\n",
        "        color: var(--text-color);\n",
        "        cursor: pointer;\n",
        "        transition: all 0.3s ease;\n",
        "        font-size: 14px;\n",
        "        display: flex;\n",
        "        align-items: center;\n",
        "        gap: 8px;\n",
        "    }\n",
        "\n",
        "    .theme-toggle:hover {\n",
        "        background: var(--hover-color);\n",
        "    }\n",
        "\n",
        "    /* 面板样式 */\n",
        "    .left-panel {\n",
        "        padding-right: 20px;\n",
        "        border-right: 1px solid var(--border-color);\n",
        "        background: var(--bg-color);\n",
        "        width: 100%;\n",
        "    }\n",
        "\n",
        "    .right-panel {\n",
        "        height: 100vh;\n",
        "        background: var(--bg-color);\n",
        "        width: 100%;\n",
        "    }\n",
        "\n",
        "    /* 文件列表样式 */\n",
        "    .file-list {\n",
        "        margin-top: 10px;\n",
        "        padding: 12px;\n",
        "        background: var(--panel-bg);\n",
        "        border-radius: 8px;\n",
        "        font-size: 14px;\n",
        "        line-height: 1.6;\n",
        "        border: 1px solid var(--border-color);\n",
        "    }\n",
        "\n",
        "    /* 答案框样式 */\n",
        "    .answer-box {\n",
        "        min-height: 500px !important;\n",
        "        background: var(--panel-bg);\n",
        "        border-radius: 8px;\n",
        "        padding: 16px;\n",
        "        font-size: 15px;\n",
        "        line-height: 1.6;\n",
        "        border: 1px solid var(--border-color);\n",
        "    }\n",
        "\n",
        "    /* 输入框样式 */\n",
        "    textarea {\n",
        "        background: var(--panel-bg) !important;\n",
        "        color: var(--text-color) !important;\n",
        "        border: 1px solid var(--border-color) !important;\n",
        "        border-radius: 8px !important;\n",
        "        padding: 12px !important;\n",
        "        font-size: 14px !important;\n",
        "    }\n",
        "\n",
        "    /* 按钮样式 */\n",
        "    button.primary {\n",
        "        background: var(--primary-color) !important;\n",
        "        color: white !important;\n",
        "        border-radius: 8px !important;\n",
        "        padding: 8px 16px !important;\n",
        "        font-weight: 500 !important;\n",
        "        transition: all 0.3s ease !important;\n",
        "    }\n",
        "\n",
        "    button.primary:hover {\n",
        "        opacity: 0.9;\n",
        "        transform: translateY(-1px);\n",
        "    }\n",
        "\n",
        "    /* 标题和文本样式 */\n",
        "    h1, h2, h3 {\n",
        "        color: var(--text-color) !important;\n",
        "        font-weight: 600 !important;\n",
        "    }\n",
        "\n",
        "    .footer-note {\n",
        "        color: var(--text-color);\n",
        "        opacity: 0.8;\n",
        "        font-size: 13px;\n",
        "        margin-top: 12px;\n",
        "    }\n",
        "\n",
        "    /* 加载和进度样式 */\n",
        "    #loading, .progress-text {\n",
        "        color: var(--text-color);\n",
        "    }\n",
        "\n",
        "    /* 聊天记录样式 */\n",
        "    .chat-container {\n",
        "        border: 1px solid var(--border-color);\n",
        "        border-radius: 8px;\n",
        "        margin-bottom: 16px;\n",
        "        max-height: 80vh;\n",
        "        height: 80vh !important;\n",
        "        overflow-y: auto;\n",
        "        background: var(--bg-color);\n",
        "    }\n",
        "\n",
        "    .chat-message {\n",
        "        padding: 12px 16px;\n",
        "        margin: 8px;\n",
        "        border-radius: 8px;\n",
        "        font-size: 14px;\n",
        "        line-height: 1.5;\n",
        "    }\n",
        "\n",
        "    .chat-message.user {\n",
        "        background: var(--chat-user-bg);\n",
        "        margin-left: 32px;\n",
        "        border-top-right-radius: 4px;\n",
        "    }\n",
        "\n",
        "    .chat-message.assistant {\n",
        "        background: var(--chat-assistant-bg);\n",
        "        margin-right: 32px;\n",
        "        border-top-left-radius: 4px;\n",
        "    }\n",
        "\n",
        "    .chat-message .timestamp {\n",
        "        font-size: 12px;\n",
        "        color: var(--text-color);\n",
        "        opacity: 0.7;\n",
        "        margin-bottom: 4px;\n",
        "    }\n",
        "\n",
        "    .chat-message .content {\n",
        "        white-space: pre-wrap;\n",
        "    }\n",
        "\n",
        "    /* 按钮组样式 */\n",
        "    .button-row {\n",
        "        display: flex;\n",
        "        gap: 8px;\n",
        "        margin-top: 8px;\n",
        "    }\n",
        "\n",
        "    .clear-button {\n",
        "        background: var(--error-color) !important;\n",
        "    }\n",
        "\n",
        "    /* API配置提示样式 */\n",
        "    .api-info {\n",
        "        margin-top: 10px;\n",
        "        padding: 10px;\n",
        "        border-radius: 5px;\n",
        "        background: var(--panel-bg);\n",
        "        border: 1px solid var(--border-color);\n",
        "    }\n",
        "\n",
        "    /* 新增: 数据可视化卡片样式 */\n",
        "    .model-card {\n",
        "        background: var(--panel-bg);\n",
        "        border-radius: 8px;\n",
        "        padding: 16px;\n",
        "        border: 1px solid var(--border-color);\n",
        "        margin-bottom: 16px;\n",
        "    }\n",
        "\n",
        "    .model-card h3 {\n",
        "        margin-top: 0;\n",
        "        border-bottom: 1px solid var(--border-color);\n",
        "        padding-bottom: 8px;\n",
        "    }\n",
        "\n",
        "    .model-item {\n",
        "        display: flex;\n",
        "        margin-bottom: 8px;\n",
        "    }\n",
        "\n",
        "    .model-item .label {\n",
        "        flex: 1;\n",
        "        font-weight: 500;\n",
        "    }\n",
        "\n",
        "    .model-item .value {\n",
        "        flex: 2;\n",
        "    }\n",
        "\n",
        "    /* 数据表格样式 */\n",
        "    .chunk-table {\n",
        "        border-radius: 8px;\n",
        "        overflow: hidden;\n",
        "        border: 1px solid var(--border-color);\n",
        "    }\n",
        "\n",
        "    .chunk-table th, .chunk-table td {\n",
        "        border: 1px solid var(--border-color);\n",
        "        padding: 8px;\n",
        "    }\n",
        "\n",
        "    .chunk-detail-box {\n",
        "        min-height: 200px;\n",
        "        padding: 16px;\n",
        "        background: var(--panel-bg);\n",
        "        border-radius: 8px;\n",
        "        border: 1px solid var(--border-color);\n",
        "        font-family: monospace;\n",
        "        white-space: pre-wrap;\n",
        "        overflow-y: auto;\n",
        "    }\n",
        "    \"\"\"\n",
        ") as demo:\n",
        "    gr.Markdown(\"# 🧠 智能文档问答系统\")\n",
        "\n",
        "    with gr.Tabs() as tabs:\n",
        "        # 第一个选项卡：问答对话\n",
        "        with gr.TabItem(\"💬 问答对话\"):\n",
        "            with gr.Row(equal_height=True):\n",
        "                # 左侧操作面板 - 调整比例为合适的大小\n",
        "                with gr.Column(scale=5, elem_classes=\"left-panel\"):\n",
        "                    gr.Markdown(\"## 📂 文档处理区\")\n",
        "                    with gr.Group():\n",
        "                        file_input = gr.File(\n",
        "                            label=\"上传PDF文档\",\n",
        "                            file_types=[\".pdf\"],\n",
        "                            file_count=\"multiple\"\n",
        "                        )\n",
        "                        upload_btn = gr.Button(\"🚀 开始处理\", variant=\"primary\")\n",
        "                        upload_status = gr.Textbox(\n",
        "                            label=\"处理状态\",\n",
        "                            interactive=False,\n",
        "                            lines=2\n",
        "                        )\n",
        "                        file_list = gr.Textbox(\n",
        "                            label=\"已处理文件\",\n",
        "                            interactive=False,\n",
        "                            lines=3,\n",
        "                            elem_classes=\"file-list\"\n",
        "                        )\n",
        "\n",
        "                    # 将问题输入区移至左侧面板底部\n",
        "                    gr.Markdown(\"## ❓ 输入问题\")\n",
        "                    with gr.Group():\n",
        "                        question_input = gr.Textbox(\n",
        "                            label=\"输入问题\",\n",
        "                            lines=3,\n",
        "                            placeholder=\"请输入您的问题...\",\n",
        "                            elem_id=\"question-input\"\n",
        "                        )\n",
        "                        with gr.Row():\n",
        "                            # 添加联网开关\n",
        "                            web_search_checkbox = gr.Checkbox(\n",
        "                                label=\"启用联网搜索\",\n",
        "                                value=False,\n",
        "                                info=\"打开后将同时搜索网络内容（需配置SERPAPI_KEY）\"\n",
        "                            )\n",
        "\n",
        "                            # 添加模型选择下拉框\n",
        "                            model_choice = gr.Dropdown(\n",
        "                                choices=[\"ollama\", \"siliconflow\"],\n",
        "                                value=\"ollama\",\n",
        "                                label=\"模型选择\",\n",
        "                                info=\"选择使用本地模型或云端模型\"\n",
        "                            )\n",
        "\n",
        "                        with gr.Row():\n",
        "                            ask_btn = gr.Button(\"🔍 开始提问\", variant=\"primary\", scale=2)\n",
        "                            clear_btn = gr.Button(\"🗑️ 清空对话\", variant=\"secondary\", elem_classes=\"clear-button\", scale=1)\n",
        "\n",
        "                    # 添加API配置提示信息\n",
        "                    api_info = gr.HTML(\n",
        "                        \"\"\"\n",
        "                        <div class=\"api-info\" style=\"margin-top:10px;padding:10px;border-radius:5px;background:var(--panel-bg);border:1px solid var(--border-color);\">\n",
        "                            <p>📢 <strong>功能说明：</strong></p>\n",
        "                            <p>1. <strong>联网搜索</strong>：%s</p>\n",
        "                            <p>2. <strong>模型选择</strong>：当前使用 <strong>%s</strong> %s</p>\n",
        "                        </div>\n",
        "                        \"\"\"\n",
        "                    )\n",
        "\n",
        "                # 右侧对话区 - 调整比例\n",
        "                with gr.Column(scale=7, elem_classes=\"right-panel\"):\n",
        "                    gr.Markdown(\"## 📝 对话记录\")\n",
        "\n",
        "                    # 对话记录显示区\n",
        "                    chatbot = gr.Chatbot(\n",
        "                        label=\"对话历史\",\n",
        "                        height=600,  # 增加高度\n",
        "                        elem_classes=\"chat-container\",\n",
        "                        show_label=False\n",
        "                    )\n",
        "\n",
        "                    status_display = gr.HTML(\"\", elem_id=\"status-display\")\n",
        "                    gr.Markdown(\"\"\"\n",
        "                    <div class=\"footer-note\">\n",
        "                        *回答生成可能需要1-2分钟，请耐心等待<br>\n",
        "                        *支持多轮对话，可基于前文继续提问\n",
        "                    </div>\n",
        "                    \"\"\")\n",
        "\n",
        "        # 第二个选项卡：分块可视化\n",
        "        with gr.TabItem(\"📊 分块可视化\"):\n",
        "            with gr.Row():\n",
        "                with gr.Column(scale=1):\n",
        "                    gr.Markdown(\"## 💡 系统模型信息\")\n",
        "\n",
        "                    # 显示系统模型信息卡片\n",
        "                    models_info = get_system_models_info()\n",
        "                    with gr.Group(elem_classes=\"model-card\"):\n",
        "                        gr.Markdown(\"### 核心模型与技术\")\n",
        "\n",
        "                        for key, value in models_info.items():\n",
        "                            with gr.Row():\n",
        "                                gr.Markdown(f\"**{key}**:\", elem_classes=\"label\")\n",
        "                                gr.Markdown(f\"{value}\", elem_classes=\"value\")\n",
        "\n",
        "                with gr.Column(scale=2):\n",
        "                    gr.Markdown(\"## 📄 文档分块统计\")\n",
        "                    refresh_chunks_btn = gr.Button(\"🔄 刷新分块数据\", variant=\"primary\")\n",
        "                    chunks_status = gr.Markdown(\"点击按钮查看分块统计\")\n",
        "\n",
        "            # 分块数据表格和详情\n",
        "            with gr.Row():\n",
        "                chunks_data = gr.Dataframe(\n",
        "                    headers=[\"来源\", \"序号\", \"字符数\", \"分词数\", \"内容预览\"],\n",
        "                    elem_classes=\"chunk-table\",\n",
        "                    interactive=False,\n",
        "                    wrap=True,\n",
        "                    row_count=(10, \"dynamic\")\n",
        "                )\n",
        "\n",
        "            with gr.Row():\n",
        "                chunk_detail_text = gr.Textbox(\n",
        "                    label=\"分块详情\",\n",
        "                    placeholder=\"点击表格中的行查看完整内容...\",\n",
        "                    lines=8,\n",
        "                    elem_classes=\"chunk-detail-box\"\n",
        "                )\n",
        "\n",
        "            gr.Markdown(\"\"\"\n",
        "            <div class=\"footer-note\">\n",
        "                * 点击表格中的行可查看该分块的完整内容<br>\n",
        "                * 分词数表示使用jieba分词后的token数量\n",
        "            </div>\n",
        "            \"\"\")\n",
        "\n",
        "\n",
        "    # 进度显示组件调整到左侧面板下方\n",
        "    with gr.Row(visible=False) as progress_row:\n",
        "        gr.HTML(\"\"\"\n",
        "        <div class=\"progress-text\">\n",
        "            <span>当前进度：</span>\n",
        "            <span id=\"current-step\" style=\"color: #2b6de3;\">初始化...</span>\n",
        "            <span id=\"progress-percent\" style=\"margin-left:15px;color: #e32b2b;\">0%</span>\n",
        "        </div>\n",
        "        \"\"\")\n",
        "\n",
        "    # 定义函数处理事件\n",
        "    def clear_chat_history():\n",
        "        return None, \"对话已清空\"\n",
        "\n",
        "    def process_chat(question, history, enable_web_search, model_choice):\n",
        "        if history is None:\n",
        "            history = []\n",
        "\n",
        "        # 更新模型选择信息的显示\n",
        "        api_text = \"\"\"\n",
        "        <div class=\"api-info\" style=\"margin-top:10px;padding:10px;border-radius:5px;background:var(--panel-bg);border:1px solid var(--border-color);\">\n",
        "            <p>📢 <strong>功能说明：</strong></p>\n",
        "            <p>1. <strong>联网搜索</strong>：%s</p>\n",
        "            <p>2. <strong>模型选择</strong>：当前使用 <strong>%s</strong> %s</p>\n",
        "        </div>\n",
        "        \"\"\" % (\n",
        "            \"已启用\" if enable_web_search else \"未启用\",\n",
        "            \"Cloud DeepSeek-R1 模型\" if model_choice == \"siliconflow\" else \"本地 Ollama 模型\",\n",
        "            \"(需要在.env文件中配置SERPAPI_KEY)\" if enable_web_search else \"\"\n",
        "        )\n",
        "\n",
        "        # 如果问题为空，不处理\n",
        "        if not question or question.strip() == \"\":\n",
        "            history.append((\"\", \"问题不能为空，请输入有效问题。\"))\n",
        "            return history, \"\", api_text\n",
        "\n",
        "        # 添加用户问题到历史\n",
        "        history.append((question, \"\"))\n",
        "\n",
        "        # 创建生成器\n",
        "        resp_generator = stream_answer(question, enable_web_search, model_choice)\n",
        "\n",
        "        # 流式更新回答\n",
        "        for response, status in resp_generator:\n",
        "            history[-1] = (question, response)\n",
        "            yield history, \"\", api_text\n",
        "\n",
        "    def update_api_info(enable_web_search, model_choice):\n",
        "        api_text = \"\"\"\n",
        "        <div class=\"api-info\" style=\"margin-top:10px;padding:10px;border-radius:5px;background:var(--panel-bg);border:1px solid var(--border-color);\">\n",
        "            <p>📢 <strong>功能说明：</strong></p>\n",
        "            <p>1. <strong>联网搜索</strong>：%s</p>\n",
        "            <p>2. <strong>模型选择</strong>：当前使用 <strong>%s</strong> %s</p>\n",
        "        </div>\n",
        "        \"\"\" % (\n",
        "            \"已启用\" if enable_web_search else \"未启用\",\n",
        "            \"Cloud DeepSeek-R1 模型\" if model_choice == \"siliconflow\" else \"本地 Ollama 模型\",\n",
        "            \"(需要在.env文件中配置SERPAPI_KEY)\" if enable_web_search else \"\"\n",
        "        )\n",
        "        return api_text\n",
        "\n",
        "    # 绑定UI事件\n",
        "    upload_btn.click(\n",
        "        process_multiple_pdfs,\n",
        "        inputs=[file_input],\n",
        "        outputs=[upload_status, file_list],\n",
        "        show_progress=True\n",
        "    )\n",
        "\n",
        "    # 绑定提问按钮\n",
        "    ask_btn.click(\n",
        "        process_chat,\n",
        "        inputs=[question_input, chatbot, web_search_checkbox, model_choice],\n",
        "        outputs=[chatbot, question_input, api_info]\n",
        "    )\n",
        "\n",
        "    # 绑定清空按钮\n",
        "    clear_btn.click(\n",
        "        clear_chat_history,\n",
        "        inputs=[],\n",
        "        outputs=[chatbot, status_display]\n",
        "    )\n",
        "\n",
        "    # 当切换联网搜索或模型选择时更新API信息\n",
        "    web_search_checkbox.change(\n",
        "        update_api_info,\n",
        "        inputs=[web_search_checkbox, model_choice],\n",
        "        outputs=[api_info]\n",
        "    )\n",
        "\n",
        "    model_choice.change(\n",
        "        update_api_info,\n",
        "        inputs=[web_search_checkbox, model_choice],\n",
        "        outputs=[api_info]\n",
        "    )\n",
        "\n",
        "    # 新增：分块可视化刷新按钮事件\n",
        "    refresh_chunks_btn.click(\n",
        "        fn=get_document_chunks,\n",
        "        outputs=[chunks_data, chunks_status]\n",
        "    )\n",
        "\n",
        "    # 新增：分块表格点击事件\n",
        "    chunks_data.select(\n",
        "        fn=show_chunk_details,\n",
        "        inputs=chunks_data,\n",
        "        outputs=chunk_detail_text\n",
        "    )\n",
        "\n",
        "# 修改JavaScript注入部分\n",
        "demo._js = \"\"\"\n",
        "function gradioApp() {\n",
        "    // 设置默认主题为暗色\n",
        "    document.documentElement.setAttribute('data-theme', 'dark');\n",
        "\n",
        "    const observer = new MutationObserver((mutations) => {\n",
        "        document.getElementById(\"loading\").style.display = \"none\";\n",
        "        const progress = document.querySelector('.progress-text');\n",
        "        if (progress) {\n",
        "            const percent = document.querySelector('.progress > div')?.innerText || '';\n",
        "            const step = document.querySelector('.progress-description')?.innerText || '';\n",
        "            document.getElementById('current-step').innerText = step;\n",
        "            document.getElementById('progress-percent').innerText = percent;\n",
        "        }\n",
        "    });\n",
        "    observer.observe(document.body, {childList: true, subtree: true});\n",
        "}\n",
        "\n",
        "function toggleTheme() {\n",
        "    const root = document.documentElement;\n",
        "    const currentTheme = root.getAttribute('data-theme');\n",
        "    const newTheme = currentTheme === 'light' ? 'dark' : 'light';\n",
        "    root.setAttribute('data-theme', newTheme);\n",
        "}\n",
        "\n",
        "// 初始化主题\n",
        "document.addEventListener('DOMContentLoaded', () => {\n",
        "    document.documentElement.setAttribute('data-theme', 'dark');\n",
        "});\n",
        "\"\"\"\n",
        "\n",
        "# 修改端口检查函数\n",
        "def is_port_available(port):\n",
        "    with socket.socket(socket.AF_INET, socket.SOCK_STREAM) as s:\n",
        "        s.settimeout(1)\n",
        "        return s.connect_ex(('127.0.0.1', port)) != 0  # 更可靠的检测方式\n",
        "\n",
        "def check_environment():\n",
        "    \"\"\"环境依赖检查\"\"\"\n",
        "    try:\n",
        "        # 添加模型存在性检查\n",
        "        model_check = session.post(\n",
        "            \"http://localhost:11434/api/show\",\n",
        "            json={\"name\": \"deepseek-r1:7b\"},\n",
        "            timeout=10\n",
        "        )\n",
        "        if model_check.status_code != 200:\n",
        "            print(\"模型未加载！请先执行：\")\n",
        "            print(\"ollama pull deepseek-r1:7b\")\n",
        "            return False\n",
        "\n",
        "        # 原有检查保持不变...\n",
        "        response = session.get(\n",
        "            \"http://localhost:11434/api/tags\",\n",
        "            proxies={\"http\": None, \"https\": None},  # 禁用代理\n",
        "            timeout=5\n",
        "        )\n",
        "        if response.status_code != 200:\n",
        "            print(\"Ollama服务异常，返回状态码:\", response.status_code)\n",
        "            return False\n",
        "        return True\n",
        "    except Exception as e:\n",
        "        print(\"Ollama连接失败:\", str(e))\n",
        "        return False\n",
        "\n",
        "# 方案2：禁用浏览器缓存（添加meta标签）\n",
        "gr.HTML(\"\"\"\n",
        "<meta http-equiv=\"Cache-Control\" content=\"no-cache, no-store, must-revalidate\">\n",
        "<meta http-equiv=\"Pragma\" content=\"no-cache\">\n",
        "<meta http-equiv=\"Expires\" content=\"0\">\n",
        "\"\"\")\n",
        "\n",
        "# 恢复主程序启动部分\n",
        "if __name__ == \"__main__\":\n",
        "    if not check_environment():\n",
        "        exit(1)\n",
        "    ports = [17995, 17996, 17997, 17998, 17999]\n",
        "    selected_port = next((p for p in ports if is_port_available(p)), None)\n",
        "\n",
        "    if not selected_port:\n",
        "        print(\"所有端口都被占用，请手动释放端口\")\n",
        "        exit(1)\n",
        "\n",
        "    try:\n",
        "        ollama_check = session.get(\"http://localhost:11434\", timeout=5)\n",
        "        if ollama_check.status_code != 200:\n",
        "            print(\"Ollama服务未正常启动！\")\n",
        "            print(\"请先执行：ollama serve 启动服务\")\n",
        "            exit(1)\n",
        "\n",
        "        webbrowser.open(f\"http://127.0.0.1:{selected_port}\")\n",
        "        demo.launch(\n",
        "            server_port=selected_port,\n",
        "            server_name=\"0.0.0.0\",\n",
        "            show_error=True,\n",
        "            ssl_verify=False,\n",
        "            height=900\n",
        "        )\n",
        "    except Exception as e:\n",
        "        print(f\"启动失败: {str(e)}\")\n",
        "\n"
      ],
      "metadata": {
        "id": "Kx7xIysvwJzO"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}